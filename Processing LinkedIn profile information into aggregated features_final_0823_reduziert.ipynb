{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5c1d0c",
   "metadata": {},
   "source": [
    "# 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84c106",
   "metadata": {},
   "source": [
    "The aim of this notebook is to process the LinkedIn profiles of individual employees into company-level features. The features are designed on the basis of the literature research of the corresponding master thesis. This notebook forms the basis for further analyses, which are described in the notebooks \"Default_final_2308\" and \"Downgrade_final_2308\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0daaa5",
   "metadata": {},
   "source": [
    "# 2. Load data and prepare libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4df7b",
   "metadata": {},
   "source": [
    "With the use of Chat GPD, comments have been added for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce06563",
   "metadata": {},
   "source": [
    "## 2.1 Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4ec45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d72b50",
   "metadata": {},
   "source": [
    "## 2.2 Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a0da9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the file path to the Excel file\n",
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Data\\LinkedIn\\linkedin_companies_matched_experience.csv'\n",
    "\n",
    "# Import Excel file\n",
    "df_notcleaned = pd.read_csv(dateipfad)\n",
    "\n",
    "# Access the imported data\n",
    "df_notcleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ac960",
   "metadata": {},
   "source": [
    "# 3. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398a0ac",
   "metadata": {},
   "source": [
    "In the course of data preperation, the data are first examined in general (3.1). Data outliers are checked (3.1.2), duplicates are checked (3.1.3) and the correlation is checked (3.1.4). It must be taken into account that the LinkedIn data has already been pre-cleaned before delivery. In the present notebook, it is nevertheless necessary to intensively clean data outliers (3.2.1), unneeded columns (3.2.2), zero values (3.2.4) and data formats (3.2.3, 3.2.5, 3.2.6, 3.2.7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23536161",
   "metadata": {},
   "source": [
    "## 3.1 Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9da43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_notcleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e1613",
   "metadata": {},
   "source": [
    "Explanations to the columns:\n",
    "\n",
    "- experience_index - an index of the experience within a member\n",
    "- member_id - an employee unique identifier\n",
    "- id - a unique identifier of the experience (of the row)\n",
    "- location - location of the experience (As the user wrote it)\n",
    "- company_name - name of the company on LinkedIn\n",
    "- comapny_url - url of the company on LinkedIn\n",
    "- date_form - date of starting the job (as the user specified. some have months, others only years). if \"0\" appears, that means this data is missing.\n",
    "- date_to - date of finishing the job (as the user specified. some have months, others only years). if \"0\" appears, that means this data is missing.\n",
    "- duration - calculated from date_to-date_from\n",
    "- relevant - source of comparison, as specified in the file you sent me. note - the \"not relevant\" companies I kept (due to my explanation in the previous email) are of conf>0.95\n",
    "- Firm_original_name - the original name of the firm in your data\n",
    "\n",
    "**Finding:** Unnamed and id contain no added information and can therefore be droped. Location is probably also not relevant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35becf38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_notcleaned.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f457b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_member_ids = df_notcleaned['member_id'].nunique()\n",
    "print(\"Anzahl der eindeutigen Werte in der Spalte 'member_id':\", unique_member_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac59b4",
   "metadata": {},
   "source": [
    "Unique values are for some rows high (company_name) because the names need to be matched to the firm original name. The original name is not available in all entries yet. Unique values of original is with 598 as expected.\n",
    "\n",
    "**Finding:** company name, company url and firm original name display the same information. Therefore it needs to be tidyed up in one row and the remaining to needs to be droped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d43c55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_notcleaned.info()\n",
    "#int = integers\n",
    "#object = undefined format, can mean anything & is difficult to process\n",
    "#float64 = decimal numbers, fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8776b",
   "metadata": {},
   "source": [
    "**Findings:** \n",
    "- date_from and date_to need to be converted to datetime64\n",
    "- duration can remain float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a4cae",
   "metadata": {},
   "source": [
    "### 3.1.1 Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55a034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_notcleaned.isnull().sum()\n",
    "#Note: is zero counts the values that are equal to missing or NaN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f967bb95",
   "metadata": {},
   "source": [
    "**Finding:** location can be droped since it containes many empty fields and the expected relevance to the use case is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_notcleaned.columns:\n",
    "    count_filtered = (df_notcleaned[column].isin([0, '0.0'])).sum()\n",
    "    print(f\"{column}: {count_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faecbaac",
   "metadata": {},
   "source": [
    "Experience index contains nulls. In context of contet comprehensible and therefore ok. Duration on the other hand should never be zero, since that indicates, that the position was never staffed.\n",
    "\n",
    "**Findind:** Rows that contain null in duration needs to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c65ce6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DataFrame is called df_notcleaned\n",
    "\n",
    "# Output the entry in line 40321\n",
    "row = df_notcleaned.loc[40321]\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5647fd",
   "metadata": {},
   "source": [
    "test line shows that zeros are contained in date_to. It can be assumed that these entries are ongoing occupations. Therefore, the value \"31-12-2018\" is set in the data cleaning.  A similar check needs to be performed for the date_from column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of entries with the text \"0\" in the column \"date_to\"/ \"date_from\"\n",
    "count_zeros_text = (df_notcleaned['date_to'].astype(str) == \"0\").sum()\n",
    "print(f\"Number of entries with the text '0' in the column 'date_to': {count_zeros_text}\")\n",
    "\n",
    "count_zeros_text = (df_notcleaned['date_from'].astype(str) == \"0\").sum()\n",
    "print(f\"Number of entries with the text '0' in the column 'date_from': {count_zeros_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff92fb",
   "metadata": {},
   "source": [
    "As expected, the number of zeros is significantly lower for date_from. These lines should be dropped in cleaning process because there is no functional explanation for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bdcb95",
   "metadata": {},
   "source": [
    "### 3.1.2 Checking for data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e97c5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "With the information about what each column says, it makes no sense to check each row for data outliers. Therefore, only the duration column is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['duration']\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_notcleaned[selected_columns].boxplot()\n",
    "plt.title(\"Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7eba9",
   "metadata": {},
   "source": [
    "40000 days correspond to about 109 years. This is an unrealistic value. The average working life lasts about 40 years, i.e. about 15,000 working days. First, a 98 per cent quantile is formed. Then re-evaluate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12af84f",
   "metadata": {},
   "source": [
    "### 3.1.3 Checking for dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437cd32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicates = df_notcleaned[df_notcleaned.duplicated()]\n",
    "print(\"Duplicate Rows : \",len(duplicates))\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67bb843",
   "metadata": {},
   "source": [
    "As expected there are no dublicates in this dataset. No findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c268d",
   "metadata": {},
   "source": [
    "### 3.1.4 Check for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d4f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = df_notcleaned.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Korrelationsmatrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d759008",
   "metadata": {},
   "source": [
    "Correlation of 0 does not represent a linear relationship. Basically, the results range around values close to zero. Only between unnamed and member_id is there a moderately strong positive correlation. Unnamed does not contain any additional value and is therefore removed in the course of the cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001568a",
   "metadata": {},
   "source": [
    "## What needs to be done to clean the data? A summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103769b",
   "metadata": {},
   "source": [
    "The following findings can be noted:\n",
    "\n",
    "1. Clean data outliers in duration.\n",
    "2. Remove columns unnamed and id. No added content. Also Remove location due to many missing fields and low business impact.\n",
    "3. Replace 0 in date_to with 31-12-2018. Note that the duration is not recalculated (number of zeros will not reduce).\n",
    "4. Drop all rows that contain 0 in Date_from.\n",
    "5. Converte Date_from and Date_to to date values.\n",
    "6. Calculate the duration in a new column. Check if column duration still contains zero.\n",
    "7. Stock exchange abbreviations at the end of the company_original_name lead to problems with the naming of the dataframes. Therefore, a cleansed line must be created. The new line needs to be checked for special characters. \n",
    "\n",
    "\n",
    "The following points are left as they are. A clean-up would have the consequence that mappability would worsen and feature engeneering at employee level would become difficult.\n",
    "- Use only one column for the identification of company (name/url/original name)\n",
    "- Only keep companies that are mappable (Yes, additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a90c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The dataset has {} rows and {} columns. This results in {} data entries.'.format(df_notcleaned.shape[0],df_notcleaned.shape[1], df_notcleaned.size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624b62f",
   "metadata": {},
   "source": [
    "Size is being used to generally identify if an cleaning action was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c6b69",
   "metadata": {},
   "source": [
    "## 3.2 Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def5e41",
   "metadata": {},
   "source": [
    "A copy of the original dataset is created in a new dataframe to allow for backtracking and better control of the work status. rfp = ready for preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp = df_notcleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notcleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4266a",
   "metadata": {},
   "source": [
    "### 3.2.1. Clean data outliers in duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fe6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_98 = df_notcleaned['duration'].quantile(0.98)\n",
    "df_notcleaned = df_notcleaned[df_notcleaned['duration'] <= quantile_98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36323bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notcleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6980287",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['duration']\n",
    "plt.figure(figsize=(10, 8))\n",
    "df_notcleaned[selected_columns].boxplot()\n",
    "plt.title(\"Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29882f8",
   "metadata": {},
   "source": [
    "### 3.2.2 Remove columns unnamed, id and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e896619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rfp = df_rfp.drop(['Unnamed: 0', 'id', 'location'], axis=1)\n",
    "df_rfp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec5ea8",
   "metadata": {},
   "source": [
    "### 3.2.3 Replace 0 in date_to with 31-12-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fc8bd",
   "metadata": {},
   "source": [
    "Reminder: Number of entries with the text '0' in the column 'date_to': 3140329 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace entries with the text \"0\" in the column \"date_to\" with the value \"31-12-2018\"\n",
    "df_rfp.loc[df_rfp['date_to'].astype(str) == \"0\", 'date_to'] = \"2018-12-31\"\n",
    "\n",
    "# Check the update\n",
    "count_zeros_text_updated = (df_rfp['date_to'].astype(str) == \"0\").sum()\n",
    "print(f\"Number of entries with the text '0' in the column 'date_to' after the update: {count_zeros_text_updated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674c78e",
   "metadata": {},
   "source": [
    "### 3.2.4 Delete rows with 0 in date_from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b49de",
   "metadata": {},
   "source": [
    "Reminder: Number of entries with the text '0' in the column 'date_from': 782209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ed25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame without the rows containing 0 in the \"date_from\" column\n",
    "df_rfp2 = df_rfp[df_rfp['date_from'].astype(str) != \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zeros_text_updated = (df_rfp2['date_from'].astype(str) == \"0\").sum()\n",
    "print(f\"Number of entries with the text '0' in the column 'date_from' after the update: {count_zeros_text_updated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65243e28",
   "metadata": {},
   "source": [
    "### 3.2.5 Converte columns date_from and date_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ceb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp3= df_rfp2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f3780",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = df_rfp3.loc[37, 'date_to']\n",
    "data_type = type(value)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e4efd",
   "metadata": {},
   "source": [
    "Indicates that values are shown as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef183011",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_zero_text = df_rfp3['date_to'] == '0'\n",
    "count_zero_text = is_zero_text.sum()\n",
    "print(\"Number of '0' 'date_to':\", count_zero_text)\n",
    "is_zero_text = df_rfp3['date_from'] == '0'\n",
    "count_zero_text = is_zero_text.sum()\n",
    "print(\"Number of '0' 'date_from':\", count_zero_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ddffe",
   "metadata": {},
   "source": [
    "There are no missing values due to cleaning of duration.\n",
    "Now it is necessary to clean up the formats that are in the YYYY format. Columns 36 and 37 contain such values in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp3['date_from'] = df_rfp3['date_from'].apply(lambda x: x + '-01-01' if len(str(x)) == 4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ca894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_rfp3['date_from'].head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp3['date_to'] = df_rfp3['date_to'].apply(lambda x: x + '-12-31' if len(str(x)) == 4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92328db7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_rfp3['date_to'].head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3bcc8",
   "metadata": {},
   "source": [
    "Cleaning successful. All data that had the format YYYY are now YYYY -01-01 in column date_from and YYYY 31-12 in date_to. Everything else remained the same.\n",
    "Since the columns are now consistent, the entries can be converted from text format to datetime64 format. It is expected, that there are a few values that are not conform to YYYY-MM-DD. They are converted to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp3['date_from'] = pd.to_datetime(df_rfp3['date_from'], format='%Y-%m-%d', errors='coerce')\n",
    "df_rfp3['date_to'] = pd.to_datetime(df_rfp3['date_to'], format='%Y-%m-%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b8b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_nat_date_from = df_rfp3['date_from'].isna().sum()\n",
    "count_nat_date_to = df_rfp3['date_to'].isna().sum()\n",
    "\n",
    "print(f\"Number of NaT values in 'date_from': {count_nat_date_from}\")\n",
    "print(f\"Number of NaT values in 'date_to': {count_nat_date_to}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90807739",
   "metadata": {},
   "source": [
    "As expected very few entries. They are droped from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rfp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp3.dropna(subset=['date_from', 'date_to'], inplace=True)\n",
    "print(df_rfp3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6026c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rfp3['date_from'].dtype)\n",
    "print(df_rfp3['date_to'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea146cc3",
   "metadata": {},
   "source": [
    "### 3.2.6 Exact calculation of date_from and calculation of duration where date_to was previously missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp4= df_rfp3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc80fcc",
   "metadata": {},
   "source": [
    "Since the date is now in the format datetime, the exact date_from can be calculated on the basis of the duration for those values where only a year number was present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad41e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rfp4.insert(df_rfp4.columns.get_loc(\"date_from\") + 1, \"date_from_original\", df_rfp4[\"date_from\"].copy())\n",
    "df_rfp4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef431c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_rfp4['duration'] != 0.0) & (df_rfp4['duration'] != 0)\n",
    "df_rfp4.loc[mask, 'date_from'] = df_rfp4.loc[mask, 'date_to'] - pd.to_timedelta(df_rfp4.loc[mask, 'duration'], unit='D')\n",
    "df_rfp4.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314c94d",
   "metadata": {},
   "source": [
    "For those lines where the date_to value was missing, the duration is now calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp4.insert(df_rfp4.columns.get_loc(\"duration\") + 1, \"duration_original\", df_rfp4[\"duration\"].copy())\n",
    "df_rfp4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fe50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp4['duration'] = (df_rfp4['date_to'] - df_rfp4['date_from']).dt.days\n",
    "df_rfp4.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c489c4b",
   "metadata": {},
   "source": [
    "With lines 35 and 26, it can be determined that the cleaning was successful. The two copied columns can be removed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp4.drop(['date_from_original', 'duration_original'], axis=1, inplace=True)\n",
    "df_rfp4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cccc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_filtered = (df_rfp4['duration'].astype(int) == 0).sum()\n",
    "print(f\"Count: {count_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ebf2b",
   "metadata": {},
   "source": [
    "There are still 405.902 values with null. Since those values do not add value they are removed from the dataframe. Rows should be reduced by this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66593a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = df_rfp4.loc[1, 'duration']\n",
    "data_type = type(value)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfp4 = df_rfp4[df_rfp4['duration'] != 0]\n",
    "print(df_rfp4['duration'].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = df_rfp4.shape\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496b49b",
   "metadata": {},
   "source": [
    "### 3.2.7 Cleaning Firm_original_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b595158a",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_rfp5= df_rfp4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b40618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy the column\n",
    "df_rfp5['Copy_Firm_original_name'] = df_rfp5['Firm_original_name']\n",
    "\n",
    "# Remove values in brackets\n",
    "df_rfp5['Firm_original_name'] = df_rfp5['Firm_original_name'].apply(lambda x: re.sub(r'\\(.*\\)', '', str(x)).strip())\n",
    "\n",
    "df_rfp5.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d7e4e",
   "metadata": {},
   "source": [
    "Checking for special characters. Special characters are removed from the respective name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d2420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_rfp5[df_rfp5['Firm_original_name'].str.contains(r'[^\\w\\s]', regex=True)]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1314ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cleaning  Firm_original_name\n",
    "def clean_firm_name(name):\n",
    "    return re.sub(r'[^\\w\\s]', '', str(name))\n",
    "\n",
    "# clean Firm_original_name \n",
    "df_rfp5['Firm_original_name'] = df_rfp5['Firm_original_name'].apply(clean_firm_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_rfp5[df_rfp5['Firm_original_name'].str.contains(r'[^\\w\\s]', regex=True)]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates_a = df_rfp5.duplicated().sum()\n",
    "num_duplicates_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe12a04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_rfpfinal= df_rfp5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b6aff",
   "metadata": {},
   "source": [
    "## 3.3 Feature engeneering & strucuturing the data as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be18cd",
   "metadata": {},
   "source": [
    "In the following, aggegated features are formed from the individual profile information. In order to extract new features from the raw data, the data must first be prepared beyond the classic data preperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy for better work controll --> df_rfe = ready for engeneering\n",
    "df_rfe1= df_rfpfinal.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc0043",
   "metadata": {},
   "source": [
    "### 3.3.1 Prepering for engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804a21d",
   "metadata": {},
   "source": [
    "#### Step 1: filtering the relevant timeline (2014 - 2018)\n",
    "\n",
    "The dataset df_rfe1 is still very extensive and contains data on employees who worked for one of the relevant companies before 2014. These employees are irrelevant for further analysis. Therefore, entries for the period between 2014 and 2018 are filtered first and then a dataframe is formed that contains all entries on employees who worked for a relevant company in the period mentioned. Note: relevant is not yet cleansed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify entries with period between 2014 and 2018\n",
    "date_from = pd.to_datetime(df_rfe1['date_from'])\n",
    "date_to = pd.to_datetime(df_rfe1['date_to'])\n",
    "mask = (date_from.dt.year >= 2014) & (date_from.dt.year <= 2018) & (date_to.dt.year >= 2014) & (date_to.dt.year <= 2018)\n",
    "relevant_entries = df_rfe1[mask]\n",
    "\n",
    "# Step 2: Check condition Firm_original_name > 2 characters\n",
    "relevant_entries = relevant_entries[relevant_entries['Firm_original_name'].str.len() > 2]\n",
    "\n",
    "# Step 3: Remember the Member IDs\n",
    "relevant_member_ids = relevant_entries['member_id'].unique()\n",
    "\n",
    "# Step 4: Create new dataframe df_rfe2 with the identified member IDs\n",
    "df_rfe2 = df_rfe1[df_rfe1['member_id'].isin(relevant_member_ids)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbebb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df0a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 266 in df_rfe2['member_id'].values:\n",
    "    print(\"266 contained in df\")\n",
    "else:\n",
    "    print(\"266 not contained in df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371e3cc",
   "metadata": {},
   "source": [
    "Removal of entries outside 2014 -2018 successful. Member ID 266 serves as a sample. Relevant work experience here was 1988. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76500d58",
   "metadata": {},
   "source": [
    "#### Step 2: Mapping between company and member id\n",
    "\n",
    "At this point, all companies that are not clearly mappable are removed from the data set. Only those companies that have Yes or additional in the relevant column are retained. Subsequently, the employees (member_id) are assigned to the respective companies. This step is necessary because in order to reduce the size and optimise performance, the currently used data set df_rfe2 was reduced by the entries that have no entry in the column Firm_original_name. However, the information about the individual employees will be relevant again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f6128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Forming new dataframe that onnly containes Yes and additional values\n",
    "df_yes_additional = df_rfe2[df_rfe2['relevant'].isin(['Yes', 'additional'])]\n",
    "df_yes_additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the lines where the Firm_original_name is \"Prudential plc\".\n",
    "filtered_rows = df_yes_additional[df_yes_additional[\"Firm_original_name\"] == \"Prudential plc\"]\n",
    "\n",
    "# Print\n",
    "print(filtered_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01928f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the assignment of Firm_original_name to Member-IDs\n",
    "member_id_dict = {}\n",
    "\n",
    "# Iterate over each row of the DataFrame df_yes_additional\n",
    "for index, row in df_yes_additional.iterrows():\n",
    "    # Extract the Firm_original_name and the Member-ID of the current line\n",
    "    firm_name = row['Firm_original_name']\n",
    "    member_id = row['member_id']\n",
    "    \n",
    "    # Add the member ID to the corresponding Firm_original_name in the dictionary\n",
    "    member_id_dict.setdefault(firm_name, []).append(member_id)\n",
    "\n",
    "# Output the assignment of Firm_original_name to Member IDs\n",
    "for firm_name, member_ids in list(member_id_dict.items())[:2]:\n",
    "    print(f\"Firm_original_name: {firm_name}, Member-IDs: {member_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each item in the member_id_dict\n",
    "for firm_name, member_ids in member_id_dict.items():\n",
    "    # Remove duplicate member_ids\n",
    "    unique_member_ids = list(set(member_ids))\n",
    "    \n",
    "    # Update the member_ids in the member_id_dict\n",
    "    member_id_dict[firm_name] = unique_member_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first company from member_id_dict\n",
    "first_company = next(iter(member_id_dict.keys()))\n",
    "\n",
    "# Get the member IDs for the first company\n",
    "member_ids = member_id_dict[first_company]\n",
    "\n",
    "# Print the first company and its member IDs\n",
    "print(f\"First Company: {first_company}\")\n",
    "print(f\"Member IDs: {member_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dffb66",
   "metadata": {},
   "source": [
    "#### Step 3: Extract member_id data for each company \n",
    "A separate data frame is created for each company in the column Firm_original_name. This contains all entries on the employees who worked for the company in the period between 2014 and 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7814d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all unique values in the column \"Firm_name_original\" in the desired format\n",
    "unique_firm_names = df_rfe2['Firm_original_name'].unique()\n",
    "formatted_firm_names = ', '.join(['\"' + name + '\"' for name in unique_firm_names])\n",
    "print(formatted_firm_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78008944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 200,000 entries of the DataFrame df_rfe2\n",
    "df_subset = df_rfe2.head(200000)\n",
    "\n",
    "# Create the directory path and file name\n",
    "csv_filename = r'C:\\Users\\wildn\\Downloads\\Master\\Debug.csv'\n",
    "\n",
    "# Save the Subset DataFrame as CSV\n",
    "df_subset.to_csv(csv_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each unique firm_name in df_rfe2 and create separate CSV files\n",
    "for firm_name in df_yes_additional['Firm_original_name'].unique():\n",
    "    # Get the associated member_id list from member_id_dict for the current firm_name\n",
    "    member_ids = member_id_dict.get(firm_name, [])\n",
    "    \n",
    "    # Filter the rows for the current firm_name using the member_id list\n",
    "    company_dataframe = df_rfe2[df_rfe2['member_id'].isin(member_ids)].copy()\n",
    "\n",
    "    # Print the name of the created DataFrame\n",
    "    print(f\"Created DataFrame for: {firm_name}\")\n",
    "\n",
    "    # Create the directory path and filename\n",
    "    csv_filename = os.path.join(r'C:\\Users\\wildn\\Downloads\\Master\\company_dictonary', f\"df_{firm_name}.csv\")\n",
    "\n",
    "    # Save the DataFrame as CSV\n",
    "    company_dataframe.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the dataframes\n",
    "company_dataframes = {}\n",
    "\n",
    "# Directory path containing the CSV files\n",
    "directory_path = r'C:\\Users\\wildn\\Downloads\\Master\\company_dictonary'\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith('.csv'):\n",
    "        # Generate the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Read the CSV file and create a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract the DataFrame name from the file name (without extension .csv)\n",
    "        dataframe_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Add the DataFrame to the company_dataframes dictionary with the filename as the key\n",
    "        company_dataframes[dataframe_name] = df\n",
    "\n",
    "        # Print the name of the loaded DataFrame\n",
    "        print(f\"Loaded DataFrame: {dataframe_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'df_nan' from the dictionary if it exists\n",
    "company_dataframes.pop('df_nan', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d304149",
   "metadata": {},
   "source": [
    "The company Ferrovial S.A. will be used to validate code results to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b2f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_dataframes['df_Ferrovial  SA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates = company_dataframes['df_Ferrovial  SA'].duplicated().sum()\n",
    "num_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7651",
   "metadata": {},
   "source": [
    "### 3.3.2 Feature engeneering\n",
    "\n",
    "#### Feature 1: Calculation of the employees' work experience at annual level\n",
    "Work experience is calculated on the basis of the duration column. For this purpose, the sum of the entries for duration from the previous professional stations is formed and divided by 365. The column Years of experience shows how much work experience the employee had when he was hired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty column \"Years of Experience\" in each DataFrame of the company_dataframes-Dictionary\n",
    "for df_name, df in company_dataframes.items():\n",
    "    df['Years of Experience'] = ''\n",
    "\n",
    "    # Sort the current DataFrame by member_id and date_from\n",
    "    df = df.sort_values(['member_id', 'date_from'])\n",
    "\n",
    "    # Iterate over each unique member_id in the current DataFrame\n",
    "    for member_id in df['member_id'].unique():\n",
    "        # Filtere den aktuellen DataFrame nach der aktuellen member_id\n",
    "        member_id_df = df[df['member_id'] == member_id]\n",
    "\n",
    "        # Calculate work experience for each row\n",
    "        previous_experience = 0\n",
    "        previous_duration = 0\n",
    "        for index, row in member_id_df.iterrows():\n",
    "            duration = row['duration']\n",
    "\n",
    "            # Check if it is the first entry\n",
    "            if previous_duration != 0:\n",
    "                years_of_experience = (previous_experience + previous_duration) / 365\n",
    "                df.at[index, 'Years of Experience'] = years_of_experience\n",
    "\n",
    "            previous_experience += previous_duration\n",
    "            previous_duration = duration\n",
    "\n",
    "    # Update the DataFrame in the company_dataframes dictionary\n",
    "    company_dataframes[df_name] = df\n",
    "\n",
    "    # Show the modified DataFrame with the new \"Years of Experience\" column\n",
    "    print(f\"DataFrame: {df_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79619619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = company_dataframes['df_Ferrovial  SA'][company_dataframes['df_Ferrovial  SA']['member_id'] == 3963].sort_values('date_from')\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe6aa71",
   "metadata": {},
   "source": [
    "#### Feature 2: Employee development\n",
    "\n",
    "In order to be able to track the development of employees over the years 2014 - 2018, it is necessary to break down to the annual level how many employees were employed and when. For the Migration of Work Experience feature, it will be relevant to know which employee worked when. Therefore, the member_id is used to determine which periods of activity exist. This is done for each dataframe of the respective companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb179b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract Year\n",
    "# Iterate over each DataFrame in the company_dataframes dictionary.\n",
    "for df_name, df in company_dataframes.items():\n",
    "    # Convert \"date_from\" column to datetime64 format\n",
    "    df['date_from'] = pd.to_datetime(df['date_from'], format='%Y-%m-%d')\n",
    "    \n",
    "    # Convert \"date_to\" column to datetime64 format\n",
    "    df['date_to'] = pd.to_datetime(df['date_to'], format='%Y-%m-%d')\n",
    "\n",
    "    # Extract the year from the \"date_from\" column\n",
    "    df['Year date_from'] = df['date_from'].dt.year\n",
    "    \n",
    "    # Extract the year from the \"date_to\" column\n",
    "    df['Year date_to'] = df['date_to'].dt.year\n",
    "    \n",
    "    # Update DataFrame in company_dataframes dictionary\n",
    "    company_dataframes[df_name] = df\n",
    "    \n",
    "    # Output name of edited DataFrame\n",
    "    print(f\"DataFrame: {df_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(company_dataframes['df_Ferrovial  SA']['date_from'].dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_dataframes['df_Ferrovial  SA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing column Years total to have the Years at one place\n",
    "# Iterate over each DataFrame in the company_dataframes dictionary\n",
    "for df_name, df in company_dataframes.items():\n",
    "    # Iterate over each row in the current DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        years_range = list(range(row['Year date_from'], row['Year date_to'] + 1))\n",
    "        years_total = list(set(years_range + [row['Year date_from'], row['Year date_to']]))\n",
    "        df.at[index, 'Years total'] = ','.join(map(str, years_total))\n",
    "\n",
    "    # Update the DataFrame in the company_dataframes dictionary\n",
    "    company_dataframes[df_name] = df\n",
    "    \n",
    "    # Print edited dfs\n",
    "    print(f\"DataFrame: {df_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_dataframes['df_Ferrovial  SA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef34357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each DataFrame in the company_dataframes dictionary.\n",
    "for df_name, df in company_dataframes.items():\n",
    "    # 1. filter by the name of the company\n",
    "    firm_name = df_name.replace(\"df_\", \"\")  # Entferne \"df_\" aus dem DataFrame-Namen\n",
    "    df['Firm_original_name'] = df['Firm_original_name'].fillna('')  # Replace NaN with an empty string\n",
    "    df.loc[df['Firm_original_name'].str.contains(firm_name), 'Firm_original_name'] = firm_name\n",
    "\n",
    "    # 2. Add columns for years 2014-2018\n",
    "    for year in range(2014, 2019):\n",
    "        df[str(year)] = 0\n",
    "\n",
    "    # 3. Set 1 in the corresponding years in which the Member ID has worked\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Firm_original_name'] == firm_name:\n",
    "            years_worked = list(map(int, row['Years total'].split(',')))\n",
    "            for year in range(2014, 2019):\n",
    "                if year in years_worked:\n",
    "                    df.at[index, str(year)] = 1\n",
    "                else:\n",
    "                    df.at[index, str(year)] = 0\n",
    "\n",
    "    # Update the DataFrame in the company_dataframes dictionary\n",
    "    company_dataframes[df_name] = df\n",
    "\n",
    "    # Print edited dfs\n",
    "    print(f\"DataFrame: {df_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f5c94",
   "metadata": {},
   "source": [
    "Validation of the code with Ferrovial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d05fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_dataframes['df_Ferrovial  SA'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fbb1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = company_dataframes['df_Ferrovial  SA'][company_dataframes['df_Ferrovial  SA']['Firm_original_name'] == 'Ferrovial  SA']\n",
    "filtered_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aec5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\wildn\\Downloads\\Master\\KLA Corporation.csv'\n",
    "company_dataframes['df_KLA Corporation'].to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8736e6",
   "metadata": {},
   "source": [
    "Spot checks in df for Ferrovial successful. The allocation in time slices was done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc093d2",
   "metadata": {},
   "source": [
    "A new dataframe is created. This will be used to display the aggregated features of the individual companies. The dataframe is called df_waf (= _with aggregated features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique values from df_rfe1 column Firm_original_name\n",
    "unique_firms = df_rfe1['Firm_original_name'].unique()\n",
    "\n",
    "# Create DataFrame df_waf\n",
    "df_waf = pd.DataFrame(columns=['Firm_original_name'])\n",
    "\n",
    "# Fill Firm_original_name column with unique values\n",
    "df_waf['Firm_original_name'] = unique_firms\n",
    "\n",
    "# Add columns for the years 2014-2018 and initialise with 0\n",
    "for year in range(2014, 2019):\n",
    "    df_waf[str(year)] = 0\n",
    "\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc089e36",
   "metadata": {},
   "source": [
    "All member_ids that show a 1 in the respective column in the company dataframe are written into the columns for the years 2014 to 2018. Duplicate ids are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47ace5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Edit all DataFrames in the company_dataframes dictionary\n",
    "for df_name, df in company_dataframes.items():\n",
    "    # 2. Go through years 2014-2018\n",
    "    for year in range(2014, 2019):\n",
    "        # 3. Filter to 1 in the corresponding column\n",
    "        filtered_df = df[df[str(year)] == 1]\n",
    "        \n",
    "        # 4. Identify the values in the column 'member_id'.\n",
    "        member_ids_year = filtered_df['member_id'].tolist()\n",
    "        \n",
    "        # 5. Transfer the member_ids into the DataFrame df_waf into the corresponding column\n",
    "        firm_name = df_name.replace(\"df_\", \"\")  # Remove 'df_' from the DataFrame name\n",
    "        row_index = df_waf[df_waf['Firm_original_name'] == firm_name].index[0]\n",
    "        existing_member_ids = df_waf.at[row_index, str(year)]\n",
    "        new_member_ids = set(member_ids_year)  # Remove duplicate member_ids\n",
    "        if existing_member_ids:\n",
    "            existing_member_ids = existing_member_ids.split(',')  # Break down the existing member IDs into a list\n",
    "            new_member_ids.update(existing_member_ids)  # Add existing member_ids\n",
    "        df_waf[str(year)] = df_waf[str(year)].astype('object')  # Make sure that the corresponding column has the data type 'object'.\n",
    "        df_waf.at[row_index, str(year)] = ','.join(str(id_) for id_ in new_member_ids)  # Update df_waf\n",
    "\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3eb61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify the rows with empty values in the \"2014\" column\n",
    "mask_null_or_empty = df_waf[\"2014\"].isnull() | df_waf[\"2014\"].eq(\"\")\n",
    "\n",
    "# Filter the rows that contain empty values in the \"2014\" column and output the \"Firm_original_name\".\n",
    "rows_with_null_or_empty = df_waf[mask_null_or_empty]\n",
    "firm_names_with_null_or_empty = rows_with_null_or_empty[\"Firm_original_name\"]\n",
    "\n",
    "print(f\"The following company names have empty values in the '2014' column:\")\n",
    "print(firm_names_with_null_or_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ferrovial_index = df_waf[df_waf['Firm_original_name'] == 'Ferrovial  SA'].index[0]\n",
    "values_2015 = df_waf.at[ferrovial_index, '2015']\n",
    "print(values_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd2117",
   "metadata": {},
   "source": [
    "By counting the individual member ids, the number of member ids can be determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1708b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through years 2014-2018\n",
    "for year in range(2014, 2019):\n",
    "    # Count the number of member IDs in the corresponding column for each row\n",
    "    column_name = str(year)\n",
    "    new_column_name = f'Number of employees {year}'\n",
    "    df_waf[new_column_name] = df_waf[column_name].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed78e77",
   "metadata": {},
   "source": [
    "Validation of the results: The original data frame is accessed and evaluated to see how many member ids are present and how many dubilakte are contained. The difference should correspond to the number in df_waf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['2014', '2015', '2016', '2017', '2018']\n",
    "\n",
    "# Filter DataFrame company_dataframes['df_Ferrovial  S.A.'] for columns\n",
    "filtered_df = company_dataframes['df_Ferrovial  SA'][columns]\n",
    "\n",
    "# Count 1 in selected columns\n",
    "occurrences = filtered_df.eq(1).sum()\n",
    "\n",
    "print(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter for company name and column 2014\n",
    "filtered_df = company_dataframes['df_Ferrovial  SA'][(company_dataframes['df_Ferrovial  SA']['Firm_original_name'] == 'Ferrovial  SA') & (company_dataframes['df_Ferrovial  SA']['2014'] == 1)]\n",
    "\n",
    "# 2. Filter dublicate member-IDs in column 'member_id' and count frequency\n",
    "duplicate_member_ids = filtered_df['member_id'].value_counts()\n",
    "duplicate_member_ids = duplicate_member_ids[duplicate_member_ids > 1]\n",
    "\n",
    "# 3. Calculate sum of frequency\n",
    "total_duplicates_sum = duplicate_member_ids.sum()\n",
    "\n",
    "# Results\n",
    "print(\"Frequency of dublicate member-IDs  2014:\")\n",
    "print(duplicate_member_ids)\n",
    "print(\"Frequency total:\")\n",
    "print(total_duplicates_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c650c7",
   "metadata": {},
   "source": [
    "Check successful: There are 815 entries for the year 2014, of which 181 are duplicates. In order not to eliminate duplicates completely, length must be added again with 87. \n",
    "\n",
    "815-181+87= 721\n",
    "\n",
    "Now Feature Employee development can be created for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba4c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and populate \"Employee development\" column for the year 2015\n",
    "df_waf['Employee development 2015'] = ((df_waf['Number of employees 2015'] - df_waf['Number of employees 2014']) / df_waf['Number of employees 2014']) * 100\n",
    "\n",
    "# Calculate and populate \"Employee development\" column for the year 2016\n",
    "df_waf['Employee development 2016'] = ((df_waf['Number of employees 2016'] - df_waf['Number of employees 2015']) / df_waf['Number of employees 2015']) * 100\n",
    "\n",
    "# Calculate and populate \"Employee development\" column for the year 2017\n",
    "df_waf['Employee development 2017'] = ((df_waf['Number of employees 2017'] - df_waf['Number of employees 2016']) / df_waf['Number of employees 2016']) * 100\n",
    "\n",
    "# Calculate and populate \"Employee development\" column for the year 2018\n",
    "df_waf['Employee development 2018'] = ((df_waf['Number of employees 2018'] - df_waf['Number of employees 2017']) / df_waf['Number of employees 2017']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b5c5d",
   "metadata": {},
   "source": [
    "#### Feature 3: Calculation of notices in each year\n",
    "\n",
    "To calculate notices the member Ids from previous year were searched in current year. Those who could not be found were identfied as employees who left the organisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each year (2014-2018)\n",
    "for year in range(2014, 2019):\n",
    "    current_year_col = str(year)\n",
    "    next_year_col = str(year + 1)\n",
    "    notices_col = f'Notices {year}'\n",
    "    \n",
    "    # Create a new column for notices\n",
    "    df_waf[notices_col] = ''\n",
    "    \n",
    "    # Check if the next year column exists\n",
    "    if next_year_col in df_waf.columns:\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df_waf.iterrows():\n",
    "            current_year_ids = set(str(row[current_year_col]).split(','))\n",
    "            next_year_ids = set(str(row[next_year_col]).split(','))\n",
    "\n",
    "            missing_ids = []\n",
    "\n",
    "            # Iterate over each ID in the current year's IDs\n",
    "            for member_id in current_year_ids:\n",
    "                if member_id not in next_year_ids:\n",
    "                    missing_ids.append(member_id)\n",
    "\n",
    "            # Join the missing IDs into a comma-separated string\n",
    "            missing_ids_str = ','.join(missing_ids)\n",
    "\n",
    "            # Update the notices column for the current year\n",
    "            df_waf.at[index, notices_col] = missing_ids_str \n",
    "    else:\n",
    "        # If the next year column doesn't exist, set the notices column as 'Not available'\n",
    "        df_waf[notices_col] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620e9bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b5141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the years 2014-2018\n",
    "for year in range(2014, 2019):\n",
    "    # Count the number of member_ids in the corresponding column for each row\n",
    "    column_name = f'Notices {year}'\n",
    "    new_column_name = f'Number of notices {year}'\n",
    "    df_waf[new_column_name] = df_waf[column_name].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "df_waf[\"Number of Notices 2018\"] = 0\n",
    "\n",
    "# Print\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value in the \"2014\" column of the 3rd row\n",
    "value_2014 = df_waf.loc[3, \"Notices 2014\"]\n",
    "\n",
    "# Print the value\n",
    "print(f\"Value in '2014' column of the 3rd row: {value_2014}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303f8cf",
   "metadata": {},
   "source": [
    "#### Feature 4: Calculation of migration work expierince due to notices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e77eaf",
   "metadata": {},
   "source": [
    "In der Spalte Notices Year habe ich die Information welche member_id gegangen ist. In dem df_company habe ich das year_date to und die Spalte Years of experience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through 2014-2018\n",
    "for year in range(2014, 2019):\n",
    "    year_col = str(year)\n",
    "    migrating_col = f'Migrating work experience {year}'\n",
    "\n",
    "    # Add new column \"Migrating work experience <Year>\" \n",
    "    df_waf[migrating_col] = 0.0\n",
    "\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c94f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the final results per year\n",
    "results = {}\n",
    "\n",
    "# Iterate over the years 2014 to 2018\n",
    "for firm_name in df_waf[\"Firm_original_name\"].unique():\n",
    "    # Go through years 2014 to 2018\n",
    "    for year in range(2014, 2019):\n",
    "       # 2. Extract the member_ids from the corresponding column Notices <Year>\n",
    "        notices_column = f\"Notices {year}\"\n",
    "        member_ids = df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, notices_column].str.split(\",\").explode()\n",
    "\n",
    "        # Remove empty or invalid values\n",
    "        member_ids = member_ids[member_ids != ''].astype(int)\n",
    "\n",
    "        # 3. Check if there is a corresponding DataFrame in the company_dataframes dictionary\n",
    "        dataframe_key = f\"df_{firm_name}\"\n",
    "        if dataframe_key not in company_dataframes:\n",
    "            print(f\"Dataframe '{dataframe_key}' nicht gefunden. Überspringe Prüfung für '{firm_name}' im Jahr {year}.\")\n",
    "            continue\n",
    "\n",
    "        df_firm = company_dataframes[dataframe_key]\n",
    "\n",
    "        # 4. Perform actions for the found DataFrame\n",
    "        years_of_experience_sum = 0\n",
    "\n",
    "        for member_id in member_ids:\n",
    "            # 4.1 Filter by member_id and sort by date_from\n",
    "            filtered_df = df_firm[df_firm[\"member_id\"] == member_id].sort_values(\"date_from\")\n",
    "\n",
    "            for index, row in filtered_df.iterrows():\n",
    "                # 4.3 Check for the year in the Year date_to column\n",
    "                if row[\"Year date_to\"] == year:\n",
    "                    # 4.4 Check for a higher date in the Year date_to column\n",
    "                    next_index = index + 1\n",
    "                    if next_index < len(filtered_df) and filtered_df.loc[next_index, \"Year date_to\"] > year:\n",
    "                        # 4.4.2 Calculate the duration in years and add to years_of_experience_sum\n",
    "                        duration_years = filtered_df.loc[index, \"duration\"] / 365\n",
    "                        years_of_experience = row[\"Years of Experience\"]\n",
    "                        if years_of_experience != '':\n",
    "                            years_of_experience_sum += duration_years + float(years_of_experience)\n",
    "                    else:\n",
    "                        # 4.4.1 Store value from the Years of Experience column\n",
    "                        years_of_experience = row[\"Years of Experience\"]\n",
    "                        if years_of_experience != '':\n",
    "                            years_of_experience_sum += float(years_of_experience)\n",
    "\n",
    "        # 4.6 Calculate final result\n",
    "        number_of_notices = df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, f\"Number of notices {year}\"].values[0]\n",
    "        migrating_work_experience = years_of_experience_sum / number_of_notices\n",
    "\n",
    "        # Save result in results-Dictionary\n",
    "        results[(firm_name, year)] = migrating_work_experience\n",
    "\n",
    "# 5. Check and update the entries in the df_waf DataFrame\n",
    "for (firm_name, year), value in results.items():\n",
    "    column_name = f\"Migrating work experience {year}\"\n",
    "    df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, column_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f2190",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe029a",
   "metadata": {},
   "source": [
    "Random validation confirms the order of magnitude of the figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085cd91",
   "metadata": {},
   "source": [
    "#### Feature 5: Calcuating Number of new joiners and work experience they bring along "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff08f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each year (2014-2018)\n",
    "for year in range(2014, 2019):\n",
    "    current_year_col = str(year)\n",
    "    previous_year_col = str(year - 1)\n",
    "    new_joiners_col = f'New joiners {year}'\n",
    "    \n",
    "    # Create a new column for new joiners\n",
    "    df_waf[new_joiners_col] = ''\n",
    "    \n",
    "    # Check if the previous year column exists\n",
    "    if previous_year_col in df_waf.columns:\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df_waf.iterrows():\n",
    "            current_year_ids = set(str(row[current_year_col]).split(','))\n",
    "            previous_year_ids = set(str(row[previous_year_col]).split(','))\n",
    "\n",
    "            missing_ids = []\n",
    "\n",
    "            # Iterate over each ID in the current year's IDs\n",
    "            for member_id in current_year_ids:\n",
    "                if member_id not in previous_year_ids:\n",
    "                    missing_ids.append(member_id)\n",
    "\n",
    "            # Join the missing IDs into a comma-separated string\n",
    "            missing_ids_str = ','.join(missing_ids)\n",
    "\n",
    "            # Update the new joiners column for the current year\n",
    "            df_waf.at[index, new_joiners_col] = missing_ids_str \n",
    "    else:\n",
    "        # If the previous year column doesn't exist, set the new joiners column as '0'\n",
    "        df_waf[new_joiners_col] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04870b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the years 2014-2018\n",
    "for year in range(2014, 2019):\n",
    "    # Count the number of member IDs in the corresponding column for each row\n",
    "    column_name = f'New joiners {year}'\n",
    "    new_column_name = f'Number of New Joiners {year}'\n",
    "    df_waf[new_column_name] = df_waf[column_name].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "df_waf[\"Number of New Joiners 2014\"] = 0\n",
    "\n",
    "# Print\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43612431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the final results per year\n",
    "results = {}\n",
    "\n",
    "# Iterate over each unique company name in the \"Firm_original_name\" column of the df_waf DataFrame\n",
    "for firm_name in df_waf[\"Firm_original_name\"].unique():\n",
    "    # Iterate over each year from 2014 to 2018\n",
    "    for year in range(2014, 2019):\n",
    "        # Extract the member IDs from the corresponding \"New joiners <Year>\" column and store them in the member_ids variable\n",
    "        new_joiners_column = f\"New joiners {year}\"\n",
    "        member_ids = df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, new_joiners_column].str.split(\",\").explode()\n",
    "\n",
    "        # Remove empty or invalid values from the member IDs\n",
    "        member_ids = member_ids[member_ids != ''].astype(int)\n",
    "\n",
    "        # Check if there is a corresponding DataFrame for the company in the company_dataframes dictionary\n",
    "        dataframe_key = f\"df_{firm_name}\"\n",
    "        if dataframe_key not in company_dataframes:\n",
    "            print(f\"Dataframe '{dataframe_key}' not found. Skipping check for '{firm_name}' in year {year}.\")\n",
    "            continue\n",
    "\n",
    "        # Retrieve the DataFrame for the company and store it in the df_firm variable\n",
    "        df_firm = company_dataframes[dataframe_key]\n",
    "\n",
    "        # Initialize the sum for years of experience (years_of_experience_sum)\n",
    "        years_of_experience_sum = 0\n",
    "\n",
    "        # Iterate over each member ID\n",
    "        for member_id in member_ids:\n",
    "            # Filter by member ID and sort by the start date (\"date_from\")\n",
    "            filtered_df = df_firm[df_firm[\"member_id\"] == member_id].sort_values(\"date_from\")\n",
    "\n",
    "            # Iterate over each row in the filtered DataFrame\n",
    "            for index, row in filtered_df.iterrows():\n",
    "                # Check if the year in the \"Year date_from\" column matches the current year\n",
    "                if row[\"Year date_from\"] == year:\n",
    "                    # Take the value from the \"Years of Experience\" column and add it to years_of_experience_sum\n",
    "                    years_of_experience = row[\"Years of Experience\"]\n",
    "                    if years_of_experience != '':\n",
    "                        years_of_experience_sum += float(years_of_experience)\n",
    "\n",
    "        # Calculate the final result, the average work experience per New Joiner\n",
    "        number_of_new_joiners = df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, f\"Number of New Joiners {year}\"].values[0]\n",
    "        joining_work_experience = years_of_experience_sum / number_of_new_joiners\n",
    "\n",
    "        # Save the result in the results dictionary using the company name and year as the key\n",
    "        results[(firm_name, year)] = joining_work_experience\n",
    "\n",
    "# Check and update the entries in the df_waf DataFrame\n",
    "for (firm_name, year), value in results.items():\n",
    "    column_name = f\"New joining work experience {year}\"\n",
    "    df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, column_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16076c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613ae68",
   "metadata": {},
   "source": [
    "#### Feature 6: Fluctuation rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66cde5",
   "metadata": {},
   "source": [
    "The employee turnover rate is defined as follows: Number of employee departures divided by the average number of employees multiplied by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ad5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2014, 2019)\n",
    "\n",
    "# Iterate over each year\n",
    "for year in years:\n",
    "    # Create the column name for the fluctuation rate\n",
    "    column_name = f\"Fluctuation rate {year}\"\n",
    "    \n",
    "    # Calculate the fluctuation rate for the current year\n",
    "    # by dividing the number of notices by the number of employees and multiplying by 100\n",
    "    df_waf[column_name] = (df_waf[f\"Number of notices {year}\"] / df_waf[f\"Number of employees {year}\"]) * 100\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d5b70",
   "metadata": {},
   "source": [
    "As the number of employees registered on LinkedIn does not correspond to the total number of employees, this figure should be viewed with caution. Calculation correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48cad0",
   "metadata": {},
   "source": [
    "#### Feature 7: Average average length of service with the company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2c217",
   "metadata": {},
   "source": [
    "The average work experience can be calculated by restricting the company dataframe in the column Firm_original_name to the searched company. Then the sum of the entries in the column duration is calculated and divided by 365 to obtain years. This number is divided by the number of unique values in the column member_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column \"Average years of service with the company\" in the df_waf DataFrame\n",
    "df_waf[\"Average years of service with the company\"] = \"\"\n",
    "\n",
    "# Iterate over each unique company name in the \"Firm_original_name\" column of the df_waf DataFrame\n",
    "for firm_name in df_waf[\"Firm_original_name\"].unique():\n",
    "    # Check if there is a corresponding DataFrame for the company in the company_dataframes dictionary\n",
    "    dataframe_key = f\"df_{firm_name}\"\n",
    "    if dataframe_key in company_dataframes:\n",
    "        # Filter the company dataframe by the current company name in the \"Firm_original_name\" column\n",
    "        company_df = company_dataframes[dataframe_key].loc[company_dataframes[dataframe_key][\"Firm_original_name\"] == firm_name]\n",
    "        \n",
    "        # Sum the values in the \"duration\" column of the filtered list\n",
    "        duration_sum = company_df[\"duration\"].sum()\n",
    "        \n",
    "        # Divide the sum by 365 to get an intermediate result\n",
    "        intermediate_result = duration_sum / 365\n",
    "        \n",
    "        # Get the number of unique values in the \"member_id\" column of the filtered list\n",
    "        unique_member_ids = company_df[\"member_id\"].nunique()\n",
    "        \n",
    "        # Check if unique_member_ids is greater than 0 to avoid division by zero\n",
    "        if unique_member_ids > 0:\n",
    "            # Calculate the final result by dividing the intermediate result by the number of unique member IDs\n",
    "            final_result = intermediate_result / unique_member_ids\n",
    "        else:\n",
    "            # Set a default value for the final result if unique_member_ids is 0\n",
    "            final_result = 0.0  # Change this to any appropriate default value\n",
    "        \n",
    "        # Write the final result into the \"Average years of service with the company\" column for the current company row\n",
    "        df_waf.loc[df_waf[\"Firm_original_name\"] == firm_name, \"Average years of service with the company\"] = final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd100fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c8069",
   "metadata": {},
   "source": [
    "Company affiliations were validated using Ferrioval S.A. as an example. Based on the dataframe, 3.2 years of average work experience were also calculated in Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f350e",
   "metadata": {},
   "source": [
    "#### Feature 8: Employees who worked for the company more than once or in diffrent positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Iterate over each company name\n",
    "for firm_name in df_waf['Firm_original_name']:\n",
    "    # Step 2: Check whether a corresponding DataFrame is available\n",
    "    if f'df_{firm_name}' in company_dataframes:\n",
    "        # Step 3: Filter the DataFrame by company name\n",
    "        filtered_df = company_dataframes[f'df_{firm_name}'][company_dataframes[f'df_{firm_name}']['Firm_original_name'] == firm_name]\n",
    "        \n",
    "        # Step 4: Count the number of member_ids that occur more than once\n",
    "        count = filtered_df['member_id'].duplicated().sum()\n",
    "        \n",
    "        # Step 5: Insert the result of the counter into df_waf\n",
    "        df_waf.loc[df_waf['Firm_original_name'] == firm_name, 'More than once/different position'] = count\n",
    "\n",
    "df_waf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce1ce2",
   "metadata": {},
   "source": [
    "1060 can be validated as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83318602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory\n",
    "directory_path = r'C:\\Users\\wildn\\Downloads\\Master'\n",
    "\n",
    "# Name\n",
    "filename = 'df_waf.tsv'\n",
    "\n",
    "# Create the full file path\n",
    "tsv_filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "# Save the DataFrame as a TSV file\n",
    "df_waf.to_csv(tsv_filepath, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf_final = df_waf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['2014', '2015', '2016', '2017', '2018',\n",
    "                   'Notices 2014', 'Notices 2015', 'Notices 2016', 'Notices 2017', 'Notices 2018',\n",
    "                   'New joiners 2014', 'New joiners 2015', 'New joiners 2016', 'New joiners 2017', 'New joiners 2018']\n",
    "\n",
    "df_waf_final.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory\n",
    "directory_path = r'C:\\Users\\wildn\\Downloads\\Master'\n",
    "\n",
    "# Namw\n",
    "filename = 'df_waf_final.csv'\n",
    "\n",
    "# Create the full file path\n",
    "csv_filepath = os.path.join(directory_path, filename)\n",
    "\n",
    "# Save the DataFrame as CSV with the chosen separator and adjusted decimal separator\n",
    "df_waf_final.to_csv(csv_filepath, sep=';', index=False, decimal=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e878099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_waf_final.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to a specific cell in the column \"column_name\" and row 0\n",
    "value = df_waf.loc[1, \"Employee development 2016\"]\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b261ffca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
