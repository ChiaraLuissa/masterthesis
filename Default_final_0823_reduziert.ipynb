{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cffb9b",
   "metadata": {},
   "source": [
    "# 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b846d",
   "metadata": {},
   "source": [
    "Business valuations are crucial for a diverse range of stakeholders, guiding capital allocation decisions based on precise assessments of companies' economic performance, regardless of whether the trends are positive, steady, or negative. The potential risks of both overly negative valuations, misinterpreting positive trends, and overlooking negative developments are equally significant. Such misjudgments can impede a company's refinancing options, lead to missed investment prospects for investors, and result in financial losses. In the following code, we evaluate how aggregated features from LinkedIn help to improve the quality of prediction of a default. Three data frames are used - financial metrics only, LinkedIn metrics only and both combined. The evaluation is considered successful if a positive influence of the LinkedIn features on the prediction can be determined. AUC and recall are considered particularly relevant metrics. Details can be found in the Data chapter of the corresponding master thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210de2e",
   "metadata": {},
   "source": [
    "# 2. Load data and prepare libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05453d2",
   "metadata": {},
   "source": [
    "With the use of Chat GPD, comments have been added for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bda71",
   "metadata": {},
   "source": [
    "## 2.1 Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from prettytable import PrettyTable\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree, XGBClassifier, XGBRegressor\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ac0d6",
   "metadata": {},
   "source": [
    "## 2.2 Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8429086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\List of companys_onetemplate.xls'\n",
    "df_up = pd.read_excel(dateipfad)\n",
    "df_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2ffcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\df_waf_final.csv'\n",
    "df_waf_rfm = pd.read_csv(dateipfad, sep=';')\n",
    "df_waf_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378abf09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_waf_rfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af4603",
   "metadata": {},
   "source": [
    "Author knowledge: in the generation of df_waf_rfm initialisation values were used. They are droped from the dataframe before the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5268b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtere die Zeilen mit dem Wert 1 in der Spalte \"Number of Employee 2014\"\n",
    "filtered_df = df_waf_rfm[df_waf_rfm['Number of employees 2014'] == 1]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf_rfm = df_waf_rfm[df_waf_rfm['Number of employees 2014'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630eb878",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Matching.csv'\n",
    "df_match = pd.read_csv(dateipfad, sep=';')\n",
    "df_match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618db2c",
   "metadata": {},
   "source": [
    "## 2.3 Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf39896",
   "metadata": {},
   "source": [
    "Cleaning Company name to make a match with firm_original_name possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy the column \"Company Name\" to the new column \"Copy_Company_Name\" in DataFrame df_up\n",
    "df_up['Copy_Company_Name'] = df_up['Company Name']\n",
    "\n",
    "# Remove values in parentheses from the \"Company Name\" column in DataFrame df_up\n",
    "df_up['Company Name'] = df_up['Company Name'].apply(lambda x: re.sub(r'\\(.*\\)', '', str(x)).strip())\n",
    "\n",
    "# Print the DataFrame df_up after the modifications\n",
    "df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19c9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for cleaning Company Name by removing non-alphanumeric characters\n",
    "def clean_company_name(name):\n",
    "    return re.sub(r'[^\\w\\s]', '', str(name))\n",
    "\n",
    "# Clean the Company Name column using the clean_company_name function in DataFrame df_up\n",
    "df_up['Company Name'] = df_up['Company Name'].apply(clean_company_name)\n",
    "df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58e14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for fuzzy matching to find the best match for each company name\n",
    "def find_best_match(company_name, reference_names):\n",
    "    best_match = None\n",
    "    best_similarity = 0\n",
    "\n",
    "    for ref_name in reference_names:\n",
    "        similarity = fuzz.token_set_ratio(company_name, ref_name)\n",
    "        if similarity > best_similarity:\n",
    "            best_match = ref_name\n",
    "            best_similarity = similarity\n",
    "\n",
    "    # Threshold for similarity score (adjust as needed)\n",
    "    threshold_similarity = 95\n",
    "\n",
    "    # Return the best match if similarity score is above the threshold, else return None\n",
    "    return best_match if best_similarity >= threshold_similarity else None\n",
    "\n",
    "# Create an empty list to store the matched companies\n",
    "matched_companies = []\n",
    "\n",
    "# Extract company names from df_up\n",
    "company_names_up = df_up['Company Name'].tolist()\n",
    "\n",
    "# Extract firm_original_names from df_waf_rfm\n",
    "firm_original_names_waf = df_waf_rfm['Firm_original_name'].tolist()\n",
    "\n",
    "# Iterate over the company names in df_up\n",
    "for company_name_up in company_names_up:\n",
    "    # Find the best match for the current company name in df_up within df_waf_rfm\n",
    "    best_match_waf = find_best_match(company_name_up, firm_original_names_waf)\n",
    "    \n",
    "    # Append the match result to the matched_companies list\n",
    "    matched_companies.append((company_name_up, best_match_waf))\n",
    "\n",
    "# Convert the matched_companies list to a DataFrame\n",
    "results_matching = pd.DataFrame(matched_companies, columns=['Company Name Up', 'Best Match in df_waf_rfm'])\n",
    "\n",
    "# Display the results\n",
    "results_matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c534673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of entries where the Best Match is None in results_matching\n",
    "num_none_matches = results_matching['Best Match in df_waf_rfm'].isna().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of entries with 'None' in Best Match:\", num_none_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24638ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge df_up with df_waf_rfm using the results_matching as the merge key\n",
    "df_up_merged = pd.merge(df_up, results_matching, left_on='Company Name', right_on='Company Name Up', how='left')\n",
    "\n",
    "# Step 2 and 3: Iterate over the Company Names in df_up and search in results_matching\n",
    "for index_up, row_up in df_up.iterrows():\n",
    "    company_name_up = row_up['Company Name']\n",
    "    \n",
    "    # Step 4: Check if the Company Name in results_matching is None\n",
    "    best_match_waf = results_matching.loc[results_matching['Company Name Up'] == company_name_up, 'Best Match in df_waf_rfm'].values[0]\n",
    "    if pd.isna(best_match_waf):\n",
    "        # Step 5: If None is found, fill None in the previously added columns from df_waf_rfm\n",
    "        df_up_merged.loc[index_up, df_waf_rfm.columns] = None\n",
    "    else:\n",
    "        # Step 6: If a match is found, extract the row from df_waf_rfm and merge the entries to df_up_merged\n",
    "        row_waf = df_waf_rfm.loc[df_waf_rfm['Firm_original_name'] == best_match_waf]\n",
    "        df_up_merged.loc[index_up, df_waf_rfm.columns] = row_waf.values[0]\n",
    "\n",
    "df_up_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93847f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the number of entries where the Number of employees 2014 is NaN in df_up_merged\n",
    "num_nan_employees = df_up_merged['Number of employees 2014'].isna().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of entries with NaN in Number of employees 2014:\", num_nan_employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in the \"Number of employees 2014\" column in df_up_merged\n",
    "df_up_merged.dropna(subset=['Number of employees 2014'], inplace=True)\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df_up_merged.reset_index(drop=True, inplace=True)\n",
    "df_up_merged.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefe503",
   "metadata": {},
   "source": [
    "Matching was successful in df_up_merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c963b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values searched\n",
    "gesuchte_werte = ['Hovnanian', 'Community Health Systems', 'Denbury Inc', 'WESTMORELAND COAL CO',\n",
    "                  'ICONIX BRAND GROUP INC', 'NORTHERN OIL & GAS INC', 'SEARS HOLDINGS CORP',\n",
    "                  'PARKER DRILLING CO', 'PG&E CORP', 'CLOUD PEAK ENERGY INC', 'PHI INC',\n",
    "                  'BRISTOW GROUP INC', 'WEATHERFORD INTL PLC', 'ALTA MESA RESOURCES INC',\n",
    "                  'CHESAPEAKE ENERGY CORP', 'EP ENERGY CORP', 'RITE AID CORP',\n",
    "                  'DESTINATION MATERNITY CORP', 'DEAN FOODS CO', 'MALLINCKRODT PLC',\n",
    "                  'FRONTIER COMMUNIC PARENT INC', 'LSC COMMUNICATIONS INC',\n",
    "                  'DIAMOND OFFSHRE DRILLING INC', 'ENVISION HEALTHCARE CORP', 'UNIT CORP',\n",
    "                  'TUESDAY MORNING CORP', 'CSI COMPRESSCO LP', 'FERRELLGAS PARTNERS -LP',\n",
    "                  'W&T OFFSHORE INC', 'TUPPERWARE BRANDS CORP', 'SEADRILL LTD',\n",
    "                  'GLOBAL EAGLE ENTERTAINMENT', 'FORUM ENERGY TECH INC', 'TRANSOCEAN LTD',\n",
    "                  'TOWN SPORTS INTL HOLDINGS', 'SUMMIT MIDSTREAM PARTNERS LP',\n",
    "                  'GULFPORT ENERGY CORP', 'NABORS INDUSTRIES LTD', 'PACIFIC DRILLING SA',\n",
    "                  'CALLON PETROLEUM CO/DE']\n",
    "\n",
    "# Check if the values in the column \"Company Name\" are included\n",
    "gesuchte_werte_in_df = df_up_merged[df_up_merged['Company Name'].isin(gesuchte_werte)]\n",
    "\n",
    "# Print\n",
    "gesuchte_werte_in_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f55ad1",
   "metadata": {},
   "source": [
    "# 3. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e2a5b",
   "metadata": {},
   "source": [
    "During data preperation, the data are first examined in general (3.1). Then missing values (3.1.1), duplicates (3.1.2), non-numerical columns (3.1.4) and the distribution of the target variable are checked (3.1.5). Due to the data type, data outliers can only be checked downstream. Therefore, an initial data cleaning is carried out in 3.2. In the course of this, columns that are not needed are removed (3.2.1), the data type is corrected (3.2.2), the column country (3.2.3) and industry (3.2.4) are cleaned. On this basis, the data outliers can be examined in 3.3.1. Subsequently, the content-related data distribution is checked (3.3.2, 3.3.3). The final data cleaning is carried out in chapter 3.4. Values that are not to be taken into account are removed (3.4.1, 3.4.2), encoding takes place where necessary (3.4.3), empty values are treated (3.4.4). Finally, collinarity and multicollinarity are checked (3.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5dfed0",
   "metadata": {},
   "source": [
    "## 3.1 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae0997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_up_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75170f8e",
   "metadata": {},
   "source": [
    "The following column do not add value to the context and are therefor not needed:\n",
    "- Adress\n",
    "- S&P Entity ID\n",
    "- Excel Company ID\n",
    "- Index Constituents [Secondary Listings]\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)\n",
    "- &P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (Rating Date)\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)\t\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch Date)\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Outlook)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Outlook Date)\n",
    "- the author decided to focus on the timeseries 2014-2018. Therefore the values for 2013 and >2018 can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547be9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the size and scope of the data set\n",
    "print('The dataset has {} rows and {} columns. This results in {} data entries.'.format(df_up_merged.shape[0],df_up_merged.shape[1], df_up_merged.size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b9d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze the data types of columns in df_up_merged\n",
    "column_data_types = df_up_merged.dtypes\n",
    "\n",
    "# Set the option to display all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the result\n",
    "print(\"Data Types of Columns in df_up_merged:\")\n",
    "print(column_data_types.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684223dd",
   "metadata": {},
   "source": [
    "Apart from the first 11 columns, the other entries are numbers. These must be converted into float values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b935a1",
   "metadata": {},
   "source": [
    "### 3.1.1 Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check for null values in df_up_merged\n",
    "null_counts = df_up_merged.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of null values in each column of df_up_merged:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f062c4",
   "metadata": {},
   "source": [
    "The following columns contain highest Number of None/NaN fields that need to be cleaned:\n",
    "- Gross Profit/ Employee 2018\n",
    "- All Rating and changes in Rating\n",
    "- Consider dropping companys that have missing values in financials.\n",
    "\n",
    "Columns are included that are no longer needed and contain some empty values. These are: \n",
    "- Rating 2018 ALT (author knows that the column offers no professional added value)\n",
    "- Rating 2012\n",
    "- Change 2012/2013 (does not concern analysis period)\n",
    "- New joining work experience 2014 (empty)\n",
    "- Number of Notices 2018\n",
    "- Number of notices 2018\n",
    "- Number of New Joiners 2014 (authors knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb8ef0",
   "metadata": {},
   "source": [
    "### 3.1.2 Checking for dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337a543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicates = df_up_merged[df_up_merged.duplicated()]\n",
    "print(\"Duplicate Rows : \",len(duplicates))\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945805f",
   "metadata": {},
   "source": [
    "As expected there are no dublicates in this dataframe. No cleaning nessercary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886018aa",
   "metadata": {},
   "source": [
    "### 3.1.3 Checking for data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8517c",
   "metadata": {},
   "source": [
    "The checking for data outliers is done later in this notebook. Most columns needs to be converted to a processable formate for numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02382589",
   "metadata": {},
   "source": [
    "### 3.1.4 Inspecting non-numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22889dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_up_merged['Geographic Region'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_region_values = df_up_merged['Geographic Region'].unique()\n",
    "unique_region_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d611f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged['Land'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba847a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_country_values = df_up_merged['Land'].unique()\n",
    "unique_country_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d23ee",
   "metadata": {},
   "source": [
    "Correlation between Geografic Region an Country expected. Geographic Region contains less information and should be droped if needed. Values in country needs to be cleaned since there are the same letter in capital and small letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2495e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_up_merged['Exchange'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_exchange_values = df_up_merged['Exchange'].unique()\n",
    "unique_exchange_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c312cfc",
   "metadata": {},
   "source": [
    "INFO: \n",
    "- OM: Nasdaq OMX Nordic, a stock exchange in Sweden, Denmark, Finland, and Iceland;\n",
    "- SWX: SIX Swiss Exchange, the Swiss stock exchange;\n",
    "- NYSE: New York Stock Exchange, the stock exchange in New York City, USA;\n",
    "- ENXTPA: Euronext Paris, the French stock exchange;\n",
    "- NasdaqGS: Nasdaq Global Select Market, a US-based stock exchange, part of the Nasdaq Stock Market;\n",
    "- XTRA: Frankfurt Stock Exchange, the stock exchange in Frankfurt, Germany;\n",
    "- ENXTAM: Euronext Amsterdam, the Dutch stock exchange;\n",
    "- BME: Bolsas y Mercados Españoles, the stock exchange in Spain;\n",
    "- LSE: London Stock Exchange, the stock exchange in London, United Kingdom;\n",
    "- ENXTBR: Euronext Brussels, the stock exchange in Belgium;\n",
    "- BIT: Borsa Italiana, the stock exchange in Italy;\n",
    "- ISE: Irish Stock Exchange, the stock exchange in Ireland;\n",
    "- CPSE: Euronext Lisbon, the stock exchange in Portugal;\n",
    "- WBAG: Wiener Börse AG, the stock exchange in Austria;\n",
    "- OB: Oslo Børs, the stock exchange in Norway;\n",
    "- HLSE: Helsinki Stock Exchange, the stock exchange in Finland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of Tickers. They act as unique identifier per company and should be kept. \n",
    "unique_Ticker_count = df_up_merged['Ticker'].nunique()\n",
    "unique_Ticker_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a3ab7",
   "metadata": {},
   "source": [
    "No cleaning of column ticker needed from a subject specific point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fcefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_industry_values = df_up_merged['S&P RatingsDirect® Industry'].unique()\n",
    "unique_industry_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678a2e6",
   "metadata": {},
   "source": [
    "Cleaning tasks: \"Corporates; Industrials\" is a pre configuration and can be droped. The main industry following in the breakdown is the intresting one and needs to be keept. All the other information are considered details and should be droped. Also rename the column to \"Industry\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for unique values\n",
    "columns_to_check = ['Rating 2014', 'Rating 2015', 'Rating 2016', 'Rating 2017', 'Rating 2018', 'Rating 2019']\n",
    "\n",
    "for column in columns_to_check:\n",
    "    # Get the unique values in the specified column\n",
    "    unique_values = df_up_merged[column].unique()\n",
    "\n",
    "    # Print the unique values for the current column\n",
    "    print(\"Unique values for \" + column + \":\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775cb60",
   "metadata": {},
   "source": [
    "Variables need to be converted to kategorial features to use them in futher analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5787e",
   "metadata": {},
   "source": [
    "###  3.1.5 Checking for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for unique values\n",
    "columns_to_check = [\"Default\"]\n",
    "\n",
    "for column in columns_to_check:\n",
    "    # Get the unique values in the specified column\n",
    "    unique_values = df_up_merged[column].unique()\n",
    "\n",
    "    # Print the unique values for the current column\n",
    "    print(\"Unique values for \" + column + \":\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_default_count = df_up_merged[\"Default\"].value_counts()[0]\n",
    "default_count = df_up_merged[\"Default\"].value_counts()[1]\n",
    "\n",
    "print(\"Number of healthy companies:\", No_default_count)\n",
    "print(\"Number of default dataset:\", default_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b4191",
   "metadata": {},
   "source": [
    "Poor database of default - common in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea98",
   "metadata": {},
   "source": [
    "## 3.2 First data cleansing to enable deeper Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f657d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy for better work contorl\n",
    "df_up = df_up_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2447a",
   "metadata": {},
   "source": [
    "### 3.2.1 Droping columns that are not needed or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216dc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\n",
    "    \"Rating 2018 ALT\",\n",
    "    \"Rating 2012\",\n",
    "    \"Change 2012/13\",\n",
    "    \"New joining work experience 2014\",\n",
    "    \"Migrating work experience 2018\",\n",
    "    \"Number of Notices 2018\",\n",
    "    \"Number of notices 2018\", \n",
    "    \"Number of New Joiners 2014\",\n",
    "    'Adress',\n",
    "    'S&P Entity ID',\n",
    "    'Excel Company ID',\n",
    "    'Index Constituents [Secondary Listings]',\n",
    "    'Index Constituents [Primary Listing]',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (Rating Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Outlook)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Outlook Date)',\n",
    "    \"Market Capitalization [12/31/2013] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2019] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2020] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2013]\",\n",
    "    \"Total Employees - Capital IQ [CY 2019]\",\n",
    "    \"Total Employees - Capital IQ [CY 2020]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Copy_Company_Name\", \"Company Name Up\", \"Best Match in df_waf_rfm\", \"Firm_original_name\",\n",
    "    \"Rating 2020\",\n",
    "    \"Rating 2021\",\n",
    "    \"Rating 2022\",\n",
    "    \"Rating 2023\",\n",
    "    \"Change 2019/20\",\n",
    "    \"Change 2020/21\",\n",
    "    \"Change 2021/22\",\n",
    "    \"Change 2022/23\"\n",
    "]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "df_up.drop(columns=columns_to_remove, inplace=True)\n",
    "df_up.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2246e34",
   "metadata": {},
   "source": [
    "### 3.2.2 Converting columns from object to float. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0dafc6",
   "metadata": {},
   "source": [
    "First there is a need to check for special characters (spaces, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_special_characters(df, columns_to_check):\n",
    "    pattern = re.compile(r'[^\\w\\s.]')  # define a pattern for special characters (everything except letters, numbers, spaces and full stops)\n",
    "    result = []\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        for index, value in df[column].items():\n",
    "            if re.search(pattern, str(value)):\n",
    "                result.append((index, column, value))\n",
    "\n",
    "    if result:\n",
    "        print(\"Folgende Sonderzeichen wurden gefunden:\")\n",
    "        for row in result:\n",
    "            print(f\"Row {row[0]}, Column {row[1]}, Value: {row[2]}\")\n",
    "    else:\n",
    "        print(\"Keine Sonderzeichen in den angegebenen Spalten gefunden.\")\n",
    "\n",
    "\n",
    "columns_to_check = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",]\n",
    "\n",
    "check_for_special_characters(df_up, columns_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b6732",
   "metadata": {},
   "source": [
    "There are negative numbers, kommas and also empty fields indicated by -. This charackters needs to be cleaned. \n",
    "Next it must be taken into account whether whole numbers are present or if we decimal numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d39b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for integers in columns\n",
    "def check_for_integers(df, columns_to_check):\n",
    "    integer_columns = []\n",
    "    for column in columns_to_check:\n",
    "        is_integer = df[column].apply(lambda x: str(x).isdigit()).all()\n",
    "        if is_integer:\n",
    "            integer_columns.append(column)\n",
    "    return integer_columns\n",
    "\n",
    "columns_to_convert = [   ]  \n",
    "integer_columns = check_for_integers(df_up, columns_to_convert)\n",
    "\n",
    "if integer_columns:\n",
    "    print(\"The following columns contain integers:\")\n",
    "    print(integer_columns)\n",
    "else:\n",
    "    print(\"No columns with only integers were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8b197",
   "metadata": {},
   "source": [
    "Columns can be converted to float, since dataset only contains dicomal numbers. Last the decimal separator is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [     \n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",]  \n",
    "\n",
    "def check_comma_or_dot(df, columns):\n",
    "    comma_columns = []\n",
    "    dot_columns = []\n",
    "\n",
    "    for column in columns:\n",
    "        if df[column].str.contains(',').any():\n",
    "            comma_columns.append(column)\n",
    "        elif df[column].str.contains('.').any():\n",
    "            dot_columns.append(column)\n",
    "\n",
    "    return comma_columns, dot_columns\n",
    "\n",
    "comma_columns, dot_columns = check_comma_or_dot(df_up, columns_to_convert)\n",
    "\n",
    "print(\"Spalten mit Komma:\")\n",
    "print(comma_columns)\n",
    "\n",
    "print(\"Spalten mit Punkt:\")\n",
    "print(dot_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086b1a3",
   "metadata": {},
   "source": [
    "To convert successful equal decimal seperators needs to be used. Therefor kommas are replaced by points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [\"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",]\n",
    "\n",
    "# Replace commas with dots in the relevant columns\n",
    "for column in columns_to_convert:\n",
    "    df_up[column] = df_up[column].str.replace(',', '.')\n",
    "\n",
    "# Print\n",
    "df_up.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943c31b",
   "metadata": {},
   "source": [
    "The - accounting fo an empty value are converted to NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633dff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some columns there are - as empty values. Those need to be replaced bevor we can convert to float.\n",
    "columns_to_convert = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "# Replace the \"-\" character with NaN (Not-a-Number) in the relevant columns\n",
    "for column in columns_to_convert:\n",
    "    df_up[column] = df_up[column].replace('-', float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571880c9",
   "metadata": {},
   "source": [
    "Lastly the columns can be converted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba133c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to convert to float and replace \"object\" values with NaN\n",
    "columns_to_convert = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",\n",
    "]\n",
    "\n",
    "def convert_to_float_with_negatives(value):\n",
    "    try:\n",
    "        # Attempts to convert the value to a float\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        # If the value cannot be converted into a float (e.g. if there is a minus sign in front of a number), return the value unchanged\n",
    "        return value\n",
    "\n",
    "# Convert the columns to the data type \"float\" and keep the negative values\n",
    "for column in columns_to_convert:\n",
    "    df_up[column] = df_up[column].apply(convert_to_float_with_negatives)\n",
    "\n",
    "# Print\n",
    "df_up.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7215c",
   "metadata": {},
   "source": [
    "Check if converting was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe68f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze the data types of columns in df_up_merged\n",
    "column_data_types = df_up.dtypes\n",
    "\n",
    "# Set the option to display all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the result\n",
    "print(\"Data Types of Columns in df_up:\")\n",
    "print(column_data_types.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee197f",
   "metadata": {},
   "source": [
    "### 3.2.3 Cleaning column country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878dfaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only capital letters\n",
    "df_up[\"Land\"] = df_up[\"Land\"].str.upper()\n",
    "\n",
    "# checking unique values\n",
    "unique_land_values = df_up[\"Land\"].unique()\n",
    "unique_land_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only capital letters\n",
    "df_up[\"Land\"] = df_up[\"Land\"].str.upper()\n",
    "\n",
    "# Convert the 'Land' column to string data type\n",
    "df_up[\"Land\"] = df_up[\"Land\"].astype(str)\n",
    "\n",
    "# checking unique values\n",
    "unique_land_values = df_up[\"Land\"].unique()\n",
    "unique_land_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4279a0e",
   "metadata": {},
   "source": [
    "### 3.2.4 Unify values in Industies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up.rename(columns={\"S&P RatingsDirect® Industry\": \"Industry\"}, inplace=True)\n",
    "df_up.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove \"Corporates; Industrials;\" from the entries in the \"Industry\" column\n",
    "df_up['Industry'] = df_up['Industry'].str.replace('Corporates; Industrials;', '', regex=False)\n",
    "\n",
    "# Step 2: Remove all words after the first semicolon in the \"Industry\" column\n",
    "df_up['Industry'] = df_up['Industry'].str.split(';').str[0]\n",
    "\n",
    "# Display unique values in the \"Industry\" column\n",
    "unique_industries = df_up['Industry'].unique()\n",
    "unique_industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c08508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy for better work control / df_pp = pre proccesed\n",
    "df_pp = df_up.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87539987",
   "metadata": {},
   "source": [
    "### 3.3.1 Checking for data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f821ff",
   "metadata": {},
   "source": [
    "Note: Data outliers are checked in gruops to confirm, that there are no obvious errors in the data. Due to the nature of the domain it is not absolutly nessercary to clean the outliers - espacially since the source of the financials is Bloomberg, wich accounts as a reliable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Market Capitalization Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26972b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"EBITDA Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeada4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"EBIT Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Net Income Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5418917",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Equity Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "\t\"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Debt Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92eb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Assets Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Debt Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Gross Profit Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Employees Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Cash from Ops Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Revenue Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f221b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    " \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Employee development Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d65249",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "  \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Migrating work experience Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a3b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"New joining work experience Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "  \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Fluctuation rate Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20732917",
   "metadata": {},
   "source": [
    "Form a statistical point of view I would use at least a 98 % quantil.From a professional point of view most data outliers make sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cb323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "\"More than once/different position\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Serveral positions Boxplots\")\n",
    "plt.ylabel(\"Number of people who worked there in more than one position\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be019e5",
   "metadata": {},
   "source": [
    "### 3.3.2 Data allocation with respect to the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a89bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the entries are distributed among the properties of the target variable. \n",
    "df_pp['Default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e13167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value counts of the target variable\n",
    "value_counts = df_pp['Default'].value_counts()\n",
    "\n",
    "# Extract the values and labels for the pie chart\n",
    "labels = ['1', '0']\n",
    "values = [value_counts.get(1, 0), value_counts.get(0, 0)]\n",
    "\n",
    "# Define the explode parameter for the pie chart\n",
    "explode = [0.1, 0]\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.2f%%', explode=explode)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.\n",
    "plt.title('Distribution of Change_2018/19_down grade')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff0034",
   "metadata": {},
   "source": [
    "After Split in test & train data the train data should be oversampled using the SMOTE technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe07a6",
   "metadata": {},
   "source": [
    "### 3.3.3 Checking distribution in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0d8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each country in the 'Land' column\n",
    "country_counts = df_pp['Land'].value_counts()\n",
    "\n",
    "# Create a pie chart to visualize the distribution\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(country_counts, labels=country_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution of Companies by Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf265eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_land_values = df_pp['Land'].nunique()\n",
    "print(\"Unique values:\", unique_land_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f755b3b",
   "metadata": {},
   "source": [
    "Most companies are from the USA. Second biggest group is GB, followed by Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each country in the 'Land' column\n",
    "country_counts = df_pp['Industry'].value_counts()\n",
    "\n",
    "# Create a DataFrame to store the counts and percentage\n",
    "country_distribution = pd.DataFrame({'Industry': country_counts.index, 'Count': country_counts.values})\n",
    "\n",
    "# Calculate the percentage of each country in the 'Land' column\n",
    "total_countries = len(df_pp['Industry'])\n",
    "country_distribution['Percentage'] = (country_distribution['Count'] / total_countries) * 100\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "country_distribution = country_distribution.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the tabular view of the distribution\n",
    "print(country_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4468837",
   "metadata": {},
   "source": [
    "Insurances should not be contained and need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentages for different years\n",
    "def calculate_percentage(row, year):\n",
    "    total_employees = row[\"Total Employees - Capital IQ [CY %d]\" % year]\n",
    "    employees = row[\"Number of employees %d\" % year]\n",
    "    \n",
    "    # Calculate percentage if not NaN and total employees is not zero\n",
    "    percentage = (employees / total_employees) * 100 if (not pd.isna(total_employees) and total_employees != 0) else None\n",
    "    \n",
    "    return percentage\n",
    "\n",
    "# List of years to calculate percentages for\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "# Calculate percentages for each year and apply the function to the DataFrame rows\n",
    "for year in years:\n",
    "    col_name = \"Percentage of employees on Linkedin %d\" % year\n",
    "    df_pp[col_name] = df_pp.apply(calculate_percentage, axis=1, args=(year,))\n",
    "\n",
    "# Displaying the results\n",
    "output_df = df_pp[[\"Company Name\"] + [\"Percentage of employees on Linkedin %d\" % year for year in years]]\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for percentage ranges\n",
    "bins = [0, 5, 10, 15, 20, float('inf')]  # The last bin represents 20% or more\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['<5%', '5-10%', '10-15%', '15-20%', '20%+']\n",
    "\n",
    "# Create a new column with bins\n",
    "output_df['Percentage Range 2017'] = pd.cut(output_df['Percentage of employees on Linkedin 2017'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "percentage_counts = output_df['Percentage Range 2017'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34348705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for percentage ranges\n",
    "bins = [0, 5, 10, 15, 20, float('inf')]  # The last bin represents 20% or more\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['<5%', '5-10%', '10-15%', '15-20%', '20%+']\n",
    "\n",
    "# Create a new column with bins\n",
    "output_df['Percentage Range 2018'] = pd.cut(output_df['Percentage of employees on Linkedin 2018'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "percentage_counts = output_df['Percentage Range 2018'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61d413",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter the rows in \"df_pp\" where the value in \"Change_2018/19_down grade\" column is 1\n",
    "filtered_rows = df_pp[df_pp['Default'] == 1]\n",
    "\n",
    "# Extract the \"Company Name\" from the filtered rows\n",
    "company_names = filtered_rows['Company Name']\n",
    "\n",
    "# Filter and display the corresponding rows in \"output_df\" based on the \"Company Name\" values\n",
    "result_df = output_df[output_df['Company Name'].isin(company_names)]\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for percentage ranges\n",
    "bins = [0, 5, 10, 15, 20, float('inf')]  # The last bin represents 20% or more\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['<5%', '5-10%', '10-15%', '15-20%', '20%+']\n",
    "\n",
    "# Create a new column with bins\n",
    "result_df['Percentage Range 2018'] = pd.cut(result_df['Percentage of employees on Linkedin 2018'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "percentage_counts = result_df['Percentage Range 2018'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ccbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['Percentage of employees on Linkedin 2018', 'Percentage of employees on Linkedin 2014', 'Percentage of employees on Linkedin 2015', 'Percentage of employees on Linkedin 2016', 'Percentage of employees on Linkedin 2017']\n",
    "df_pp = df_pp.drop(columns_to_remove, axis=1)\n",
    "df_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eeee08",
   "metadata": {},
   "source": [
    "Defaultet companys show a rather low percentage of employees on LinkedIn. Distribution doesnt change over the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcd124",
   "metadata": {},
   "source": [
    "Majority of the companies has a percentage below 5%. Take into account, that there are no employee numbers for around 60-70 companies, wich results in 0. The reduction can be explained by the general reduction in the data in 2018. The LinkedIn dataset was probably retrieved during 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa759bb1",
   "metadata": {},
   "source": [
    "## 3.4 Final data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp2 = df_pp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2dee3a",
   "metadata": {},
   "source": [
    "### 3.4.1 Cleaning data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22478a73",
   "metadata": {},
   "source": [
    "This feature counts the numberr of people who have worked in diffrent position in the company. Even though 120.000 might be realistic in bigger firms, it is cleaned here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_pp2 is your DataFrame\n",
    "quantile_98 = df_pp2['More than once/different position'].quantile(0.98)\n",
    "\n",
    "# Filter the DataFrame to keep only values up to the 98% quantile\n",
    "df_pp2 = df_pp2[df_pp2['More than once/different position'] <= quantile_98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "\"More than once/different position\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp2[selected_columns].boxplot()\n",
    "plt.title(\"Serveral positions Boxplots\")\n",
    "plt.ylabel(\"Number of people who worked there in more than one position\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc300a9",
   "metadata": {},
   "source": [
    "### 3.4.2 Removing Industry Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_pp2 is your DataFrame\n",
    "df_pp2 = df_pp2[df_pp2['Industry'] != 'Insurance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each country in the 'Land' column\n",
    "country_counts = df_pp2['Industry'].value_counts()\n",
    "\n",
    "# Create a DataFrame to store the counts and percentage\n",
    "country_distribution = pd.DataFrame({'Industry': country_counts.index, 'Count': country_counts.values})\n",
    "\n",
    "# Calculate the percentage of each country in the 'Land' column\n",
    "total_countries = len(df_pp2['Industry'])\n",
    "country_distribution['Percentage'] = (country_distribution['Count'] / total_countries) * 100\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "country_distribution = country_distribution.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the tabular view of the distribution\n",
    "print(country_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3150e4d",
   "metadata": {},
   "source": [
    "Removal of Incurance successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af11bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pp2['Industry'].dtypes)\n",
    "#covnert to 'str' (String)\n",
    "df_pp2['Industry'] = df_pp2['Industry'].astype(str)\n",
    "print(df_pp2['Industry'].dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fafc6d",
   "metadata": {},
   "source": [
    "Checking effect on target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how the entries are distributed among the properties of the target variable. \n",
    "df_pp['Default'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767c36e",
   "metadata": {},
   "source": [
    "### 3.4.3 Putting Rating in kategorial values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1ff06",
   "metadata": {},
   "source": [
    "The ratings need to be put in kategorial variables to be useful in futher analysis. Integer encoding can be used. The rating contains a score that is reflected in the ascending values. To simplify and uniy the values a aggregated skala is being used:\n",
    "AAA 1\n",
    "AA 1\n",
    "A 2\n",
    "BBB 3\n",
    "BB 4\n",
    "B 5\n",
    "CCC 6\n",
    "CC 6\n",
    "C 6\n",
    "D 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteriere über die Spalten und gib die eindeutigen Werte aus\n",
    "for year in range(2013, 2019):\n",
    "    column_name = f'Rating {year}'\n",
    "    unique_values = df_pp2[column_name].unique()\n",
    "    print(f'Unique values in {column_name}: {unique_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf82dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer Encoding\n",
    "# Create a dictionary to map the original values to the categorical variables\n",
    "rating_mapping = {\n",
    "    \"A-\": 1,\n",
    "    \"A\": 2,\n",
    "    \"A+\": 3,\n",
    "    \"AA\": 4,\n",
    "    \"AA-\": 5,\n",
    "    \"AA+\": 6,\n",
    "    \"B\": 7,\n",
    "    \"B+\": 8,\n",
    "    \"BB\": 9,\n",
    "    \"BB-\": 10,\n",
    "    \"BB+\": 11,\n",
    "    \"BBB\": 12,\n",
    "    \"BBB-\": 13,\n",
    "    \"BBB+\": 14,\n",
    "    \"CCC\": 15,\n",
    "    # If NaN is present in the data, it will be mapped to 0 as per your requirement\n",
    "    # You may skip this line if there are no NaN values in the columns\n",
    "    pd.NA: 0\n",
    "}\n",
    "\n",
    "# Loop through the years and convert the values in each \"Rating\" column to categorical variables\n",
    "for year in range(2013, 2020):\n",
    "    column_name = f'Rating {year}'\n",
    "    df_pp2[column_name] = df_pp2[column_name].replace(rating_mapping)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pp2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7398a",
   "metadata": {},
   "source": [
    "### 3.4.4 Handling empty entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check for null values in df_up_merged\n",
    "null_counts = df_pp2.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of null values in each column of df_up_merged:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73381e",
   "metadata": {},
   "source": [
    "The following adjustments are made: \n",
    "- Change <year>: Set \"no change\"\n",
    "- Financials: median of the column\n",
    "- Total Employees <year>: use following year or mean\n",
    "- Gross Profit/ Employee 2018: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1978d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_values = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Equity ratio 2018\",\n",
    "    \"Debt ratio (in Prozent) 2018\",\n",
    "    \"Debt-equity ratio 2018\",  \n",
    "    \"Return on equity 2018\",\n",
    "    \"Return on sales 2018\",\n",
    "]\n",
    "\n",
    "# Iterate over the selected columns\n",
    "for col in columns_with_missing_values:\n",
    "    # Identify rows with missing values (NaN or empty)\n",
    "    missing_values_mask = df_pp2[col].isnull() | (df_pp2[col] == '')\n",
    "\n",
    "    # Calculate the median value of the column excluding the missing values\n",
    "    median_value = df_pp2.loc[~missing_values_mask, col].median()\n",
    "\n",
    "    # Replace the missing values with the median value\n",
    "    df_pp2.loc[missing_values_mask, col] = median_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e23966",
   "metadata": {},
   "source": [
    "Filling the Total Employyes - either with future value or with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2015]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2016]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2016]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2017]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2018]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2014 = df_pp2['Total Employees - Capital IQ [CY 2014]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(median_employees_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2016]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2016]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2018]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2015 = df_pp2['Total Employees - Capital IQ [CY 2015]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(median_employees_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5743093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2016]'] = df_pp2['Total Employees - Capital IQ [CY 2016]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2016]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2016]'] = df_pp2['Total Employees - Capital IQ [CY 2016]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2016 = df_pp2['Total Employees - Capital IQ [CY 2016]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2016]'] = df_pp2['Total Employees - Capital IQ [CY 2016]'].fillna(median_employees_2016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2017]'] = df_pp2['Total Employees - Capital IQ [CY 2017]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2017 = df_pp2['Total Employees - Capital IQ [CY 2017]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2017]'] = df_pp2['Total Employees - Capital IQ [CY 2017]'].fillna(median_employees_2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0263f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"Total Employees - Capital IQ [CY 2018]\" is filled\n",
    "if df_pp2['Total Employees - Capital IQ [CY 2018]'].notnull().any():\n",
    "    # Fill missing values in \"Total Employees - Capital IQ [CY 2018]\" with values from \"Total Employees - Capital IQ [CY 2017]\"\n",
    "    df_pp2['Total Employees - Capital IQ [CY 2018]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8aeba",
   "metadata": {},
   "source": [
    "Droping Gross Profit / Employee 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde1998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the column \"Gross Profit/ Employee 2018\" from df_pp2\n",
    "df_pp2.drop(\"Gross Profit/ Employee 2018\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cceec",
   "metadata": {},
   "source": [
    "No change is beeing set as a value for the missing change indivators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check and fill with \"no change\"\n",
    "columns_to_fill_with_no_change = [\n",
    "    'Change 2013/14',\n",
    "    'Change 2014/15',\n",
    "    'Change 2015/16',\n",
    "    'Change 2016/17',\n",
    "    'Change 2017/18'\n",
    "]\n",
    "\n",
    "# Fill the NaN values in the specified columns with \"no change\"\n",
    "df_pp2[columns_to_fill_with_no_change] = df_pp2[columns_to_fill_with_no_change].fillna(\"no change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abefec9",
   "metadata": {},
   "source": [
    "Check if all missing fields are eliminated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aff56d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check for null values in df_up_merged\n",
    "null_counts = df_pp2.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of null values in each column of df_up_merged:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23ca92",
   "metadata": {},
   "source": [
    "Cleaning of missing values successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting number of downgrades in \"Change_2018/19_down grade\"\n",
    "count_of_ones = df_pp2['Default'].value_counts().get(1, 0)\n",
    "print(\"Number of defaults\", count_of_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee31df7",
   "metadata": {},
   "source": [
    "### 3.4.5 Label encoding, of the remaining columns with strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc228c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pp3 = df_pp2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788abb1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First drop rows that contain similar information\n",
    "columns_to_remove = ['Exchange', 'Ticker', 'Geographic Region']\n",
    "\n",
    "# Remove the specified columns\n",
    "df_pp3.drop(columns=columns_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '-' with NaN to properly detect null values\n",
    "df_pp3.replace('-', float('nan'), inplace=True)\n",
    "\n",
    "# Calculate the most frequent value in the 'Industry' column\n",
    "most_frequent_value = df_pp3['Industry'].mode().iloc[0]\n",
    "\n",
    "# Impute the missing values with the most frequent value\n",
    "df_pp3['Industry'].fillna(most_frequent_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "# Apply Binary-Encoding to the 'Land' column\n",
    "binary_encoded_data = label_binarizer.fit_transform(df_pp3['Industry'])\n",
    "binary_encoded_cols = [f\"Industry{val}\" for val in label_binarizer.classes_]\n",
    "binary_encoded_df = pd.DataFrame(binary_encoded_data, columns=binary_encoded_cols)\n",
    "\n",
    "# Drop the original 'Land' column from df_pp2\n",
    "df_pp3.drop('Industry', axis=1, inplace=True)\n",
    "\n",
    "# Concatenate binary_encoded_df with df_pp2\n",
    "df_pp3 = pd.concat([df_pp3, binary_encoded_df], axis=1)\n",
    "\n",
    "# Display the updated DataFrame with binary-encoded 'Land' column\n",
    "df_pp3.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fa0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Land' column to 'Country'\n",
    "df_pp3.rename(columns={'Land': 'Country'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378921ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to be binary encoded\n",
    "column_to_encode = 'Country'\n",
    "\n",
    "# Convert the column to strings\n",
    "df_pp3[column_to_encode] = df_pp3[column_to_encode].astype(str)\n",
    "\n",
    "# Initialize the LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "\n",
    "# Apply Binary-Encoding to the selected column\n",
    "binary_encoded_data = label_binarizer.fit_transform(df_pp3[column_to_encode])\n",
    "binary_encoded_df = pd.DataFrame(binary_encoded_data, columns=[f\"{column_to_encode}_{val}\" for val in label_binarizer.classes_])\n",
    "\n",
    "# Concatenate the binary-encoded columns to the original DataFrame\n",
    "df_pp3 = pd.concat([df_pp3, binary_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original column 'Land' from the DataFrame\n",
    "df_pp3.drop(column_to_encode, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb645d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp3.drop(\"Country_nan\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482a169",
   "metadata": {},
   "source": [
    "Ideally, the company name should still be identifiable. Since numbers could cause a false correlation, the column is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be removed\n",
    "columns_to_remove = [\"Company Name\", \"Change 2013/14\", \"Change 2014/15\", \"Change 2015/16\", \"Change 2016/17\", \"Change 2017/18\"]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "df_pp3 = df_pp3.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6d308",
   "metadata": {},
   "source": [
    "### 3.5 Checking for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4 = df_pp3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b32788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pp4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there are too many columns the correlation matrix is displayed without names and therefore rather used as a heatmap\n",
    "correlation_matrix = df_pp4.corr()\n",
    "\n",
    "plt.figure(figsize=(40, 32))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Korrelationsmatrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0578905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne die Korrelationsmatrix\n",
    "correlation_matrix = df_pp4.corr()\n",
    "\n",
    "# Erstelle eine leere Liste, um die Ergebnisse zu speichern\n",
    "correlation_results = []\n",
    "\n",
    "# Iteriere über die Spalten der Korrelationsmatrix und berechne die Korrelation zwischen jedem Feature-Paar\n",
    "for i, feature1 in enumerate(correlation_matrix.columns):\n",
    "    for j, feature2 in enumerate(correlation_matrix.columns):\n",
    "        if i < j:\n",
    "            correlation_value = correlation_matrix.iloc[i, j]\n",
    "            correlation_results.append([feature1, feature2, correlation_value])\n",
    "\n",
    "# Erstelle ein DataFrame mit den Korrelationsergebnissen\n",
    "correlation_df = pd.DataFrame(correlation_results, columns=['Feature 1', 'Feature 2', 'Korrelationswert'])\n",
    "\n",
    "# Zeige das DataFrame mit den Korrelationsergebnissen an\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cfb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtere die Korrelationswerte: Über 0,7 und nicht gleich 1\n",
    "filtered_correlation_df = correlation_df[\n",
    "    (correlation_df['Korrelationswert'] > 0.7) & (correlation_df['Korrelationswert'] < 1)\n",
    "]\n",
    "\n",
    "# Zeige das DataFrame mit den gefilterten Korrelationsergebnissen an\n",
    "filtered_correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5cccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_correlation_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29c98a",
   "metadata": {},
   "source": [
    "In the dataset, there are metrics for multiple years, and it is observed that these metrics exhibit strong correlations among themselves. From a domain perspective, this is understandable and indicates a stable company. It is important to note that the LinkedIn and Finance KPIs do not show a high correlation to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cf6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where 'Change_2018/19_down grade' appears in either 'Feature 1' or 'Feature 2'\n",
    "correlation_results_filtered = correlation_df[(correlation_df['Feature 1'] == 'Default') | \n",
    "                                              (correlation_df['Feature 2'] == 'Default')]\n",
    "\n",
    "# Sort the results based on the absolute value of correlation in descending order\n",
    "correlation_results_filtered = correlation_results_filtered.iloc[correlation_results_filtered['Korrelationswert'].abs().argsort()[::-1]]\n",
    "correlation_results_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f960c",
   "metadata": {},
   "source": [
    "To prevent correlation between the one hot coded target variable, the Change_2018/19_first rating column is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc845942",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_ones = df_pp4['Default'].value_counts().get(1, 0)\n",
    "print(\"Number of defaults\", count_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen Sie Sonderzeichen aus den Spaltennamen\n",
    "df_pp4.columns = df_pp4.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n",
    "df_pp4.head(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Spalten, die auf NaN-Werte überprüft werden sollen\n",
    "columns_to_check = ['Rating2013', 'Rating2014', 'Rating2015', 'Rating2016', 'Rating2017', 'Rating2018', 'Rating2019',\n",
    "                    'Change201819', 'MarketCapitalization12312014EURmmHistoricalrate',\n",
    "                    'MarketCapitalization12312015EURmmHistoricalrate', 'MarketCapitalization12312016EURmmHistoricalrate',\n",
    "                    'MarketCapitalizationMySetting12312017EURmmHistoricalrate',\n",
    "                    'MarketCapitalizationMySetting12312018EURmmHistoricalrate', 'EBITDACapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'EBITDACapitalIQCY2015EURmmHistoricalrate', 'EBITDACapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'EBITDACapitalIQCY2017EURmmHistoricalrate', 'EBITDACapitalIQCY2018EURmmHistoricalrate',\n",
    "                    'EBITCapitalIQCY2014EURmmHistoricalrate', 'EBITCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'EBITCapitalIQCY2016EURmmHistoricalrate', 'EBITCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'EBITCapitalIQCY2018EURmmHistoricalrate', 'NetIncomeCapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'NetIncomeCapitalIQCY2015EURmmHistoricalrate', 'NetIncomeCapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'NetIncomeCapitalIQCY2017EURmmHistoricalrate', 'NetIncomeCapitalIQCY2018EURmmHistoricalrate',\n",
    "                    'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'TotalEquityCapitalIQCY2016EURmmHistoricalrate', 'TotalEquityCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'TotalEquityCapitalIQCY2018EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'TotalDebtCapitalIQCY2015EURmmHistoricalrate', 'TotalDebtCapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'TotalDebtCapitalIQCY2017EURmmHistoricalrate', 'TotalDebtCapitalIQCY2018EURmmHistoricalrate',\n",
    "                    'TotalAssetsCapitalIQCY2014EURmmHistoricalrate', 'TotalAssetsCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'TotalAssetsCapitalIQCY2016EURmmHistoricalrate', 'TotalAssetsCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'TotalAssetsCapitalIQCY2018EURmmHistoricalrate', 'NetDebtCapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'NetDebtCapitalIQCY2015EURmmHistoricalrate', 'NetDebtCapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'NetDebtCapitalIQCY2017EURmmHistoricalrate', 'NetDebtCapitalIQCY2018EURmmHistoricalrate',\n",
    "                    'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'GrossProfitCapitalIQCY2016EURmmHistoricalrate', 'GrossProfitCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'GrossProfitCapitalIQCY2018EURmmHistoricalrate', 'TotalEmployeesCapitalIQCY2014',\n",
    "                    'TotalEmployeesCapitalIQCY2015', 'TotalEmployeesCapitalIQCY2016', 'TotalEmployeesCapitalIQCY2017',\n",
    "                    'TotalEmployeesCapitalIQCY2018', 'CashfromOpsCapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'CashfromOpsCapitalIQCY2015EURmmHistoricalrate', 'CashfromOpsCapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'CashfromOpsCapitalIQCY2017EURmmHistoricalrate', 'CashfromOpsCapitalIQCY2018EURmmHistoricalrate',\n",
    "                    'TotalRevenueCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'TotalRevenueCapitalIQCY2016EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'TotalRevenueCapitalIQCY2018EURmmHistoricalrate', 'Equityratio2018', 'DebtratioinProzent2018',\n",
    "                    'Debtequityratio2018', 'Returnonequity2018', 'Returnonsales2018', 'Default',\n",
    "                    'Numberofemployees2014', 'Numberofemployees2015', 'Numberofemployees2016', 'Numberofemployees2017',\n",
    "                    'Numberofemployees2018', 'Employeedevelopment2015', 'Employeedevelopment2016',\n",
    "                    'Employeedevelopment2017', 'Employeedevelopment2018', 'Numberofnotices2014', 'Numberofnotices2015',\n",
    "                    'Numberofnotices2016', 'Numberofnotices2017', 'Migratingworkexperience2014',\n",
    "                    'Migratingworkexperience2015', 'Migratingworkexperience2016', 'Migratingworkexperience2017',\n",
    "                    'NumberofNewJoiners2015', 'NumberofNewJoiners2016', 'NumberofNewJoiners2017',\n",
    "                    'NumberofNewJoiners2018', 'Newjoiningworkexperience2015', 'Newjoiningworkexperience2016',\n",
    "                    'Newjoiningworkexperience2017', 'Newjoiningworkexperience2018', 'Fluctuationrate2014',\n",
    "                    'Fluctuationrate2015', 'Fluctuationrate2016', 'Fluctuationrate2017', 'Fluctuationrate2018',\n",
    "                    'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition']\n",
    "\n",
    "# Schritt 1: Überprüfe, ob alle Spalten in columns_to_check NaN-Werte haben\n",
    "rows_with_all_nan = df_pp4[df_pp4[columns_to_check].isnull().all(axis=1)]\n",
    "\n",
    "# Schritt 2: Entferne die Zeilen mit allen NaN-Werten aus dem DataFrame df_pp4\n",
    "df_pp4 = df_pp4.drop(rows_with_all_nan.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84122a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Schritt 1: Überprüfe, ob es Zeilen mit NaN-Werten gibt\n",
    "rows_with_nan = df_pp4[df_pp4.isnull().any(axis=1)]\n",
    "\n",
    "# Schritt 2: Ersetze NaN-Werte in rows_with_nan durch Nullen\n",
    "df_pp4.fillna(0, inplace=True)\n",
    "df_pp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8453a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_ones = df_pp4['Default'].value_counts().get(1, 0)\n",
    "print(\"Number of defaults\", count_of_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ede10",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df_pp4[['NetDebtCapitalIQCY2014EURmmHistoricalrate','TotalEmployeesCapitalIQCY2014', 'EBITDACapitalIQCY2014EURmmHistoricalrate', 'EBITCapitalIQCY2014EURmmHistoricalrate', 'NetIncomeCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate', 'TotalAssetsCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'CashfromOpsCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2014EURmmHistoricalrate','Numberofemployees2014', 'Numberofnotices2014', 'Migratingworkexperience2014', 'NumberofNewJoiners2015', 'Newjoiningworkexperience2015', 'Fluctuationrate2014', 'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition', 'Default']]\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif['Features'] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4 = df_pp4.drop(['EBITDACapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'EBITDACapitalIQCY2015EURmmHistoricalrate', 'EBITDACapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'EBITDACapitalIQCY2017EURmmHistoricalrate', 'EBITDACapitalIQCY2018EURmmHistoricalrate','CashfromOpsCapitalIQCY2014EURmmHistoricalrate',\n",
    "                    'CashfromOpsCapitalIQCY2015EURmmHistoricalrate', 'CashfromOpsCapitalIQCY2016EURmmHistoricalrate',\n",
    "                    'CashfromOpsCapitalIQCY2017EURmmHistoricalrate', 'CashfromOpsCapitalIQCY2018EURmmHistoricalrate','Numberofemployees2014', 'Numberofemployees2015', 'Numberofemployees2016', 'Numberofemployees2017',\n",
    "                    'Numberofemployees2018',],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df_pp4[['EBITCapitalIQCY2014EURmmHistoricalrate', 'NetIncomeCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate', 'TotalAssetsCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2014EURmmHistoricalrate', 'Numberofnotices2014', 'Migratingworkexperience2014', 'NumberofNewJoiners2015', 'Newjoiningworkexperience2015', 'Fluctuationrate2014', 'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition', 'Default']]\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif['Features'] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b41b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4 = df_pp4.drop(['TotalAssetsCapitalIQCY2014EURmmHistoricalrate', 'TotalAssetsCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'TotalAssetsCapitalIQCY2016EURmmHistoricalrate', 'TotalAssetsCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'TotalAssetsCapitalIQCY2018EURmmHistoricalrate'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df_pp4[['EBITCapitalIQCY2014EURmmHistoricalrate', 'NetIncomeCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2014EURmmHistoricalrate', 'Numberofnotices2014', 'Migratingworkexperience2014', 'NumberofNewJoiners2015','Newjoiningworkexperience2015', 'Fluctuationrate2014', 'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition', 'Default']]\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif['Features'] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87c53d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4 = df_pp4.drop(['EBITCapitalIQCY2014EURmmHistoricalrate', 'EBITCapitalIQCY2015EURmmHistoricalrate',\n",
    "                    'EBITCapitalIQCY2016EURmmHistoricalrate', 'EBITCapitalIQCY2017EURmmHistoricalrate',\n",
    "                    'EBITCapitalIQCY2018EURmmHistoricalrate'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df_pp4[['NetIncomeCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2014EURmmHistoricalrate', 'Numberofnotices2014', 'Migratingworkexperience2014', 'NumberofNewJoiners2015','Newjoiningworkexperience2015', 'Fluctuationrate2014', 'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition', 'Default']]\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif['Features'] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273704c",
   "metadata": {},
   "source": [
    "Rather too high a correlation is accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f364cc",
   "metadata": {},
   "source": [
    "# 4 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b685dcbc",
   "metadata": {},
   "source": [
    "In the following, the required data frames are first formed on the basis of the cleaned data (4.0). Then the ML models are first applied to the financial ratios (4.1), then to the LinkedIn features (4.2) and finally to the combined data (4.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df_pp4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956cb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e526d",
   "metadata": {},
   "source": [
    "## 4.0 Building dataframes\n",
    "\n",
    "Create the required data frames:\n",
    "- df_financials\n",
    "- df_linkedin\n",
    "- df_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf94886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting column names\n",
    "# column_names = [\"{}\".format(col) for col in dfm.columns]\n",
    "# print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns for financials (ratings can not be used, because there are no ratings for defaulted companies)\n",
    "selected_columns = ['NetIncomeCapitalIQCY2014EURmmHistoricalrate', 'NetIncomeCapitalIQCY2015EURmmHistoricalrate', 'NetIncomeCapitalIQCY2016EURmmHistoricalrate', 'NetIncomeCapitalIQCY2017EURmmHistoricalrate', 'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2015EURmmHistoricalrate', 'TotalEquityCapitalIQCY2016EURmmHistoricalrate', 'TotalEquityCapitalIQCY2017EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate', 'TotalDebtCapitalIQCY2015EURmmHistoricalrate', 'TotalDebtCapitalIQCY2016EURmmHistoricalrate', 'TotalDebtCapitalIQCY2017EURmmHistoricalrate', 'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2015EURmmHistoricalrate', 'GrossProfitCapitalIQCY2016EURmmHistoricalrate', 'GrossProfitCapitalIQCY2017EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2015EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2016EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2017EURmmHistoricalrate', 'Default',]\n",
    "\n",
    "# Creat new dataframe\n",
    "df_financials = dfm[selected_columns]\n",
    "df_financials.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80525da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default\\\\Financials\\\\default_financials.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_financials.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e129a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns for linkedin\n",
    "selected_columns = ['Employeedevelopment2015', 'Employeedevelopment2016', 'Employeedevelopment2017', 'Numberofnotices2014', 'Numberofnotices2015', 'Numberofnotices2016', 'Numberofnotices2017', 'Migratingworkexperience2014', 'Migratingworkexperience2015', 'Migratingworkexperience2016', 'Migratingworkexperience2017', 'NumberofNewJoiners2015', 'NumberofNewJoiners2016', 'NumberofNewJoiners2017', 'Newjoiningworkexperience2015', 'Newjoiningworkexperience2016', 'Newjoiningworkexperience2017', 'Fluctuationrate2014', 'Fluctuationrate2015', 'Fluctuationrate2016', 'Fluctuationrate2017', 'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition', 'Default']\n",
    "\n",
    "# Creat new dataframe\n",
    "df_linkedin = dfm[selected_columns]\n",
    "df_linkedin.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default\\\\LinkedIn\\\\default_LinkedIn.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_linkedin.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns for combined\n",
    "selected_columns = ['Employeedevelopment2015', 'Employeedevelopment2016', 'Employeedevelopment2017', 'Numberofnotices2014', 'Numberofnotices2015', 'Numberofnotices2016', 'Numberofnotices2017', 'Migratingworkexperience2014', 'Migratingworkexperience2015', 'Migratingworkexperience2016', 'Migratingworkexperience2017', 'NumberofNewJoiners2015', 'NumberofNewJoiners2016', 'NumberofNewJoiners2017', 'Newjoiningworkexperience2015', 'Newjoiningworkexperience2016', 'Newjoiningworkexperience2017', 'Fluctuationrate2014', 'Fluctuationrate2015', 'Fluctuationrate2016', 'Fluctuationrate2017', 'Averageyearsofservicewiththecompany', 'Morethanoncedifferentposition', 'NetIncomeCapitalIQCY2014EURmmHistoricalrate', 'NetIncomeCapitalIQCY2015EURmmHistoricalrate', 'NetIncomeCapitalIQCY2016EURmmHistoricalrate', 'NetIncomeCapitalIQCY2017EURmmHistoricalrate', 'TotalEquityCapitalIQCY2014EURmmHistoricalrate', 'TotalEquityCapitalIQCY2015EURmmHistoricalrate', 'TotalEquityCapitalIQCY2016EURmmHistoricalrate', 'TotalEquityCapitalIQCY2017EURmmHistoricalrate', 'TotalDebtCapitalIQCY2014EURmmHistoricalrate', 'TotalDebtCapitalIQCY2015EURmmHistoricalrate', 'TotalDebtCapitalIQCY2016EURmmHistoricalrate', 'TotalDebtCapitalIQCY2017EURmmHistoricalrate', 'GrossProfitCapitalIQCY2014EURmmHistoricalrate', 'GrossProfitCapitalIQCY2015EURmmHistoricalrate', 'GrossProfitCapitalIQCY2016EURmmHistoricalrate', 'GrossProfitCapitalIQCY2017EURmmHistoricalrate', 'GrossProfitCapitalIQCY2018EURmmHistoricalrate','TotalRevenueCapitalIQCY2014EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2015EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2016EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2017EURmmHistoricalrate', 'TotalRevenueCapitalIQCY2018EURmmHistoricalrate', 'Default']\n",
    "\n",
    "# Creat new dataframe\n",
    "df_com = dfm[selected_columns]\n",
    "df_com.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default\\\\Com\\\\downgrade_combined.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_com.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e167ae6",
   "metadata": {},
   "source": [
    "## 4.1 Running modells on df_financials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b895d",
   "metadata": {},
   "source": [
    "### 4.1.1 Splitting between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation of the property to be predicted\n",
    "target = df_financials['Default']\n",
    "\n",
    "# Isolation of all properties that contribute to the prediction.\n",
    "predictors = df_financials.drop(['Default'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891269e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of the following dataframes:\n",
    "# X_train = training data from all properties that are not the target column (80%).\n",
    "# X_test = analog X_train, but only 20%.\n",
    "# Y_train =Training data from the target variable (80%)\n",
    "# Y_test = analog Y_train, but only 20%.\n",
    "X_train_pre, X_test, y_train_pre, y_test = train_test_split(predictors, target, test_size=0.2, random_state=356)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e9ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4792971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62a898",
   "metadata": {},
   "source": [
    "### 4.1.2 Generating synthetic data of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Instanz der ADASYN-Klasse\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Anwenden von ADASYN, um synthetische Daten zu generieren\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_pre, y_train_pre)\n",
    "\n",
    "# Sie können auch die generierten numpy-Arrays wieder in Dataframes umwandeln, falls erforderlich\n",
    "X_train = pd.DataFrame(X_train_adasyn, columns=X_train_pre.columns)\n",
    "y_train = pd.Series(y_train_adasyn, name=y_train_pre.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caadefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame(y_train_pre)\n",
    "temp2 = pd.DataFrame(y_train)\n",
    "\n",
    "print('Before SMOTE')\n",
    "print(temp1['Default'].value_counts())\n",
    "print('After SMOTE')\n",
    "print(temp2['Default'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bff2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.DataFrame(y_test)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Default'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dein vorhandener Code\n",
    "temp3 = pd.DataFrame(y_test)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Default'].value_counts())\n",
    "\n",
    "# Erstelle ein Kreisdiagramm\n",
    "plt.figure(figsize=(6, 6))\n",
    "temp3['Default'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=['skyblue', 'lightcoral'])\n",
    "plt.title('Distribution of Default in Test Data')\n",
    "plt.ylabel('')  # Entferne die Y-Achsenbeschriftung\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538d5ee",
   "metadata": {},
   "source": [
    "Oversampling of train data succsessfull. Test data still unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Financials\\x_test_financials.csv', index=False)\n",
    "X_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Financials\\x_train_financials.csv', index=False)\n",
    "y_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Financials\\y_test_financials.csv', index=False)\n",
    "y_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Financials\\y_train_financials.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb06025",
   "metadata": {},
   "source": [
    "### 4.1.4  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466927c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the X-target variable is compared with the predicted values\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e70948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "plot_confusion_matrix(classifier,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp/(fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"True Negatives: \" + str(tn))\n",
    "print(\"False Positives: \" + str(fp))\n",
    "print(\"False Negatives: \" + str(fn))\n",
    "print(\"True Positives: \" + str(tp))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(classifier, X_train, y_train, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cb6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11addbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445eba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143c4be",
   "metadata": {},
   "source": [
    "For verification, we check how the target variable of the training data is predicted. Therefore, the y_train is predicted using logistic regression and using the properties (x_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711596c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparison and results check \n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials = {\n",
    "    \"Logistic Regression\": [0.43, 0.38, 0.88, 0.88],}\n",
    "# precision, recall, then acc, then auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_financials = {} \n",
    "Robustness_financials[\"Logistic Regression\"]=[0.85, 0.88]\n",
    "# recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38a018",
   "metadata": {},
   "source": [
    "### 4.1.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451df95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4398f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred_tree)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e493bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Führe eine 5-fache Cross-Validation auf den Trainingsdaten durch\n",
    "cross_val_scores = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "\n",
    "# Gib die Ergebnisse der Cross-Validation aus\n",
    "print(\"Cross-validation scores:\", cross_val_scores)\n",
    "print(\"Average Cross-validation score:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc61b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_tree = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the Decision Tree classifier\n",
    "auc_score_tree = roc_auc_score(y_test, y_pred_prob_tree)\n",
    "print(\"AUC Score for Decision Tree:\", auc_score_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1da344",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree,X_train,y_train, cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43ef65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a92e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(tree, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(tree, X_train, y_train, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials.update({\n",
    "    \"Decision Tree\": [0.44, 0.88, 0.87, 0.87]\n",
    "})\n",
    "#precision, recall, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_financials[\"Decision Tree\"]= [0.89, 0.90]\n",
    "# recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbfddb",
   "metadata": {},
   "source": [
    "### 4.1.6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada54a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = [5, 10, 20, 30]\n",
    "for i in tree_depth:\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Max tree depth: ', i)\n",
    "    print('Train results: ', classification_report(y_train, rf.predict(X_train)))\n",
    "    print('Test results: ',classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the RandomForestClassifier\n",
    "auc_score_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
    "print('AUC Score for Random Forest:', auc_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b409922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(rf, X_train, y_train, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b63402",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials.update({\n",
    "    \"Random Forest\": [0.50, 0.88, 0.89, 0.94],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_financials[\"Random Forest\"]=[0.91, 0.94]\n",
    "# recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcef5a",
   "metadata": {},
   "source": [
    "### 4.1.7 XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = xgb.XGBClassifier()\n",
    "cross_val_scores = cross_val_score(xgb_classifier, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d52780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data for accuracy\n",
    "cross_val_scores_acc = cross_val_score(xgb_classifier, X_train, y_train, cv=5)\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores_acc)\n",
    "print(\"Average Cross-validation ACC score:\", cross_val_scores_acc.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for AUC\n",
    "cross_val_scores_auc = cross_val_score(xgb_classifier, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(\"Cross-validation AUC scores:\", cross_val_scores_auc)\n",
    "print(\"Average Cross-validation AUC score:\", cross_val_scores_auc.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for Recall\n",
    "cross_val_scores_recall = cross_val_score(xgb_classifier, X_train, y_train, cv=5, scoring='recall')\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0f83a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the XGBoost classifier to the training data\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "importance = plot_importance(xgb_classifier, height=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ac3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports for train and test sets\n",
    "print('Train results: ', classification_report(y_train, xgb_classifier.predict(X_train)))\n",
    "print('Test results: ', classification_report(y_test, xgb_classifier.predict(X_test)))\n",
    "\n",
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_xgb = xgb_classifier.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e30cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC score for the XGBoost classifier\n",
    "auc_score_xgb = roc_auc_score(y_test, y_pred_prob_xgb)\n",
    "print('AUC Score for XGBoost:', auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39518130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials.update({\n",
    "    \"XGBoost\": [0.47, 0.88, 0.88, 0.94],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47eb21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_financials[\"XGBoost\"]= [0.92, 0.93]\n",
    "# recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c14c5",
   "metadata": {},
   "source": [
    "## 4.2 Running modells on df_linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246fe388",
   "metadata": {},
   "source": [
    "### 4.2.1 Splitting between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation of the property to be predicted\n",
    "target = df_linkedin['Default'] \n",
    "# Isolation of all properties that contribute to the prediction.\n",
    "predictors = df_linkedin.drop(['Default'], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b37a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of the following dataframes:\n",
    "# X_trainL = training data from all properties that are not the target column (80%).\n",
    "# X_testL = analog X_train, but only 20%.\n",
    "# Y_trainL =Training data from the target variable (80%)\n",
    "# Y_testL = analog Y_train, but only 20%.\n",
    "X_trainL_pre, X_testL, y_trainL_pre, y_testL = train_test_split(predictors, target, test_size=0.2, random_state=356)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac32fd",
   "metadata": {},
   "source": [
    "### 4.2.2 Generating syntetic data of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43374d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Instanz der ADASYN-Klasse\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Anwenden von ADASYN, um synthetische Daten zu generieren\n",
    "X_trainL_adasyn, y_trainL_adasyn = adasyn.fit_resample(X_trainL_pre, y_trainL_pre)\n",
    "\n",
    "# Sie können auch die generierten numpy-Arrays wieder in Dataframes umwandeln, falls erforderlich\n",
    "X_trainL = pd.DataFrame(X_trainL_adasyn, columns=X_trainL_pre.columns)\n",
    "y_trainL = pd.Series(y_trainL_adasyn, name=y_trainL_pre.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c234b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame(y_trainL_pre)\n",
    "temp2 = pd.DataFrame(y_trainL)\n",
    "\n",
    "print('Before SMOTE')\n",
    "print(temp1['Default'].value_counts())\n",
    "print('After SMOTE')\n",
    "print(temp2['Default'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e59ce3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp3 = pd.DataFrame(y_testL)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Default'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2167",
   "metadata": {},
   "source": [
    "Oversampling successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44631194",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testL.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\LinkedIn\\x_test_linkedin.csv', index=False)\n",
    "X_trainL_pre.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\LinkedIn\\x_train_linkedin.csv', index=False)\n",
    "y_testL.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\LinkedIn\\y_test_linkedin.csv', index=False)\n",
    "y_trainL_pre.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\LinkedIn\\y_train_linkedin.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4778b6",
   "metadata": {},
   "source": [
    "### 4.2.3  Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e52eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_trainL, y_trainL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_testL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the X-target variable is compared with the predicted values\n",
    "cm = confusion_matrix(y_testL, y_pred)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27186b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "plot_confusion_matrix(classifier,X_testL,y_testL,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f69dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp/(fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"True Negatives: \" + str(tn))\n",
    "print(\"False Positives: \" + str(fp))\n",
    "print(\"False Negatives: \" + str(fn))\n",
    "print(\"True Positives: \" + str(tp))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_testL, y_pred)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_testL,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob = classifier.predict_proba(X_testL)[:, 1]\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_testL, y_pred_prob)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_trainL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99dfb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison and results check \n",
    "print(classification_report(y_trainL,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(classifier, X_trainL, y_trainL, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36167b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(classifier, X_trainL, y_trainL, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin = {}\n",
    "Precisions_linkedin = {\n",
    "    \"Logistic Regression\": [0.22, 0.62, 0.72, 0.75]}\n",
    "# precision, recall, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8441d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_linkedin = {} \n",
    "Robustness_linkedin[\"Logistic Regression\"]= [0.85, 0.85]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c824de5",
   "metadata": {},
   "source": [
    "### 4.2.4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5464aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_trainL, y_trainL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4852270",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree.predict(X_testL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree,X_testL,y_testL,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_testL, y_pred_tree)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1601fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_testL, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_tree = tree.predict_proba(X_testL)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the Decision Tree classifier\n",
    "auc_score_tree = roc_auc_score(y_testL, y_pred_prob_tree)\n",
    "print(\"AUC Score for Decision Tree:\", auc_score_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75619f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = tree.predict(X_trainL)\n",
    "plot_confusion_matrix(tree,X_trainL,y_trainL, cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(tree, X_trainL, y_trainL, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(tree, X_trainL, y_trainL, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin.update({\n",
    "    \"Decision Tree\": [0.14, 0.25, 0.76, 0.54],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_linkedin = {}\n",
    "Robustness_linkedin[\"Decision Tree\"] = [0.79, 0.85]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda18e5",
   "metadata": {},
   "source": [
    "### 4.2.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449723e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = [5, 10, 30]\n",
    "for i in tree_depth:\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    rf.fit(X_trainL, y_trainL)\n",
    "    print('Max tree depth: ', i)\n",
    "    print('Train results: ', classification_report(y_trainL, rf.predict(X_trainL)))\n",
    "    print('Test results: ',classification_report(y_testL, rf.predict(X_testL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_rf = rf.predict_proba(X_testL)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the RandomForestClassifier\n",
    "auc_score_rf = roc_auc_score(y_testL, y_pred_prob_rf)\n",
    "print('AUC Score for Random Forest:', auc_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4575b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(rf.feature_importances_, index=X_trainL.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(rf, X_trainL, y_trainL, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20854d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(rf, X_trainL, y_trainL, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f829da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin.update({\n",
    "    \"Random Forest\": [0.00, 0.00, 0.76, 0.79],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e722360",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_linkedin[\"Random Forest\"] = [0.86, 0.88]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864eef47",
   "metadata": {},
   "source": [
    "### 4.2.5 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for accuracy\n",
    "cross_val_scores_acc = cross_val_score(xgb_classifier, X_trainL, y_trainL, cv=5)\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores_acc)\n",
    "print(\"Average Cross-validation ACC score:\", cross_val_scores_acc.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for AUC\n",
    "cross_val_scores_auc = cross_val_score(xgb_classifier, X_trainL, y_trainL, cv=5, scoring='roc_auc')\n",
    "print(\"Cross-validation AUC scores:\", cross_val_scores_auc)\n",
    "print(\"Average Cross-validation AUC score:\", cross_val_scores_auc.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for Recall\n",
    "cross_val_scores_recall = cross_val_score(xgb_classifier, X_trainL, y_trainL, cv=5, scoring='recall')\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the XGBoost classifier to the training data\n",
    "xgb_classifier.fit(X_trainL, y_trainL)\n",
    "\n",
    "# Plot feature importance\n",
    "importance = plot_importance(xgb_classifier, height=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14ec8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print classification reports for train and test sets\n",
    "print('Train results: ', classification_report(y_trainL, xgb_classifier.predict(X_trainL)))\n",
    "print('Test results: ', classification_report(y_testL, xgb_classifier.predict(X_testL)))\n",
    "\n",
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_xgb = xgb_classifier.predict_proba(X_testL)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737e5c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the AUC score for the XGBoost classifier\n",
    "auc_score_xgb = roc_auc_score(y_testL, y_pred_prob_xgb)\n",
    "print('AUC Score for XGBoost:', auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin.update({\n",
    "    \"XGBoost\": [0.10, 0.12, 0.79, 0.75],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_linkedin[\"XGBoost\"]= [0.87, 0.96]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770175df",
   "metadata": {},
   "source": [
    "## 4.3 Running modells on df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae7d12",
   "metadata": {},
   "source": [
    "### 4.3.1 Splitting between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation of the property to be predicted\n",
    "target = df_com['Default']\n",
    "\n",
    "# Isolation of all properties that contribute to the prediction.\n",
    "predictors = df_com.drop(['Default'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of the following dataframes:\n",
    "# X_train = training data from all properties that are not the target column (80%).\n",
    "# X_test = analog X_train, but only 20%.\n",
    "# Y_train =Training data from the target variable (80%)\n",
    "# Y_test = analog Y_train, but only 20%.\n",
    "X_trainC_pre, X_testC, y_trainC_pre, y_testC = train_test_split(predictors, target, test_size=0.2, random_state=356)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7695e9",
   "metadata": {},
   "source": [
    "### 4.3.2 Oversampling of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c28b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Instanz der ADASYN-Klasse\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Anwenden von ADASYN, um synthetische Daten zu generieren\n",
    "X_trainC_adasyn, y_trainC_adasyn = adasyn.fit_resample(X_trainC_pre, y_trainC_pre)\n",
    "\n",
    "# Sie können auch die generierten numpy-Arrays wieder in Dataframes umwandeln, falls erforderlich\n",
    "X_trainC = pd.DataFrame(X_trainC_adasyn, columns=X_trainC_pre.columns)\n",
    "y_trainC = pd.Series(y_trainC_adasyn, name=y_trainC_pre.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame(y_trainC_pre)\n",
    "temp2 = pd.DataFrame(y_trainC)\n",
    "\n",
    "print('Before SMOTE')\n",
    "print(temp1['Default'].value_counts())\n",
    "print('After SMOTE')\n",
    "print(temp2['Default'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.DataFrame(y_testC)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Default'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bfb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testC.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Com\\x_test_com.csv', index=False)\n",
    "X_trainC_pre.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Com\\x_train_com.csv', index=False)\n",
    "y_testC.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Com\\y_test_com.csv', index=False)\n",
    "y_trainC_pre.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Default\\Com\\y_train_com.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ade16",
   "metadata": {},
   "source": [
    "### 4.3.3  Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_trainC, y_trainC)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_testC)\n",
    "# Here the X-target variable is compared with the predicted values\n",
    "cm = confusion_matrix(y_testC, y_pred)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "plot_confusion_matrix(classifier,X_testC,y_testC,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp/(fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"True Negatives: \" + str(tn))\n",
    "print(\"False Positives: \" + str(fp))\n",
    "print(\"False Negatives: \" + str(fn))\n",
    "print(\"True Positives: \" + str(tp))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701df7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_testC, y_pred)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a2f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_testC,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a486cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob = classifier.predict_proba(X_testC)[:, 1]\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_testC, y_pred_prob)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_trainC)\n",
    "# Comparison and results check \n",
    "print(classification_report(y_trainC,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e756e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(classifier, X_trainC, y_trainC, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(classifier, X_trainC, y_trainC, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c685349",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com= {\n",
    "    \"Logistic Regression\": [0.60, 0.38, 0.91, 0.95],}\n",
    "# first test, then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_com = {} \n",
    "Robustness_com[\"Logistic Regression\"] = [0.99, 0.97]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1265e",
   "metadata": {},
   "source": [
    "### 4.3.4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_trainC, y_trainC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree.predict(X_testC)\n",
    "plot_confusion_matrix(tree,X_testC,y_testC,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_testC, y_pred_tree)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f72a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_testC, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_tree = tree.predict_proba(X_testC)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the Decision Tree classifier\n",
    "auc_score_tree = roc_auc_score(y_testC, y_pred_prob_tree)\n",
    "print(\"AUC Score for Decision Tree:\", auc_score_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = tree.predict(X_trainC)\n",
    "plot_confusion_matrix(tree,X_trainC,y_trainC, cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c019e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy : \", accuracy_score(y_trainC, y_train_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ae958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_trainC, y_train_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f74cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(tree, X_trainC, y_trainC, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(tree, X_trainC, y_trainC, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com.update({\n",
    "    \"Decision Tree\": [0.27, 0.50, 0.80, 0.67],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16175c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_com[\"Decision Tree\"] = [0.88, 0.91]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd8924",
   "metadata": {},
   "source": [
    "### 4.3.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = [5, 10, 20, 30]\n",
    "for i in tree_depth:\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    rf.fit(X_trainC, y_trainC)\n",
    "    print('Max tree depth: ', i)\n",
    "    print('Train results: ', classification_report(y_trainC, rf.predict(X_trainC)))\n",
    "    print('Test results: ',classification_report(y_testC, rf.predict(X_testC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_rf = rf.predict_proba(X_testC)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the RandomForestClassifier\n",
    "auc_score_rf = roc_auc_score(y_testC, y_pred_prob_rf)\n",
    "print('AUC Score for Random Forest:', auc_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(rf.feature_importances_, index=X_trainC.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8255601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores = cross_val_score(rf, X_trainC, y_trainC, cv=5)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores)\n",
    "print(\"Average Accuracy:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recall metric for use with cross_val_score\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "cross_val_scores_recall = cross_val_score(rf, X_trainC, y_trainC, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Print the results of cross-validation\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751db07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com.update({\n",
    "    \"Random Forest\": [0.55, 0.75, 0.91, 0.96],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb426c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_com[\"Random Forest\"] = [0.91, 0.96]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38384e0f",
   "metadata": {},
   "source": [
    "### 4.3.5 XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for accuracy\n",
    "cross_val_scores_acc = cross_val_score(xgb_classifier, X_trainC, y_trainC, cv=5)\n",
    "print(\"Cross-validation ACC scores:\", cross_val_scores_acc)\n",
    "print(\"Average Cross-validation ACC score:\", cross_val_scores_acc.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for AUC\n",
    "cross_val_scores_auc = cross_val_score(xgb_classifier, X_trainC, y_trainC, cv=5, scoring='roc_auc')\n",
    "print(\"Cross-validation AUC scores:\", cross_val_scores_auc)\n",
    "print(\"Average Cross-validation AUC score:\", cross_val_scores_auc.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for Recall\n",
    "cross_val_scores_recall = cross_val_score(xgb_classifier, X_trainC, y_trainC, cv=5, scoring='recall')\n",
    "print(\"Cross-validation Recall scores:\", cross_val_scores_recall)\n",
    "print(\"Average Cross-validation Recall score:\", cross_val_scores_recall.mean())\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data for Precision\n",
    "precision_scorer = make_scorer(precision_score)\n",
    "cross_val_scores_precision = cross_val_score(xgb_classifier, X_trainC, y_trainC, cv=5, scoring=precision_scorer)\n",
    "print(\"Cross-validation Precision scores:\", cross_val_scores_precision)\n",
    "print(\"Average Cross-validation Precision score:\", cross_val_scores_precision.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the XGBoost classifier to the training data\n",
    "xgb_classifier.fit(X_trainC, y_trainC)\n",
    "\n",
    "# Plot feature importance\n",
    "importance = plot_importance(xgb_classifier, height=0.9, max_num_features=10)\n",
    "plt.show()\n",
    "\n",
    "# Print classification reports for train and test sets\n",
    "print('Train results: ', classification_report(y_trainC, xgb_classifier.predict(X_trainC)))\n",
    "print('Test results: ', classification_report(y_testC, xgb_classifier.predict(X_testC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f18cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_xgb = xgb_classifier.predict_proba(X_testC)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the XGBoost classifier\n",
    "auc_score_xgb = roc_auc_score(y_testC, y_pred_prob_xgb)\n",
    "print('AUC Score for XGBoost:', auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54559c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com.update({\n",
    "    \"XGBoost\": [0.55, 0.75, 0.91, 0.95],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Robustness_com[\"XGBoost\"] = [0.96, 0.95]\n",
    "#recall, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1ec67",
   "metadata": {},
   "source": [
    "# 5. Evaluating - comparing Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421053d0",
   "metadata": {},
   "source": [
    "The results are compiled and neatly presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75367774",
   "metadata": {},
   "source": [
    "## 5.1 Results financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"\", \"Precision (1)\", \"Recall (1)\", \"Accuracy\", \"AUC\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = headers\n",
    "\n",
    "# List to store the maximum values in each column\n",
    "max_values = [0.0] * len(headers)\n",
    "\n",
    "best_model_auc = \"\"\n",
    "max_auc_value = 0.0\n",
    "\n",
    "for model, metrics in Precisions_financials.items():\n",
    "    precision_test_data = metrics[0]\n",
    "    precision_train_data = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    auc = metrics[3]\n",
    "\n",
    "    # Update the maximum values for each column\n",
    "    max_values[1] = max(max_values[1], precision_test_data)\n",
    "    max_values[2] = max(max_values[2], precision_train_data)\n",
    "    max_values[3] = max(max_values[3], accuracy)\n",
    "    max_values[4] = max(max_values[4], auc)\n",
    "\n",
    "    # Update the best model based on the highest AUC score\n",
    "    if auc > max_auc_value:\n",
    "        max_auc_value = auc\n",
    "        best_model_auc = model\n",
    "\n",
    "    # Add a row to the table\n",
    "    table.add_row([model, precision_test_data, precision_train_data, accuracy, auc])\n",
    "\n",
    "# Mark the highest value in each column in red\n",
    "for row in table._rows:\n",
    "    for i in range(1, len(headers)):\n",
    "        if row[i] == max_values[i]:\n",
    "            row[i] = f\"\\033[31m{row[i]}\\033[0m\"  # Red color for the highest value\n",
    "\n",
    "# Print the table with the highest values in each column marked in red\n",
    "print(table)\n",
    "\n",
    "# Print the \"Best model\" message in red\n",
    "print(f\"\\033[31mBest model based on AUC: {best_model_auc}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robustness3 = pd.DataFrame.from_dict(Robustness_financials, orient='index', columns=['Recall', 'Accuracy'])\n",
    "print(df_robustness3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308cbfd",
   "metadata": {},
   "source": [
    "## 5.2 Results linkedin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"\", \"Precision (1)\", \"Recall (1)\", \"Accuracy\", \"AUC\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = headers\n",
    "\n",
    "# List to store the maximum values in each column\n",
    "max_values = [0.0] * len(headers)\n",
    "\n",
    "best_model_auc = \"\"\n",
    "max_auc_value = 0.0\n",
    "\n",
    "for model, metrics in Precisions_linkedin.items():\n",
    "    precision_test_data = metrics[0]\n",
    "    precision_train_data = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    auc = metrics[3]\n",
    "\n",
    "    # Update the maximum values for each column\n",
    "    max_values[1] = max(max_values[1], precision_test_data)\n",
    "    max_values[2] = max(max_values[2], precision_train_data)\n",
    "    max_values[3] = max(max_values[3], accuracy)\n",
    "    max_values[4] = max(max_values[4], auc)\n",
    "\n",
    "    # Update the best model based on the highest AUC score\n",
    "    if auc > max_auc_value:\n",
    "        max_auc_value = auc\n",
    "        best_model_auc = model\n",
    "\n",
    "    # Add a row to the table\n",
    "    table.add_row([model, precision_test_data, precision_train_data, accuracy, auc])\n",
    "\n",
    "# Mark the highest value in each column in red\n",
    "for row in table._rows:\n",
    "    for i in range(1, len(headers)):\n",
    "        if row[i] == max_values[i]:\n",
    "            row[i] = f\"\\033[31m{row[i]}\\033[0m\"  # Red color for the highest value\n",
    "\n",
    "# Print the table with the highest values in each column marked in red\n",
    "print(table)\n",
    "\n",
    "# Print the \"Best model\" message in red\n",
    "print(f\"\\033[31mBest model based on AUC: {best_model_auc}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eca663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robustness2 = pd.DataFrame.from_dict(Robustness_linkedin, orient='index', columns=['Recall', 'Accuracy'])\n",
    "print(df_robustness2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe8164",
   "metadata": {},
   "source": [
    "## 5.3 Results combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b204ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"\", \"Precision (1)\", \"Recall (1)\", \"Accuracy\", \"AUC\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = headers\n",
    "\n",
    "# List to store the maximum values in each column\n",
    "max_values = [0.0] * len(headers)\n",
    "\n",
    "best_model_auc = \"\"\n",
    "max_auc_value = 0.0\n",
    "\n",
    "for model, metrics in Precisions_com.items():\n",
    "    precision_test_data = metrics[0]\n",
    "    precision_train_data = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    auc = metrics[3]\n",
    "\n",
    "    # Update the maximum values for each column\n",
    "    max_values[1] = max(max_values[1], precision_test_data)\n",
    "    max_values[2] = max(max_values[2], precision_train_data)\n",
    "    max_values[3] = max(max_values[3], accuracy)\n",
    "    max_values[4] = max(max_values[4], auc)\n",
    "\n",
    "    # Update the best model based on the highest AUC score\n",
    "    if auc > max_auc_value:\n",
    "        max_auc_value = auc\n",
    "        best_model_auc = model\n",
    "\n",
    "    # Add a row to the table\n",
    "    table.add_row([model, precision_test_data, precision_train_data, accuracy, auc])\n",
    "\n",
    "# Mark the highest value in each column in red\n",
    "for row in table._rows:\n",
    "    for i in range(1, len(headers)):\n",
    "        if row[i] == max_values[i]:\n",
    "            row[i] = f\"\\033[31m{row[i]}\\033[0m\"  # Red color for the highest value\n",
    "\n",
    "# Print the table with the highest values in each column marked in red\n",
    "print(table)\n",
    "\n",
    "# Print the \"Best model\" message in red\n",
    "print(f\"\\033[31mBest model based on AUC: {best_model_auc}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71462267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robustness = pd.DataFrame.from_dict(Robustness_com, orient='index', columns=['Recall', 'Accuracy'])\n",
    "print(df_robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8dc393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
