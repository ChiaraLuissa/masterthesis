{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cffb9b",
   "metadata": {},
   "source": [
    "# 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96b846d",
   "metadata": {},
   "source": [
    "Business valuations are crucial for a diverse range of stakeholders, guiding capital allocation decisions based on precise assessments of companies' economic performance, regardless of whether the trends are positive, steady, or negative. The potential risks of both overly negative valuations, misinterpreting positive trends, and overlooking negative developments are equally significant. Such misjudgments can impede a company's refinancing options, lead to missed investment prospects for investors, and result in financial losses. In the following code, we evaluate how aggregated features from LinkedIn help to improve the quality of prediction of a downgrade. Three data frames are used - financial metrics only, LinkedIn metrics only and both combined. The evaluation is considered successful if a positive influence of the LinkedIn features on the prediction can be determined. AUC and recall are considered particularly relevant metrics. Details can be found in the Data chapter of the corresponding master thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210de2e",
   "metadata": {},
   "source": [
    "# 2. Load data and prepare libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b22a6f",
   "metadata": {},
   "source": [
    "With the use of Chat GPD, comments have been added for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bda71",
   "metadata": {},
   "source": [
    "## 2.1 Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from prettytable import PrettyTable\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree, XGBClassifier, XGBRegressor\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ac0d6",
   "metadata": {},
   "source": [
    "## 2.2 Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8429086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\List of companys_onetemplate_downgrade.xls'\n",
    "df_up = pd.read_excel(dateipfad)\n",
    "df_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2ffcc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\df_waf_final.csv'\n",
    "df_waf_rfm = pd.read_csv(dateipfad, sep=';')\n",
    "df_waf_rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378abf09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_waf_rfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af4603",
   "metadata": {},
   "source": [
    "Author knowledge: in the generation of df_waf_rfm initialisation values were used. They are droped from the dataframe before the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5268b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtere die Zeilen mit dem Wert 1 in der Spalte \"Number of Employee 2014\"\n",
    "filtered_df = df_waf_rfm[df_waf_rfm['Number of employees 2014'] == 1]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_waf_rfm = df_waf_rfm[df_waf_rfm['Number of employees 2014'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630eb878",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad = r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Matching.csv'\n",
    "df_match = pd.read_csv(dateipfad, sep=';')\n",
    "df_match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618db2c",
   "metadata": {},
   "source": [
    "## 2.3 Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf39896",
   "metadata": {},
   "source": [
    "Cleaning Company name to make a match with firm_original_name possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy the column \"Company Name\" to the new column \"Copy_Company_Name\" in DataFrame df_up\n",
    "df_up['Copy_Company_Name'] = df_up['Company Name']\n",
    "\n",
    "# Remove values in parentheses from the \"Company Name\" column in DataFrame df_up\n",
    "df_up['Company Name'] = df_up['Company Name'].apply(lambda x: re.sub(r'\\(.*\\)', '', str(x)).strip())\n",
    "\n",
    "# Print the DataFrame df_up after the modifications\n",
    "df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19c9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for cleaning Company Name by removing non-alphanumeric characters\n",
    "def clean_company_name(name):\n",
    "    return re.sub(r'[^\\w\\s]', '', str(name))\n",
    "\n",
    "# Clean the Company Name column using the clean_company_name function in DataFrame df_up\n",
    "df_up['Company Name'] = df_up['Company Name'].apply(clean_company_name)\n",
    "df_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58e14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for fuzzy matching to find the best match for each company name\n",
    "def find_best_match(company_name, reference_names):\n",
    "    best_match = None\n",
    "    best_similarity = 0\n",
    "\n",
    "    for ref_name in reference_names:\n",
    "        similarity = fuzz.token_set_ratio(company_name, ref_name)\n",
    "        if similarity > best_similarity:\n",
    "            best_match = ref_name\n",
    "            best_similarity = similarity\n",
    "\n",
    "    # Threshold for similarity score (adjust as needed)\n",
    "    threshold_similarity = 95\n",
    "\n",
    "    # Return the best match if similarity score is above the threshold, else return None\n",
    "    return best_match if best_similarity >= threshold_similarity else None\n",
    "\n",
    "# Create an empty list to store the matched companies\n",
    "matched_companies = []\n",
    "\n",
    "# Extract company names from df_up\n",
    "company_names_up = df_up['Company Name'].tolist()\n",
    "\n",
    "# Extract firm_original_names from df_waf_rfm\n",
    "firm_original_names_waf = df_waf_rfm['Firm_original_name'].tolist()\n",
    "\n",
    "# Iterate over the company names in df_up\n",
    "for company_name_up in company_names_up:\n",
    "    # Find the best match for the current company name in df_up within df_waf_rfm\n",
    "    best_match_waf = find_best_match(company_name_up, firm_original_names_waf)\n",
    "    \n",
    "    # Append the match result to the matched_companies list\n",
    "    matched_companies.append((company_name_up, best_match_waf))\n",
    "\n",
    "# Convert the matched_companies list to a DataFrame\n",
    "results_matching = pd.DataFrame(matched_companies, columns=['Company Name Up', 'Best Match in df_waf_rfm'])\n",
    "\n",
    "# Display the results\n",
    "results_matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c534673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of entries where the Best Match is None in results_matching\n",
    "num_none_matches = results_matching['Best Match in df_waf_rfm'].isna().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of entries with 'None' in Best Match:\", num_none_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24638ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge df_up with df_waf_rfm using the results_matching as the merge key\n",
    "df_up_merged = pd.merge(df_up, results_matching, left_on='Company Name', right_on='Company Name Up', how='left')\n",
    "\n",
    "# Step 2 and 3: Iterate over the Company Names in df_up and search in results_matching\n",
    "for index_up, row_up in df_up.iterrows():\n",
    "    company_name_up = row_up['Company Name']\n",
    "    \n",
    "    # Step 4: Check if the Company Name in results_matching is None\n",
    "    best_match_waf = results_matching.loc[results_matching['Company Name Up'] == company_name_up, 'Best Match in df_waf_rfm'].values[0]\n",
    "    if pd.isna(best_match_waf):\n",
    "        # Step 5: If None is found, fill None in the previously added columns from df_waf_rfm\n",
    "        df_up_merged.loc[index_up, df_waf_rfm.columns] = None\n",
    "    else:\n",
    "        # Step 6: If a match is found, extract the row from df_waf_rfm and merge the entries to df_up_merged\n",
    "        row_waf = df_waf_rfm.loc[df_waf_rfm['Firm_original_name'] == best_match_waf]\n",
    "        df_up_merged.loc[index_up, df_waf_rfm.columns] = row_waf.values[0]\n",
    "\n",
    "df_up_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93847f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the number of entries where the Number of employees 2014 is NaN in df_up_merged\n",
    "num_nan_employees = df_up_merged['Number of employees 2014'].isna().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of entries with NaN in Number of employees 2014:\", num_nan_employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134c9cf",
   "metadata": {},
   "source": [
    "NaN-values in the \"Number of employee\"-fields show, that there are no LinkedIn data available. Therefore they are droped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN in the \"Number of employees 2014\" column in df_up_merged\n",
    "df_up_merged.dropna(subset=['Number of employees 2014'], inplace=True)\n",
    "\n",
    "# Reset the index after dropping rows\n",
    "df_up_merged.reset_index(drop=True, inplace=True)\n",
    "df_up_merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfefe503",
   "metadata": {},
   "source": [
    "Matching was successful in df_up_merged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f55ad1",
   "metadata": {},
   "source": [
    "# 3. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a5dab",
   "metadata": {},
   "source": [
    "During data preparation, the data are first checked in general (3.1). Then missing values (3.1.1), duplicates (3.1.2), non-numeric columns (3.1.4) and the distribution of the target variables are checked (3.1.5). Due to the data type, data outliers can only be checked downstream. Therefore, an initial data cleaning is carried out in 3.2. Columns that are not needed are removed (3.2.1), the data type is corrected (3.2.2), the column country (3.2.3) and industry (3.2.4) are cleaned. On this basis, the data outliers in 3.3.1 can be examined. Subsequently, the content-related data distribution is checked (3.3.2, 3.3.3). The final data cleaning is carried out in chapter 3.4. Values that should not be considered are removed (3.4.1, 3.4.2), coding is done where necessary (3.4.3), empty values are handled (3.4.4). The data are put into a time series format in 3.5. Finally, collinarity and multicollinarity are checked (3.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5dfed0",
   "metadata": {},
   "source": [
    "## 3.1 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae0997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_up_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75170f8e",
   "metadata": {},
   "source": [
    "The following column do not add value to the context and are therefor not needed:\n",
    "- Adress\n",
    "- S&P Entity ID\n",
    "- Excel Company ID\n",
    "- Index Constituents [Secondary Listings]\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)\n",
    "- &P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (Rating Date)\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)\t\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch Date)\n",
    "- S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Outlook)\n",
    "- S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Outlook Date)\n",
    "- the author decided to focus on the timeseries 2014-2018. Therefore the values for 2013 and >2018 can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547be9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining the size and scope of the data set\n",
    "print('The dataset has {} rows and {} columns. This results in {} data entries.'.format(df_up_merged.shape[0],df_up_merged.shape[1], df_up_merged.size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b9d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze the data types of columns in df_up_merged\n",
    "column_data_types = df_up_merged.dtypes\n",
    "\n",
    "# Set the option to display all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the result\n",
    "print(\"Data Types of Columns in df_up_merged:\")\n",
    "print(column_data_types.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684223dd",
   "metadata": {},
   "source": [
    "Apart from the first 11 columns, the other entries are numbers. These must be converted into float values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b935a1",
   "metadata": {},
   "source": [
    "### 3.1.1 Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check for null values in df_up_merged\n",
    "null_counts = df_up_merged.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of null values in each column of df_up_merged:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f062c4",
   "metadata": {},
   "source": [
    "The following columns contain highest Number of None/NaN fields that need to be cleaned:\n",
    "- Gross Profit/ Employee 2018\n",
    "- All Rating and changes in Rating\n",
    "- Consider dropping companys that have missing values in financials.\n",
    "\n",
    "Columns are included that are no longer needed and contain some empty values. Author knowledge is beeing used. These are: \n",
    "- Rating 2018 ALT \n",
    "- Rating 2012\n",
    "- Change 2012/2013 (does not concern analysis period)\n",
    "- New joining work experience 2014 (empty)\n",
    "- Number of Notices 2018\n",
    "- Number of notices 2018 (empty due to missing values in 2019)\n",
    "- Number of New Joiners 2014 (authors knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb8ef0",
   "metadata": {},
   "source": [
    "### 3.1.2 Checking for dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337a543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicates = df_up_merged[df_up_merged.duplicated()]\n",
    "print(\"Duplicate Rows : \",len(duplicates))\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945805f",
   "metadata": {},
   "source": [
    "As expected there are no dublicates in this dataframe. No cleaning nessercary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886018aa",
   "metadata": {},
   "source": [
    "### 3.1.3 Checking for data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8517c",
   "metadata": {},
   "source": [
    "The checking for data outliers is done later in this notebook. Most columns needs to be converted to a processable formate for numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02382589",
   "metadata": {},
   "source": [
    "### 3.1.4 Inspecting non-numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22889dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_up_merged['Geographic Region'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_region_values = df_up_merged['Geographic Region'].unique()\n",
    "unique_region_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d611f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up_merged['Land'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba847a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_country_values = df_up_merged['Land'].unique()\n",
    "unique_country_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d23ee",
   "metadata": {},
   "source": [
    "Correlation between Geografic Region an Country expected. Geographic Region contains less information and should be droped if needed. Values in country needs to be cleaned since there are the same letter in capital and small letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2495e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_up_merged['Exchange'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_exchange_values = df_up_merged['Exchange'].unique()\n",
    "unique_exchange_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c312cfc",
   "metadata": {},
   "source": [
    "INFO: \n",
    "- OM: Nasdaq OMX Nordic, a stock exchange in Sweden, Denmark, Finland, and Iceland;\n",
    "- SWX: SIX Swiss Exchange, the Swiss stock exchange;\n",
    "- NYSE: New York Stock Exchange, the stock exchange in New York City, USA;\n",
    "- ENXTPA: Euronext Paris, the French stock exchange;\n",
    "- NasdaqGS: Nasdaq Global Select Market, a US-based stock exchange, part of the Nasdaq Stock Market;\n",
    "- XTRA: Frankfurt Stock Exchange, the stock exchange in Frankfurt, Germany;\n",
    "- ENXTAM: Euronext Amsterdam, the Dutch stock exchange;\n",
    "- BME: Bolsas y Mercados Españoles, the stock exchange in Spain;\n",
    "- LSE: London Stock Exchange, the stock exchange in London, United Kingdom;\n",
    "- ENXTBR: Euronext Brussels, the stock exchange in Belgium;\n",
    "- BIT: Borsa Italiana, the stock exchange in Italy;\n",
    "- ISE: Irish Stock Exchange, the stock exchange in Ireland;\n",
    "- CPSE: Euronext Lisbon, the stock exchange in Portugal;\n",
    "- WBAG: Wiener Börse AG, the stock exchange in Austria;\n",
    "- OB: Oslo Børs, the stock exchange in Norway;\n",
    "- HLSE: Helsinki Stock Exchange, the stock exchange in Finland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of Tickers. They act as unique identifier per company and should be kept. \n",
    "unique_Ticker_count = df_up_merged['Ticker'].nunique()\n",
    "unique_Ticker_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a3ab7",
   "metadata": {},
   "source": [
    "No cleaning of column ticker needed from a subject specific point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fcefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_industry_values = df_up_merged['S&P RatingsDirect® Industry'].unique()\n",
    "unique_industry_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678a2e6",
   "metadata": {},
   "source": [
    "Cleaning tasks: \"Corporates; Industrials\" is a pre configuration and can be droped. The main industry following in the breakdown is the intresting one and needs to be keept. All the other information are considered details and should be droped. Also rename the column to \"Industry\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for unique values\n",
    "columns_to_check = ['Rating 2014', 'Rating 2015', 'Rating 2016', 'Rating 2017', 'Rating 2018', 'Rating 2019']\n",
    "\n",
    "for column in columns_to_check:\n",
    "    # Get the unique values in the specified column\n",
    "    unique_values = df_up_merged[column].unique()\n",
    "\n",
    "    # Print the unique values for the current column\n",
    "    print(\"Unique values for \" + column + \":\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775cb60",
   "metadata": {},
   "source": [
    "Variables need to be converted to kategorial features to use them in futher analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5787e",
   "metadata": {},
   "source": [
    "###  3.1.5 Checking for target variable downgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for unique values\n",
    "columns_to_check = [\"Change 2013/14\", \"Change 2015/16\", \"Change 2016/17\", \"Change 2017/18\", \"Change 2018/19\"]\n",
    "\n",
    "for column in columns_to_check:\n",
    "    # Get the unique values in the specified column\n",
    "    unique_values = df_up_merged[column].unique()\n",
    "\n",
    "    # Print the unique values for the current column\n",
    "    print(\"Unique values for \" + column + \":\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "downgrade_2017_18_count = df_up_merged[\"Change 2017/18\"].value_counts()[\"down grade\"]\n",
    "downgrade_2018_19_count = df_up_merged[\"Change 2018/19\"].value_counts()[\"down grade\"]\n",
    "\n",
    "print(\"Number 'down grade' Change 2017/18:\", downgrade_2017_18_count)\n",
    "print(\"Number 'down grade' Change 2018/19:\", downgrade_2018_19_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b4191",
   "metadata": {},
   "source": [
    "Poor database of down grades. For the trainings data syntetic data are going to be needed.\n",
    "\n",
    "One Hot Encoding for 2018 an 2017 necessary. Numerical coding is not chosen because it is the target variable and a binary expression is more useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea98",
   "metadata": {},
   "source": [
    "## 3.2 First data cleansing to enable deeper Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f657d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy for better work contorl\n",
    "df_up = df_up_merged.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2447a",
   "metadata": {},
   "source": [
    "### 3.2.1 Droping columns that are not needed or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216dc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\n",
    "    \"Rating 2018 ALT\",\n",
    "    \"Rating 2012\",\n",
    "    \"Change 2012/13\",\n",
    "    \"New joining work experience 2014\",\n",
    "    \"Migrating work experience 2018\",\n",
    "    \"Number of Notices 2018\",\n",
    "    \"Number of notices 2018\", \n",
    "    \"Number of New Joiners 2014\",\n",
    "    'Adress',\n",
    "    'S&P Entity ID',\n",
    "    'Excel Company ID',\n",
    "    'Index Constituents [Secondary Listings]',\n",
    "    'Index Constituents [Primary Listing]',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Rating)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (Rating Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Rating Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Local Currency LT [Latest] (CreditWatch Date)',\n",
    "    'S&P Entity Credit Rating - Issuer Credit Rating - Local Currency LT [Latest] (Outlook)',\n",
    "    'S&P Entity Credit Rating Date - Issuer Credit Rating - Foreign Currency LT [Latest] (Outlook Date)',\n",
    "    \"Market Capitalization [12/31/2013] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2019] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2020] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2013]\",\n",
    "    \"Total Employees - Capital IQ [CY 2019]\",\n",
    "    \"Total Employees - Capital IQ [CY 2020]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2013] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2019] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2020] (€EURmm, Historical rate)\",\n",
    "    \"Copy_Company_Name\", \"Company Name Up\", \"Best Match in df_waf_rfm\", \"Firm_original_name\",\n",
    "    \"Rating 2020\",\n",
    "    \"Rating 2021\",\n",
    "    \"Rating 2022\",\n",
    "    \"Rating 2023\",\n",
    "    \"Change 2019/20\",\n",
    "    \"Change 2020/21\",\n",
    "    \"Change 2021/22\",\n",
    "    \"Change 2022/23\"\n",
    "]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "df_up.drop(columns=columns_to_remove, inplace=True)\n",
    "df_up.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2246e34",
   "metadata": {},
   "source": [
    "### 3.2.2 Converting columns from object to float. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0dafc6",
   "metadata": {},
   "source": [
    "First there is a need to check for special characters (spaces, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_special_characters(df, columns_to_check):\n",
    "    pattern = re.compile(r'[^\\w\\s.]')  # Define a pattern for special characters (everything except letters, numbers, spaces and full stops)\n",
    "    result = []\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        for index, value in df[column].items():\n",
    "            if re.search(pattern, str(value)):\n",
    "                result.append((index, column, value))\n",
    "\n",
    "    if result:\n",
    "        print(\"Folgende Sonderzeichen wurden gefunden:\")\n",
    "        for row in result:\n",
    "            print(f\"Row {row[0]}, Column {row[1]}, Value: {row[2]}\")\n",
    "    else:\n",
    "        print(\"Keine Sonderzeichen in den angegebenen Spalten gefunden.\")\n",
    "\n",
    "\n",
    "columns_to_check = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",]\n",
    "\n",
    "check_for_special_characters(df_up, columns_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b6732",
   "metadata": {},
   "source": [
    "There are negative numbers, kommas and also empty fields indicated by -. This charackters needs to be cleaned. \n",
    "Next it must be taken into account whether whole numbers are present or if we decimal numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d39b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for integers in columns\n",
    "def check_for_integers(df, columns_to_check):\n",
    "    integer_columns = []\n",
    "    for column in columns_to_check:\n",
    "        is_integer = df[column].apply(lambda x: str(x).isdigit()).all()\n",
    "        if is_integer:\n",
    "            integer_columns.append(column)\n",
    "    return integer_columns\n",
    "\n",
    "columns_to_convert = [   ]  \n",
    "integer_columns = check_for_integers(df_up, columns_to_convert)\n",
    "\n",
    "if integer_columns:\n",
    "    print(\"The following columns contain integers:\")\n",
    "    print(integer_columns)\n",
    "else:\n",
    "    print(\"No columns with only integers were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8b197",
   "metadata": {},
   "source": [
    "Columns can be converted to float, since dataset only contains dicomal numbers. Last the decimal separator is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [     \n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",]  \n",
    "\n",
    "def check_comma_or_dot(df, columns):\n",
    "    comma_columns = []\n",
    "    dot_columns = []\n",
    "\n",
    "    for column in columns:\n",
    "        if df[column].str.contains(',').any():\n",
    "            comma_columns.append(column)\n",
    "        elif df[column].str.contains('.').any():\n",
    "            dot_columns.append(column)\n",
    "\n",
    "    return comma_columns, dot_columns\n",
    "\n",
    "comma_columns, dot_columns = check_comma_or_dot(df_up, columns_to_convert)\n",
    "\n",
    "print(\"Columns with comma:\")\n",
    "print(comma_columns)\n",
    "\n",
    "print(\"Columns with dot:\")\n",
    "print(dot_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086b1a3",
   "metadata": {},
   "source": [
    "To convert successful equal decimal seperators needs to be used. Therefor kommas are replaced by points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [\"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",]\n",
    "\n",
    "# Replace commas with dots in the relevant columns\n",
    "for column in columns_to_convert:\n",
    "    df_up[column] = df_up[column].str.replace(',', '.')\n",
    "\n",
    "# Print\n",
    "df_up.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943c31b",
   "metadata": {},
   "source": [
    "The - accounting fo an empty value are converted to NaN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633dff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some columns there are - as empty values. Those need to be replaced bevor we can convert to float.\n",
    "columns_to_convert = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "# Replace the \"-\" character with NaN (Not-a-Number) in the relevant columns\n",
    "for column in columns_to_convert:\n",
    "    df_up[column] = df_up[column].replace('-', float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571880c9",
   "metadata": {},
   "source": [
    "Lastly the columns can be converted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba133c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of columns to convert to float and replace \"object\" values with NaN\n",
    "columns_to_convert = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",  \n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "    \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "    \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\",\n",
    "    \"More than once/different position\",\n",
    "]\n",
    "\n",
    "def convert_to_float_with_negatives(value):\n",
    "    try:\n",
    "        # Attempts to convert the value to a float\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        # If the value cannot be converted into a float (e.g. if there is a minus sign in front of a number), return the value unchanged\n",
    "        return value\n",
    "\n",
    "# Convert the columns to the data type \"float\" and keep the negative values\n",
    "for column in columns_to_convert:\n",
    "    df_up[column] = df_up[column].apply(convert_to_float_with_negatives)\n",
    "\n",
    "# Print\n",
    "df_up.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7215c",
   "metadata": {},
   "source": [
    "Check if converting was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe68f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyze the data types of columns in df_up_merged\n",
    "column_data_types = df_up.dtypes\n",
    "\n",
    "# Set the option to display all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the result\n",
    "print(\"Data Types of Columns in df_up:\")\n",
    "print(column_data_types.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee197f",
   "metadata": {},
   "source": [
    "### 3.2.3 Cleaning column country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878dfaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only capital letters\n",
    "df_up[\"Land\"] = df_up[\"Land\"].str.upper()\n",
    "\n",
    "# checking unique values\n",
    "unique_land_values = df_up[\"Land\"].unique()\n",
    "unique_land_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only capital letters\n",
    "df_up[\"Land\"] = df_up[\"Land\"].str.upper()\n",
    "\n",
    "# Convert the 'Land' column to string data type\n",
    "df_up[\"Land\"] = df_up[\"Land\"].astype(str)\n",
    "\n",
    "# checking unique values\n",
    "unique_land_values = df_up[\"Land\"].unique()\n",
    "unique_land_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4279a0e",
   "metadata": {},
   "source": [
    "### 3.2.4 Unify values in Industies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up.rename(columns={\"S&P RatingsDirect® Industry\": \"Industry\"}, inplace=True)\n",
    "df_up.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbf02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove \"Corporates; Industrials;\" from the entries in the \"Industry\" column\n",
    "df_up['Industry'] = df_up['Industry'].str.replace('Corporates; Industrials;', '', regex=False)\n",
    "\n",
    "# Step 2: Remove all words after the first semicolon in the \"Industry\" column\n",
    "df_up['Industry'] = df_up['Industry'].str.split(';').str[0]\n",
    "\n",
    "# Display unique values in the \"Industry\" column\n",
    "unique_industries = df_up['Industry'].unique()\n",
    "unique_industries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29effac6",
   "metadata": {},
   "source": [
    "### 3.2.5 One Hot Encoding for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3ae31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill NaN values in the \"Change 2018/19\" column with \"No Change\" (there are just seven)\n",
    "df_up['Change 2018/19'] = df_up['Change 2018/19'].fillna('No Change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a93346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the number of NaN values in the \"Change 2018/19\" column\n",
    "nan_count_change_2018_19 = df_up['Change 2018/19'].isna().sum()\n",
    "nan_count_change_2018_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8193403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in the \"Change 2018/19\" column with \"No Change\" (there are just seven)\n",
    "df_up['Change 2017/18'] = df_up['Change 2017/18'].fillna('No Change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54cf474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values in the \"Change 2017/18\" column\n",
    "nan_count_change_2017_18 = df_up['Change 2017/18'].isna().sum()\n",
    "nan_count_change_2017_18 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1dfb1",
   "metadata": {},
   "source": [
    "Checking if values are unify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5aa3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the \"Change 2018/19\" column\n",
    "unique_values_change_2018_19 = df_up['Change 2018/19'].unique()\n",
    "unique_values_change_2018_19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf1d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the \"Change 2018/19\" column\n",
    "unique_values_change_2017_18 = df_up['Change 2017/18'].unique()\n",
    "unique_values_change_2017_18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee48a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"No Change\" with \"no change\" in the \"Change 2018/19\" column\n",
    "df_up['Change 2018/19'] = df_up['Change 2018/19'].replace('No Change', 'no change')\n",
    "df_up['Change 2017/18'] = df_up['Change 2017/18'].replace('No Change', 'no change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c230c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values in the \"Change 2018/19\" column\n",
    "unique_values_change_2018_19 = df_up['Change 2018/19'].unique()\n",
    "unique_values_change_2018_19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda54ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for the \"Change 2018/19\" column\n",
    "df_up = pd.get_dummies(df_up, columns=['Change 2018/19'], prefix='Change 2018 19')\n",
    "\n",
    "# Perform one-hot encoding for the \"Change 2017/18\" column\n",
    "df_up = pd.get_dummies(df_up, columns=['Change 2017/18'], prefix='Change 2017 18')\n",
    "\n",
    "# Remove the desired columns for \"Change 2018/19\"\n",
    "columns_to_remove_2018 = ['Change 2018 19_first rating', 'Change 2018 19_no change', 'Change 2018 19_up grade']\n",
    "df_up.drop(columns=columns_to_remove_2018, inplace=True)\n",
    "\n",
    "# Remove the desired columns for \"Change 2017/18\"\n",
    "columns_to_remove_2017 = ['Change 2017 18_first rating', 'Change 2017 18_no change', 'Change 2017 18_up grade']\n",
    "df_up.drop(columns=columns_to_remove_2017, inplace=True)\n",
    "\n",
    "# Rename the column \"Change 2018 19_down grade\" to \"Downgrade 2018\"\n",
    "df_up.rename(columns={'Change 2018 19_down grade': 'Downgrade 2018'}, inplace=True)\n",
    "\n",
    "# Rename the column \"Change 2017 18_down grade\" to \"Downgrade 2017\"\n",
    "df_up.rename(columns={'Change 2017 18_down grade': 'Downgrade 2017'}, inplace=True)\n",
    "\n",
    "# Assign the modified DataFrame back to df_up_wt\n",
    "df_up_wt = df_up\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_up_wt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6a955",
   "metadata": {},
   "source": [
    "## 3.3 Deeper data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c08508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy for better work control / df_pp = pre proccesed\n",
    "df_pp = df_up_wt.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87539987",
   "metadata": {},
   "source": [
    "### 3.3.1 Checking for data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f821ff",
   "metadata": {},
   "source": [
    "Note: Data outliers are checked in gruops to confirm, that there are no obvious errors in the data. Due to the nature of the domain it is not absolutly nessercary to clean the outliers - especially since the source of the financials is Bloomberg, wich accounts as a reliable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Market Capitalization Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26972b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"EBITDA Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeada4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"EBIT Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Net Income Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5418917",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Equity Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "\t\"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Debt Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92eb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Assets Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Debt Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Gross Profit Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Employees - Capital IQ [CY 2014]\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Employees Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Cash from Ops Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Total Revenue Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f221b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    " \"Employee development 2015\",\n",
    "    \"Employee development 2016\",\n",
    "    \"Employee development 2017\",\n",
    "    \"Employee development 2018\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Employee development Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d65249",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "  \"Migrating work experience 2014\",\n",
    "    \"Migrating work experience 2015\",\n",
    "    \"Migrating work experience 2016\",\n",
    "    \"Migrating work experience 2017\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Migrating work experience Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a3b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "    \"New joining work experience 2015\",\n",
    "    \"New joining work experience 2016\",\n",
    "    \"New joining work experience 2017\",\n",
    "    \"New joining work experience 2018\",\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"New joining work experience Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "  \"Fluctuation rate 2014\",\n",
    "    \"Fluctuation rate 2015\",\n",
    "    \"Fluctuation rate 2016\",\n",
    "    \"Fluctuation rate 2017\",\n",
    "    \"Fluctuation rate 2018\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Fluctuation rate Boxplots\")\n",
    "plt.ylabel(\"Market Capitalization (€EURmm)\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20732917",
   "metadata": {},
   "source": [
    "Form a statistical point of view I would use at least a 98 % quantil.From a professional point of view most data outliers make sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cb323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "\"More than once/different position\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp[selected_columns].boxplot()\n",
    "plt.title(\"Serveral positions Boxplots\")\n",
    "plt.ylabel(\"Number of people who worked there in more than one position\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be019e5",
   "metadata": {},
   "source": [
    "### 3.3.2 Data allocation with respect to the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a89bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the entries for \"Downgrade 2017\"\n",
    "downgrade_2017_counts = df_pp['Downgrade 2017'].value_counts()\n",
    "print(\"Downgrade 2017:\")\n",
    "print(downgrade_2017_counts)\n",
    "\n",
    "# Count the entries for \"Downgrade 2018\"\n",
    "downgrade_2018_counts = df_pp['Downgrade 2018'].value_counts()\n",
    "print(\"\\nDowngrade 2018:\")\n",
    "print(downgrade_2018_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e13167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the value counts of \"Downgrade 2017\"\n",
    "value_counts_2017 = downgrade_2017_counts\n",
    "\n",
    "# Calculate the value counts of \"Downgrade 2018\"\n",
    "value_counts_2018 = downgrade_2018_counts\n",
    "\n",
    "# Set up the figure and create two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Pie chart for \"Downgrade 2017\"\n",
    "labels_2017 = ['1', '0']\n",
    "values_2017 = [value_counts_2017.get(1, 0), value_counts_2017.get(0, 0)]\n",
    "explode_2017 = [0.1, 0]\n",
    "ax1.pie(values_2017, labels=labels_2017, autopct='%1.2f%%', explode=explode_2017)\n",
    "ax1.set_aspect('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.\n",
    "ax1.set_title('Distribution of Downgrade 2017')\n",
    "\n",
    "# Pie chart for \"Downgrade 2018\"\n",
    "labels_2018 = ['1', '0']\n",
    "values_2018 = [value_counts_2018.get(1, 0), value_counts_2018.get(0, 0)]\n",
    "explode_2018 = [0.1, 0]\n",
    "ax2.pie(values_2018, labels=labels_2018, autopct='%1.2f%%', explode=explode_2018)\n",
    "ax2.set_aspect('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.\n",
    "ax2.set_title('Distribution of Downgrade 2018')\n",
    "\n",
    "# Display the pie charts\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff0034",
   "metadata": {},
   "source": [
    "After Split in test & train data the train data should be oversampled using the SMOTE technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe07a6",
   "metadata": {},
   "source": [
    "### 3.2.3 Checking distribution in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0d8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each country in the 'Land' column\n",
    "country_counts = df_pp['Land'].value_counts()\n",
    "\n",
    "# Create a pie chart to visualize the distribution\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(country_counts, labels=country_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution of Companies by Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f755b3b",
   "metadata": {},
   "source": [
    "Most companies are from the USA. Second biggest group is GB, followed by Switzerland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each country in the 'Land' column\n",
    "country_counts = df_pp['Industry'].value_counts()\n",
    "\n",
    "# Create a DataFrame to store the counts and percentage\n",
    "country_distribution = pd.DataFrame({'Industry': country_counts.index, 'Count': country_counts.values})\n",
    "\n",
    "# Calculate the percentage of each country in the 'Land' column\n",
    "total_countries = len(df_pp['Industry'])\n",
    "country_distribution['Percentage'] = (country_distribution['Count'] / total_countries) * 100\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "country_distribution = country_distribution.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the tabular view of the distribution\n",
    "print(country_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4468837",
   "metadata": {},
   "source": [
    "Insurances should not be contained and need to be removed. The focus of research are corporate companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentages for different years\n",
    "def calculate_percentage(row, year):\n",
    "    total_employees = row[\"Total Employees - Capital IQ [CY %d]\" % year]\n",
    "    employees = row[\"Number of employees %d\" % year]\n",
    "    \n",
    "    # Calculate percentage if not NaN and total employees is not zero\n",
    "    percentage = (employees / total_employees) * 100 if (not pd.isna(total_employees) and total_employees != 0) else None\n",
    "    \n",
    "    return percentage\n",
    "\n",
    "# List of years to calculate percentages for\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "\n",
    "# Calculate percentages for each year and apply the function to the DataFrame rows\n",
    "for year in years:\n",
    "    col_name = \"Percentage of employees on Linkedin %d\" % year\n",
    "    df_pp[col_name] = df_pp.apply(calculate_percentage, axis=1, args=(year,))\n",
    "\n",
    "# Displaying the results\n",
    "output_df = df_pp[[\"Company Name\"] + [\"Percentage of employees on Linkedin %d\" % year for year in years]]\n",
    "output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for percentage ranges\n",
    "bins = [0, 5, 10, 15, 20, float('inf')]  # The last bin represents 20% or more\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['<5%', '5-10%', '10-15%', '15-20%', '20%+']\n",
    "\n",
    "# Create a new column with bins\n",
    "output_df['Percentage Range 2017'] = pd.cut(output_df['Percentage of employees on Linkedin 2017'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "percentage_counts = output_df['Percentage Range 2017'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34348705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for percentage ranges\n",
    "bins = [0, 5, 10, 15, 20, float('inf')]  # The last bin represents 20% or more\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['<5%', '5-10%', '10-15%', '15-20%', '20%+']\n",
    "\n",
    "# Create a new column with bins\n",
    "output_df['Percentage Range 2018'] = pd.cut(output_df['Percentage of employees on Linkedin 2018'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "percentage_counts = output_df['Percentage Range 2018'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61d413",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter the rows where the value in \"Downgrade 2017\" column is 1\n",
    "filtered_rows_2017 = df_pp[df_pp['Downgrade 2017'] == 1]\n",
    "\n",
    "# Filter the rows where the value in \"Downgrade 2018\" column is 1\n",
    "filtered_rows_2018 = df_pp[df_pp['Downgrade 2018'] == 1]\n",
    "\n",
    "# Combine the filtered rows for both years\n",
    "filtered_rows_combined = pd.concat([filtered_rows_2017, filtered_rows_2018])\n",
    "\n",
    "# Extract the \"Company Name\" from the filtered rows\n",
    "company_names = filtered_rows_combined['Company Name']\n",
    "\n",
    "# Filter and display the corresponding rows in \"output_df\" based on the \"Company Name\" values\n",
    "result_df = output_df[output_df['Company Name'].isin(company_names)]\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for percentage ranges\n",
    "bins = [0, 5, 10, 15, 20, float('inf')]  # The last bin represents 20% or more\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['<5%', '5-10%', '10-15%', '15-20%', '20%+']\n",
    "\n",
    "# Create a new column with bins\n",
    "result_df['Percentage Range 2018'] = pd.cut(result_df['Percentage of employees on Linkedin 2018'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the occurrences in each bin\n",
    "percentage_counts = result_df['Percentage Range 2018'].value_counts()\n",
    "\n",
    "# Display the result\n",
    "percentage_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ccbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['Percentage of employees on Linkedin 2018', 'Percentage of employees on Linkedin 2014', 'Percentage of employees on Linkedin 2015', 'Percentage of employees on Linkedin 2016', 'Percentage of employees on Linkedin 2017']\n",
    "df_pp = df_pp.drop(columns_to_remove, axis=1)\n",
    "df_pp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eeee08",
   "metadata": {},
   "source": [
    "Defaultet companys show a rather low percentage of employees on LinkedIn. Distribution doesnt change over the year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcd124",
   "metadata": {},
   "source": [
    "Majority of the companies has a percentage below 5%. Take into account, that there are no employee numbers for around 60-70 companies, wich results in 0. The reduction can be explained by the general reduction in the data in 2018. The LinkedIn dataset was probably retrieved during 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa759bb1",
   "metadata": {},
   "source": [
    "## 3.4 Final data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp2 = df_pp.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2dee3a",
   "metadata": {},
   "source": [
    "### 3.4.1 Cleaning data outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22478a73",
   "metadata": {},
   "source": [
    "This feature counts the numberr of people who have worked in diffrent position in the company. Even though 120.000 might be realistic in bigger firms, it is cleaned here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_pp2 is your DataFrame\n",
    "quantile_98 = df_pp2['More than once/different position'].quantile(0.98)\n",
    "\n",
    "# Filter the DataFrame to keep only values up to the 98% quantile\n",
    "df_pp2 = df_pp2[df_pp2['More than once/different position'] <= quantile_98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\n",
    "\"More than once/different position\"\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Adjust the figure size if needed\n",
    "\n",
    "df_pp2[selected_columns].boxplot()\n",
    "plt.title(\"Serveral positions Boxplots\")\n",
    "plt.ylabel(\"Number of people who worked there in more than one position\")\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc300a9",
   "metadata": {},
   "source": [
    "### 3.4.2 Removing Industry Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_pp2 is your DataFrame\n",
    "df_pp2 = df_pp2[df_pp2['Industry'] != 'Insurance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each country in the 'Land' column\n",
    "country_counts = df_pp2['Industry'].value_counts()\n",
    "\n",
    "# Create a DataFrame to store the counts and percentage\n",
    "country_distribution = pd.DataFrame({'Industry': country_counts.index, 'Count': country_counts.values})\n",
    "\n",
    "# Calculate the percentage of each country in the 'Land' column\n",
    "total_countries = len(df_pp2['Industry'])\n",
    "country_distribution['Percentage'] = (country_distribution['Count'] / total_countries) * 100\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "country_distribution = country_distribution.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the tabular view of the distribution\n",
    "print(country_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3150e4d",
   "metadata": {},
   "source": [
    "Removal of Incurance successful. Checking effect on target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a214110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count the entries for \"Downgrade 2017\"\n",
    "downgrade_2017_counts = df_pp['Downgrade 2017'].value_counts()\n",
    "print(\"Downgrade 2017:\")\n",
    "print(downgrade_2017_counts)\n",
    "\n",
    "# Count the entries for \"Downgrade 2018\"\n",
    "downgrade_2018_counts = df_pp['Downgrade 2018'].value_counts()\n",
    "print(\"\\nDowngrade 2018:\")\n",
    "print(downgrade_2018_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767c36e",
   "metadata": {},
   "source": [
    "### 3.4.3 Putting Rating in kategorial values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1ff06",
   "metadata": {},
   "source": [
    "The ratings need to be put in kategorial variables to be useful in futher analysis. Integer encoding can be used. The rating contains a score that is reflected in the ascending values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281dc0e",
   "metadata": {},
   "source": [
    "First it is checked if there are rows without a rating in the relevant years from 2014 to 2018. The ratings are researched and inputed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the specified conditions\n",
    "filtered_companies = df_pp2[(df_pp2['Rating 2014'] == 0) &\n",
    "                           (df_pp2['Rating 2015'] == 0) &\n",
    "                           (df_pp2['Rating 2016'] == 0) &\n",
    "                           (df_pp2['Rating 2017'] == 0) &\n",
    "                           (df_pp2['Rating 2018'] == 0)]\n",
    "\n",
    "# Get the names of the companies from the filtered DataFrame\n",
    "company_names = filtered_companies[\"Company Name\"].tolist()\n",
    "\n",
    "print(\"Company Names with all Rating values from 2014 to 2018 as 0:\")\n",
    "print(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated rating_mapping list\n",
    "rating_mapping = {\n",
    "    'AMETEK Inc': 'BBB+',\n",
    "    'ASM International NV': 'BB+', \n",
    "    'Atlas Copco AB': 'A+',\n",
    "    'STMicroelectronics NV': 'BBB', \n",
    "    'Welltower Inc': 'BBB+',\n",
    "    'ABB Ltd': 'A-',\n",
    "    'Adecco Group AG': 'BBB+',\n",
    "    'adidas AG': 'A-',\n",
    "    'Advance Auto Parts Inc': 'BBB-',\n",
    "    'Advanced Micro Devices Inc': 'A-',\n",
    "    'Alcon Inc': 'BBB',\n",
    "    'Alliant Energy Corporation': 'A-',\n",
    "    'American Airlines Group Inc': 'B-',\n",
    "    'American Electric Power Company Inc': 'A-',\n",
    "    'Anglo American plc': 'BBB+',\n",
    "    'APA Corporation': 'BBB',\n",
    "    'Associated British Foods plc': 'A',\n",
    "    'BioRad Laboratories Inc': 'BBB',\n",
    "    'Brenntag SE': 'BBB',\n",
    "    'British American Tobacco plc': 'BBB+',\n",
    "    'BT Group plc': 'BBB',\n",
    "    'Bunge Limited': 'BBB',\n",
    "    'Bunzl plc': 'BBB+',\n",
    "    'Capgemini SE': 'BBB',\n",
    "    'Carnival Corporation  plc': 'BBB-',\n",
    "    'CDW Corporation': 'BBB-',\n",
    "    'Celanese Corporation': 'BBB-',\n",
    "    'CF Industries Holdings Inc': 'BBB',\n",
    "    'Charter Communications Inc': 'BB+',\n",
    "    'Cintas Corporation': 'A-',\n",
    "    'CNH Industrial NV': 'BBB',\n",
    "    'ConocoPhillips': 'A-',\n",
    "    'Consolidated Edison Inc': 'A-',\n",
    "    'Constellation Energy Corporation': 'BBB-',\n",
    "    'Corteva Inc': 'A-',\n",
    "    'CoStar Group Inc': 'BB+',\n",
    "    'DaVita Inc': 'BB',\n",
    "    'Devon Energy Corporation': 'BBB',\n",
    "    'Diageo plc': 'A-',\n",
    "    'Dow Inc': 'BBB',\n",
    "    'Eaton Corporation plc': 'A-',\n",
    "    'EQT Corporation': 'BBB-',\n",
    "    'Essex Property Trust Inc': 'BBB+',\n",
    "    'Experian plc': 'A-',\n",
    "    'Ferrovial SA': 'BBB',\n",
    "    'Fortinet Inc': 'BBB+',\n",
    "    'GE HealthCare Technologies Inc': 'BBB',\n",
    "    'Genuine Parts Company': 'BBB',\n",
    "    'Givaudan SA': 'A-',\n",
    "    'Glencore plc': 'BBB+',\n",
    "    'HCA Healthcare Inc': 'BBB',\n",
    "    'Hilton Worldwide Holdings Inc': 'BB+',\n",
    "    'Hormel Foods Corporation': 'A-',\n",
    "    'Host Hotels  Resorts Inc': 'BBB-',\n",
    "    'HP Inc': 'BBB',\n",
    "    'Huntington Ingalls Industries Inc': 'BBB-',\n",
    "    'Iberdrola SA': 'BBB+',\n",
    "    'IDEX Corporation': 'BBB',\n",
    "    'Imperial Brands PLC': 'BBB',\n",
    "    'Ingersoll Rand Inc': 'BBB-',\n",
    "    'Kerry Group plc': 'BBB+',\n",
    "    'Las Vegas Sands Corp': 'BB+',\n",
    "    'LyondellBasell Industries NV': 'BBB',\n",
    "    'Marks and Spencer Group plc': 'BBB-',\n",
    "    'Medtronic plc': 'A',\n",
    "    'Mohawk Industries Inc': 'BBB+',\n",
    "    'Mondi plc': 'BBB+',\n",
    "    'Motorola Solutions Inc': 'BBB-',\n",
    "    'News Corporation': 'BB+',\n",
    "    'NextEra Energy Inc': 'A-',\n",
    "    'Novo Nordisk AS': 'AA-',\n",
    "    'OMV Aktiengesellschaft': 'AA+',\n",
    "    'Organon  Co': 'BB',\n",
    "    'Pentair plc': 'BBB-',\n",
    "    'Pinnacle West Capital Corporation': 'BBB+',\n",
    "    'Pioneer Natural Resources Company': 'BBB',\n",
    "    'Porsche Automobil Holding SE': 'BBB+',\n",
    "    'PPL Corporation': 'A-',\n",
    "    'PTC Inc': 'BB+',\n",
    "    'Qorvo Inc': 'BBB-',\n",
    "    'Quanta Services Inc': 'BBB-',\n",
    "    'Regency Centers Corporation': 'BBB+',\n",
    "    'Regeneron Pharmaceuticals Inc': 'BBB+',\n",
    "    'Rio Tinto Group': 'A',\n",
    "    'Roche Holding AG': 'AA',\n",
    "    'RollsRoyce Holdings plc': 'BBB',\n",
    "    'Safran SA': 'A-',\n",
    "    'Schlumberger Limited': 'A',\n",
    "    'ServiceNow Inc': 'A-',\n",
    "    'Severn Trent PLC': 'BBB',\n",
    "    'Skyworks Solutions Inc': 'BBB-',\n",
    "    'Smurfit Kappa Group Plc': 'BBB-',\n",
    "    'TakeTwo Interactive Software Inc': 'BBB',\n",
    "    'Targa Resources Corp': 'BBB-',\n",
    "    'TE Connectivity Ltd': 'A-',\n",
    "    'Teledyne Technologies Incorporated': 'BBB',\n",
    "    'The Kraft Heinz Company': 'BBB',\n",
    "    'The Sage Group plc': 'BBB+',\n",
    "    'Thermo Fisher Scientific Inc': 'A-',\n",
    "    'United Rentals Inc': 'BB+',\n",
    "    'United Utilities Group PLC': 'A-',\n",
    "    'Universal Music Group NV': 'BBB',\n",
    "    'UPMKymmene Oyj': 'BBB',\n",
    "    'Vonovia SE': 'BBB+',\n",
    "    'WestRock Company': 'BBB',\n",
    "    'WPP plc': 'BBB',\n",
    "}\n",
    "\n",
    "# Iterate over the rows of the DataFrame and set the ratings accordingly\n",
    "for index, row in df_pp2.iterrows():\n",
    "    company_name = row['Company Name']\n",
    "    rating = rating_mapping.get(company_name)\n",
    "    if rating is not None:\n",
    "        df_pp2.at[index, 'Rating 2014'] = rating\n",
    "        df_pp2.at[index, 'Rating 2015'] = rating\n",
    "        df_pp2.at[index, 'Rating 2016'] = rating\n",
    "        df_pp2.at[index, 'Rating 2017'] = rating\n",
    "        df_pp2.at[index, 'Rating 2018'] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the columns and output the unique values\n",
    "for year in range(2013, 2019):\n",
    "    column_name = f'Rating {year}'\n",
    "    unique_values = df_pp2[column_name].unique()\n",
    "    print(f'Unique values in {column_name}: {unique_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf82dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer Encoding\n",
    "# Create a dictionary to map the original values to the categorical variables\n",
    "rating_mapping = {\n",
    "    0: -1,    # Added: 0 as numerical value for zeros\n",
    "    \"AA+\": 1,\n",
    "    \"AA\": 2,\n",
    "    \"AA-\": 3,\n",
    "    \"A+\": 4,\n",
    "    \"A\": 5,\n",
    "    \"A-\": 6,\n",
    "    \"BBB+\": 7,\n",
    "    \"BBB\": 8,\n",
    "    \"BBB-\": 9,\n",
    "    \"BB+\": 10,\n",
    "    \"BB\": 11,\n",
    "    \"BB-\": 12,\n",
    "    \"B+\": 13,\n",
    "    \"B\": 14,\n",
    "    \"B-\": 15,\n",
    "    \"CCC\": 16\n",
    "}\n",
    "\n",
    "# Include NaN and Null values in the mapping with -1\n",
    "rating_mapping[np.nan] = -1\n",
    "rating_mapping[None] = -1\n",
    "\n",
    "# Loop through the years and convert the values in each \"Rating\" column to categorical variables\n",
    "for year in range(2013, 2020):\n",
    "    column_name = f'Rating {year}'\n",
    "    df_pp2[column_name] = df_pp2[column_name].replace(rating_mapping)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pp2.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ed116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences where all Rating values from 2014 to 2018 are 0\n",
    "count_null_ratings = df_pp2[(df_pp2['Rating 2014'] == -1) &\n",
    "                            (df_pp2['Rating 2015'] == -1) &\n",
    "                            (df_pp2['Rating 2016'] == -1) &\n",
    "                            (df_pp2['Rating 2017'] == -1) &\n",
    "                            (df_pp2['Rating 2018'] == -1)]\n",
    "\n",
    "# Display the company names where all ratings from 2014 to 2018 are 0\n",
    "company_names_with_null_ratings = count_null_ratings['Company Name'].tolist()\n",
    "print(\"Company Names where all Rating values from 2014 to 2018 are 0:\")\n",
    "print(company_names_with_null_ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7398a",
   "metadata": {},
   "source": [
    "### 3.4.4 Handling empty entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check for null values in df_up_merged\n",
    "null_counts = df_pp2.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of null values in each column of df_up_merged:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f73381e",
   "metadata": {},
   "source": [
    "The following adjustments are made: \n",
    "- Change <year>: Set \"no change\"\n",
    "- Financials: median of the column\n",
    "- Total Employees <year>: use following year or mean\n",
    "- Gross Profit/ Employee 2018: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1978d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_missing_values = [\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\",\n",
    "    \"Equity ratio 2018\",\n",
    "    \"Debt ratio (in Prozent) 2018\",\n",
    "    \"Debt-equity ratio 2018\",  \n",
    "    \"Return on equity 2018\",\n",
    "    \"Return on sales 2018\",\n",
    "]\n",
    "\n",
    "# Iterate over the selected columns\n",
    "for col in columns_with_missing_values:\n",
    "    # Identify rows with missing values (NaN or empty)\n",
    "    missing_values_mask = df_pp2[col].isnull() | (df_pp2[col] == '')\n",
    "\n",
    "    # Calculate the median value of the column excluding the missing values\n",
    "    median_value = df_pp2.loc[~missing_values_mask, col].median()\n",
    "\n",
    "    # Replace the missing values with the median value\n",
    "    df_pp2.loc[missing_values_mask, col] = median_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e23966",
   "metadata": {},
   "source": [
    "Filling the Total Employyes - either with future value or with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2015]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2016]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2016]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2017]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2018]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2014 = df_pp2['Total Employees - Capital IQ [CY 2014]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2014]'] = df_pp2['Total Employees - Capital IQ [CY 2014]'].fillna(median_employees_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2016]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2016]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2018]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2015 = df_pp2['Total Employees - Capital IQ [CY 2015]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2015]'] = df_pp2['Total Employees - Capital IQ [CY 2015]'].fillna(median_employees_2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5743093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2016]'] = df_pp2['Total Employees - Capital IQ [CY 2016]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'])\n",
    "\n",
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2016]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2016]'] = df_pp2['Total Employees - Capital IQ [CY 2016]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2016 = df_pp2['Total Employees - Capital IQ [CY 2016]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2016]'] = df_pp2['Total Employees - Capital IQ [CY 2016]'].fillna(median_employees_2016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe, ob die Spalte \"Total Employees - Capital IQ [CY 2015]\" gefüllt ist\n",
    "df_pp2['Total Employees - Capital IQ [CY 2017]'] = df_pp2['Total Employees - Capital IQ [CY 2017]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2018]'])\n",
    "\n",
    "# Berechne den Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "median_employees_2017 = df_pp2['Total Employees - Capital IQ [CY 2017]'].median()\n",
    "\n",
    "# Fülle die verbleibenden fehlenden Werte mit dem Median der Spalte \"Total Employees - Capital IQ [CY 2014]\"\n",
    "df_pp2['Total Employees - Capital IQ [CY 2017]'] = df_pp2['Total Employees - Capital IQ [CY 2017]'].fillna(median_employees_2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0263f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if \"Total Employees - Capital IQ [CY 2018]\" is filled\n",
    "if df_pp2['Total Employees - Capital IQ [CY 2018]'].notnull().any():\n",
    "    # Fill missing values in \"Total Employees - Capital IQ [CY 2018]\" with values from \"Total Employees - Capital IQ [CY 2017]\"\n",
    "    df_pp2['Total Employees - Capital IQ [CY 2018]'].fillna(df_pp2['Total Employees - Capital IQ [CY 2017]'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8aeba",
   "metadata": {},
   "source": [
    "Droping Gross Profit / Employee 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde1998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the column \"Gross Profit/ Employee 2018\" from df_pp2\n",
    "df_pp2.drop(\"Gross Profit/ Employee 2018\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38cceec",
   "metadata": {},
   "source": [
    "No change is beeing set as a value for the missing change indivators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc81a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check and fill with \"no change\"\n",
    "columns_to_fill_with_no_change = [\n",
    "    'Change 2013/14',\n",
    "    'Change 2014/15',\n",
    "    'Change 2015/16',\n",
    "    'Change 2016/17',\n",
    "]\n",
    "\n",
    "# Fill the NaN values in the specified columns with \"no change\"\n",
    "df_pp2[columns_to_fill_with_no_change] = df_pp2[columns_to_fill_with_no_change].fillna(\"no change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abefec9",
   "metadata": {},
   "source": [
    "Check if all missing fields are eliminated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aff56d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the option to display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Check for null values in df_up_merged\n",
    "null_counts = df_pp2.isnull().sum()\n",
    "\n",
    "# Display the result\n",
    "print(\"Number of null values in each column of df_up_merged:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23ca92",
   "metadata": {},
   "source": [
    "Cleaning of missing values successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee31df7",
   "metadata": {},
   "source": [
    "## 3.5 Transforming into time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae726bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp3 = df_pp2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to be dropped\n",
    "columns_to_drop = [\"Exchange\", \"Land\", \"Industry\",\"Ticker\", \"Geographic Region\", \"Change 2013/14\", \"Change 2014/15\", \"Change 2015/16\", \"Change 2016/17\", \"Equity ratio 2018\", \"Debt ratio (in Prozent) 2018\", \"Debt-equity ratio 2018\",\"Return on equity 2018\",\"Return on sales 2018\",]\n",
    "\n",
    "# Drop the irrelevant columns\n",
    "df_pp3 = df_pp3.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new DataFrame with the desired structure\n",
    "years = [2014, 2015, 2016, 2017, 2018]\n",
    "df_pp3_new = pd.DataFrame(columns=[\"Company Name\", \"Years\"] + years)\n",
    "\n",
    "# Get unique values from the \"Company Name\" column in the original DataFrame\n",
    "unique_companies = df_pp3[\"Company Name\"].unique()\n",
    "\n",
    "# Iterate over each unique company\n",
    "for company in unique_companies:\n",
    "    # Create a dictionary to hold the data for the current company\n",
    "    company_data = {\"Company Name\": [company] * len(years), \"Years\": years}\n",
    "    \n",
    "    # Append the dictionary to the new DataFrame\n",
    "    df_pp3_new = df_pp3_new.append(pd.DataFrame(company_data), ignore_index=True)\n",
    "\n",
    "# Merge the new DataFrame with the original DataFrame on \"Company Name\"\n",
    "df_pp3_final = df_pp3_new.merge(df_pp3, on=\"Company Name\", how=\"left\")\n",
    "\n",
    "# Reorder the columns\n",
    "df_pp3_final = df_pp3_final[[\"Company Name\", \"Years\"]]\n",
    "\n",
    "# Encode \"Years\" column as datetime64 data type and then format as 'YYYY'\n",
    "df_pp3_final[\"Years\"] = pd.to_datetime(df_pp3_final[\"Years\"], format='%Y').dt.strftime('%Y')\n",
    "df_pp3_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413254c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a mapping of old column names to new column names\n",
    "column_mapping = {\n",
    "    \"Market Capitalization [12/31/2014] (€EURmm, Historical rate)\": \"Market Capitalization 2014\",\n",
    "    \"Market Capitalization [12/31/2015] (€EURmm, Historical rate)\": \"Market Capitalization 2015\",\n",
    "    \"Market Capitalization [12/31/2016] (€EURmm, Historical rate)\": \"Market Capitalization 2016\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2017] (€EURmm, Historical rate)\": \"Market Capitalization 2017\",\n",
    "    \"Market Capitalization [My Setting] [12/31/2018] (€EURmm, Historical rate)\": \"Market Capitalization 2018\",\n",
    "    \"EBITDA - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"EBITDA 2014\",\n",
    "    \"EBITDA - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"EBITDA 2015\",\n",
    "    \"EBITDA - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"EBITDA 2016\",\n",
    "    \"EBITDA - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"EBITDA 2017\",\n",
    "    \"EBITDA - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"EBITDA 2018\",\n",
    "    \"EBIT - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"EBIT 2014\",\n",
    "    \"EBIT - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"EBIT 2015\",\n",
    "    \"EBIT - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"EBIT 2016\",\n",
    "    \"EBIT - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"EBIT 2017\",\n",
    "    \"EBIT - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"EBIT 2018\",\n",
    "    \"Net Income - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Net Income 2014\",\n",
    "    \"Net Income - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Net Income 2015\",\n",
    "    \"Net Income - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Net Income 2016\",\n",
    "    \"Net Income - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Net Income 2017\",\n",
    "    \"Net Income - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Net Income 2018\",\n",
    "    \"Total Equity - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Total Equity 2014\",\n",
    "    \"Total Equity - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Total Equity 2015\",\n",
    "    \"Total Equity - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Total Equity 2016\",\n",
    "    \"Total Equity - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Total Equity 2017\",\n",
    "    \"Total Equity - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Total Equity 2018\",\n",
    "    \"Total Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Total Debt 2014\",\n",
    "    \"Total Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Total Debt 2015\",\n",
    "    \"Total Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Total Debt 2016\",\n",
    "    \"Total Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Total Debt 2017\",\n",
    "    \"Total Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Total Debt 2018\",\n",
    "    \"Total Assets - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Total Assets 2014\",\n",
    "    \"Total Assets - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Total Assets 2015\",\n",
    "    \"Total Assets - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Total Assets 2016\",\n",
    "    \"Total Assets - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Total Assets 2017\",\n",
    "    \"Total Assets - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Total Assets 2018\",\n",
    "    \"Net Debt - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Net Debt 2014\",\n",
    "    \"Net Debt - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Net Debt 2015\",\n",
    "    \"Net Debt - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Net Debt 2016\",\n",
    "    \"Net Debt - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Net Debt 2017\",\n",
    "    \"Net Debt - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Net Debt 2018\",\n",
    "    \"Gross Profit - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Gross Profit 2014\",\n",
    "    \"Gross Profit - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Gross Profit 2015\",\n",
    "    \"Gross Profit - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Gross Profit 2016\",\n",
    "    \"Gross Profit - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Gross Profit 2017\",\n",
    "    \"Gross Profit - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Gross Profit 2018\",\n",
    "    \"Total Employees - Capital IQ [CY 2014]\": \"Total Employees 2014\",\n",
    "    \"Total Employees - Capital IQ [CY 2015]\": \"Total Employees 2015\",\n",
    "    \"Total Employees - Capital IQ [CY 2016]\": \"Total Employees 2016\",\n",
    "    \"Total Employees - Capital IQ [CY 2017]\": \"Total Employees 2017\",\n",
    "    \"Total Employees - Capital IQ [CY 2018]\": \"Total Employees 2018\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Cash from Ops. 2014\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Cash from Ops. 2015\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Cash from Ops. 2016\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Cash from Ops. 2017\",\n",
    "    \"Cash from Ops. - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Cash from Ops. 2018\",\n",
    "    \"Total Revenue - Capital IQ [CY 2014] (€EURmm, Historical rate)\": \"Total Revenue 2014\",\n",
    "    \"Total Revenue - Capital IQ [CY 2015] (€EURmm, Historical rate)\": \"Total Revenue 2015\",\n",
    "    \"Total Revenue - Capital IQ [CY 2016] (€EURmm, Historical rate)\": \"Total Revenue 2016\",\n",
    "    \"Total Revenue - Capital IQ [CY 2017] (€EURmm, Historical rate)\": \"Total Revenue 2017\",\n",
    "    \"Total Revenue - Capital IQ [CY 2018] (€EURmm, Historical rate)\": \"Total Revenue 2018\",\n",
    "    \"Average years of service with the company\": \"Average years of service with the company 2018\",\n",
    "    \"More than once/different position\": \"More than once/different position 2018\",\n",
    "}\n",
    "\n",
    "# Rename the columns in df_pp3\n",
    "df_pp3.rename(columns=column_mapping, inplace=True)\n",
    "df_pp3.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f84a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Number of notices 2017 and insert it as Number of notices 2018\n",
    "df_pp3[\"Number of notices 2018\"] = df_pp3[\"Number of notices 2017\"]\n",
    "\n",
    "# Copy Migrating work experience 2017 and insert it as Migrating work experience 2018\n",
    "df_pp3[\"Migrating work experience 2018\"] = df_pp3[\"Migrating work experience 2017\"]\n",
    "\n",
    "# Copy Number of New Joiners 2015 and insert it as Number of New Joiners 2014\n",
    "df_pp3.insert(df_pp3.columns.get_loc(\"Employee development 2015\"), \"Employee development 2014\", df_pp3[\"Employee development 2015\"])\n",
    "\n",
    "\n",
    "# Copy Number of New Joiners 2015 and insert it as Number of New Joiners 2014\n",
    "df_pp3.insert(df_pp3.columns.get_loc(\"Number of New Joiners 2015\"), \"Number of New Joiners 2014\", df_pp3[\"Number of New Joiners 2015\"])\n",
    "\n",
    "# Copy New joining work experience 2015 and insert it as New joining work experience 2014\n",
    "df_pp3.insert(df_pp3.columns.get_loc(\"New joining work experience 2015\"), \"New joining work experience 2014\", df_pp3[\"New joining work experience 2015\"])\n",
    "\n",
    "# Display the updated df_pp3 DataFrame\n",
    "df_pp3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46073736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to look up the value for a given Company Name and Year in a specified column\n",
    "def lookup_value(company_name, year, column_name):\n",
    "    value_column = f\"{column_name} {year}\"\n",
    "    return df_pp3.loc[df_pp3[\"Company Name\"] == company_name, value_column].values[0]\n",
    "\n",
    "# List of columns to update\n",
    "columns_to_update = [\"Rating\",\"Market Capitalization\",\"EBITDA\", \"EBIT\",\"Net Income\",\"Total Equity\",\"Total Debt\",\"Total Assets\",\"Net Debt\",\"Gross Profit\",\"Total Employees\",\"Cash from Ops.\",\"Total Revenue\", \"Number of employees\", \"Employee development\", \"Number of notices\", \"Migrating work experience\",\"Number of New Joiners\", \"New joining work experience\", \"Fluctuation rate\",]\n",
    "\n",
    "# Iterate over each row in df_pp3_final\n",
    "for index, row in df_pp3_final.iterrows():\n",
    "    # Get the Company Name and Year from the current row\n",
    "    company_name = row[\"Company Name\"]\n",
    "    year = row[\"Years\"]\n",
    "    \n",
    "    # Iterate over each column to update\n",
    "    for column_name in columns_to_update:\n",
    "        # Look up the value for the Company Name, Year, and current column\n",
    "        value = lookup_value(company_name, year, column_name)\n",
    "        \n",
    "        # Assign the value to the appropriate cell in df_pp3_final\n",
    "        df_pp3_final.at[index, column_name] = value\n",
    "\n",
    "df_pp3_final.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"Default\" to df_pp3_final and initialize it with the value 0\n",
    "df_pp3_final[\"Downgrade\"] = 0\n",
    "\n",
    "df_pp3_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_with_default_1 = df_pp3[df_pp3[\"Downgrade 2017\"] == 1][\"Company Name\"].unique()\n",
    "print(companies_with_default_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_with_default_1 = df_pp3[df_pp3[\"Downgrade 2018\"] == 1][\"Company Name\"].unique()\n",
    "print(companies_with_default_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2018\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == \"2018\"]\n",
    "\n",
    "# List of companies to filter\n",
    "companies_to_filter = ['ATT Inc', 'Bath  Body Works Inc', 'Carrefour SA', 'Centrica plc',\n",
    "                       'Charles River Laboratories International Inc', 'Duke Energy Corporation',\n",
    "                       'Edison International', 'Eli Lilly and Company', 'Equifax Inc',\n",
    "                       'Eversource Energy', 'Ford Motor Company', 'LKQ Corporation',\n",
    "                       'ParkerHannifin Corporation', 'Publicis Groupe SA', 'QUALCOMM Incorporated',\n",
    "                       'Renault SA', 'Starbucks Corporation', 'Stryker Corporation',\n",
    "                       'The Boeing Company', 'The Walt Disney Company', 'CenterPoint Energy Inc','Gilead Sciences Inc', 'Marriott International Inc']\n",
    "\n",
    "# Filter df_pp3_final_year for the specified companies\n",
    "df_pp3_final_filtered = df_pp3_final_year[df_pp3_final_year[\"Company Name\"].isin(companies_to_filter)]\n",
    "\n",
    "# Set \"Downgrade\" to 1 for the filtered companies\n",
    "df_pp3_final.loc[df_pp3_final_filtered.index, \"Downgrade\"] = 1\n",
    "\n",
    "# Display the updated DataFrame, Check for ATT Inc in 184\n",
    "df_pp3_final.head(185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc382596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each value in the \"Downgrade\" column\n",
    "downgrade_counts = df_pp3_final[\"Downgrade\"].value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(downgrade_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2018\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == \"2017\"]\n",
    "\n",
    "# List of companies to filter\n",
    "companies_to_filter = ['Akzo Nobel NV', 'Campbell Soup Company',\n",
    " 'Charles River Laboratories International Inc', 'CVS Health Corporation',\n",
    " 'Engie SA', 'General Mills Inc', 'Keurig Dr Pepper Inc', 'SSE plc',\n",
    " 'Telia Company AB',]\n",
    "\n",
    "# Filter df_pp3_final_year for the specified companies\n",
    "df_pp3_final_filtered = df_pp3_final_year[df_pp3_final_year[\"Company Name\"].isin(companies_to_filter)]\n",
    "\n",
    "# Set \"Downgrade\" to 1 for the filtered companies\n",
    "df_pp3_final.loc[df_pp3_final_filtered.index, \"Downgrade\"] = 1\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pp3_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c9157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2016\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == \"2016\"]\n",
    "\n",
    "# List of companies to filter\n",
    "companies_to_filter = ['FreeportMcMoRan Inc', 'Ralph Lauren Corporation']\n",
    "\n",
    "# Filter df_pp3_final_year for the specified companies\n",
    "df_pp3_final_filtered = df_pp3_final_year[df_pp3_final_year[\"Company Name\"].isin(companies_to_filter)]\n",
    "\n",
    "# Set \"Downgrade\" to 1 for the filtered companies\n",
    "df_pp3_final.loc[df_pp3_final_filtered.index, \"Downgrade\"] = 1\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pp3_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b50923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2015\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == \"2015\"]\n",
    "\n",
    "# List of companies to filter\n",
    "companies_to_filter = ['ConocoPhillips', 'eBay Inc', 'EOG Resources Inc', 'FreeportMcMoRan Inc']\n",
    "\n",
    "# Filter df_pp3_final_year for the specified companies\n",
    "df_pp3_final_filtered = df_pp3_final_year[df_pp3_final_year[\"Company Name\"].isin(companies_to_filter)]\n",
    "\n",
    "# Set \"Downgrade\" to 1 for the filtered companies\n",
    "df_pp3_final.loc[df_pp3_final_filtered.index, \"Downgrade\"] = 1\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pp3_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2014\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == \"2014\"]\n",
    "\n",
    "# List of companies to filter\n",
    "companies_to_filter = ['FreeportMcMoRan Inc']\n",
    "\n",
    "# Filter df_pp3_final_year for the specified companies\n",
    "df_pp3_final_filtered = df_pp3_final_year[df_pp3_final_year[\"Company Name\"].isin(companies_to_filter)]\n",
    "\n",
    "# Set \"Downgrade\" to 1 for the filtered companies\n",
    "df_pp3_final.loc[df_pp3_final_filtered.index, \"Downgrade\"] = 1\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pp3_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7521374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2014\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == 2014]\n",
    "\n",
    "# Update the Rating for the specified companies\n",
    "new_ratings = {'CenterPoint Energy Inc': 6, 'ConocoPhillips': 5, 'eBay Inc': -1, 'EOG Resources Inc': 6, 'FreeportMcMoRan Inc': 9, 'Marriott International Inc': 8, 'Ralph Lauren Corporation': 5}\n",
    "\n",
    "for company, new_rating in new_ratings.items():\n",
    "    company_indices = df_pp3_final_year[df_pp3_final_year[\"Company Name\"] == company].index\n",
    "    df_pp3_final.loc[company_indices, \"Rating\"] = new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd218004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2015\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == 2015]\n",
    "\n",
    "# Update the Rating for the specified companies\n",
    "new_ratings = {'CenterPoint Energy Inc': 6, 'ConocoPhillips': 6, 'eBay Inc': 5, 'EOG Resources Inc': 7, 'FreeportMcMoRan Inc': 11, 'Marriott International Inc': 8, 'Ralph Lauren Corporation': 5}\n",
    "\n",
    "for company, new_rating in new_ratings.items():\n",
    "    company_indices = df_pp3_final_year[df_pp3_final_year[\"Company Name\"] == company].index\n",
    "    df_pp3_final.loc[company_indices, \"Rating\"] = new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2016\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == 2016]\n",
    "\n",
    "# Update the Rating for the specified companies\n",
    "new_ratings = {'CenterPoint Energy Inc': 6, 'ConocoPhillips': 6, 'EOG Resources Inc': 7, 'FreeportMcMoRan Inc': 12,'Gilead Sciences Inc': 5, 'Marriott International Inc': 8, 'Ralph Lauren Corporation': 6}\n",
    "\n",
    "for company, new_rating in new_ratings.items():\n",
    "    company_indices = df_pp3_final_year[df_pp3_final_year[\"Company Name\"] == company].index\n",
    "    df_pp3_final.loc[company_indices, \"Rating\"] = new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2017\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == 2017]\n",
    "\n",
    "# Update the Rating for the specified companies\n",
    "new_ratings = {'CenterPoint Energy Inc': 6, 'ConocoPhillips': 6, 'EOG Resources Inc': 7,'Eversource Energy': 4, 'FreeportMcMoRan Inc': 12,'Gilead Sciences Inc': 5, 'Marriott International Inc': 8, 'Ralph Lauren Corporation': 6}\n",
    "\n",
    "for company, new_rating in new_ratings.items():\n",
    "    company_indices = df_pp3_final_year[df_pp3_final_year[\"Company Name\"] == company].index\n",
    "    df_pp3_final.loc[company_indices, \"Rating\"] = new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_pp3_final for the year 2018\n",
    "df_pp3_final_year = df_pp3_final[df_pp3_final[\"Years\"] == 2018]\n",
    "\n",
    "# Update the Rating for the specified companies\n",
    "new_ratings = {'CenterPoint Energy Inc': 7, 'ConocoPhillips': 6, 'EOG Resources Inc': 7,'Eversource Energy': -1, 'FreeportMcMoRan Inc': 12,'Gilead Sciences Inc': -1, 'Marriott International Inc': 9, 'Ralph Lauren Corporation': 6}\n",
    "\n",
    "for company, new_rating in new_ratings.items():\n",
    "    company_indices = df_pp3_final_year[df_pp3_final_year[\"Company Name\"] == company].index\n",
    "    df_pp3_final.loc[company_indices, \"Rating\"] = new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ce532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Downgrade\" column to integer\n",
    "df_pp3_final[\"Downgrade\"] = df_pp3_final[\"Downgrade\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each value in the \"Downgrade\" column\n",
    "downgrade_counts = df_pp3_final[\"Downgrade\"].value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(downgrade_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767290e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Index\n",
    "df_pp3_final.set_index(\"Years\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39529983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwende die Methode factorize und erhöhe das Ergebnis um 1, um eine eindeutige ID für jeden eindeutigen Wert in der Spalte \"Company Name\" zu erhalten\n",
    "df_pp3_final[\"Company ID\"] = pd.factorize(df_pp3_final[\"Company Name\"])[0] + 1\n",
    "df_pp3_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfm is the DataFrame you want to save\n",
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Downgrade\\\\downgrade_all.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_pp3_final.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"Company Name\"\n",
    "df_pp3_final.drop(columns=[\"Company Name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6d308",
   "metadata": {},
   "source": [
    "## 3.6 Check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4 = df_pp3_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b32788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pp4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_pp4.corr()\n",
    "\n",
    "plt.figure(figsize=(40, 32))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", xticklabels=\"auto\", yticklabels=\"auto\")\n",
    "plt.title(\"Korrelationsmatrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0578905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechne die Korrelationsmatrix\n",
    "correlation_matrix = df_pp4.corr()\n",
    "\n",
    "# Erstelle eine leere Liste, um die Ergebnisse zu speichern\n",
    "correlation_results = []\n",
    "\n",
    "# Iteriere über die Spalten der Korrelationsmatrix und berechne die Korrelation zwischen jedem Feature-Paar\n",
    "for i, feature1 in enumerate(correlation_matrix.columns):\n",
    "    for j, feature2 in enumerate(correlation_matrix.columns):\n",
    "        if i < j:\n",
    "            correlation_value = correlation_matrix.iloc[i, j]\n",
    "            correlation_results.append([feature1, feature2, correlation_value])\n",
    "\n",
    "# Erstelle ein DataFrame mit den Korrelationsergebnissen\n",
    "correlation_df = pd.DataFrame(correlation_results, columns=['Feature 1', 'Feature 2', 'Korrelationswert'])\n",
    "\n",
    "# Zeige das DataFrame mit den Korrelationsergebnissen an\n",
    "correlation_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cfb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtere die Korrelationswerte: Über 0,7 und nicht gleich 1\n",
    "filtered_correlation_df = correlation_df[\n",
    "    (correlation_df['Korrelationswert'] > 0.7) & (correlation_df['Korrelationswert'] < 1)\n",
    "]\n",
    "\n",
    "# Zeige das DataFrame mit den gefilterten Korrelationsergebnissen an\n",
    "filtered_correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29c98a",
   "metadata": {},
   "source": [
    "In the dataset, there are metrics for multiple years, and it is observed that these metrics exhibit strong correlations among themselves. From a domain perspective, this is understandable and indicates a stable company. It is important to note that the LinkedIn and Finance KPIs do not show a high correlation to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004cf6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where 'Change_2018/19_down grade' appears in either 'Feature 1' or 'Feature 2'\n",
    "correlation_results_filtered = correlation_df[(correlation_df['Feature 1'] == 'Downgrade') | \n",
    "                                              (correlation_df['Feature 2'] == 'Downgrade')]\n",
    "\n",
    "# Sort the results based on the absolute value of correlation in descending order\n",
    "correlation_results_filtered = correlation_results_filtered.iloc[correlation_results_filtered['Korrelationswert'].abs().argsort()[::-1]]\n",
    "correlation_results_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99396a5c",
   "metadata": {},
   "source": [
    "The heat map visulaizes the correlation of the variables in a one-to-one relationship. Since no correlation is overproportionally high, the vif value is used to check the interaction between several variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df_pp4[['Rating', 'Market Capitalization', 'EBITDA', 'EBIT', 'Net Income',\n",
    "       'Total Equity', 'Total Debt', 'Total Assets', 'Net Debt',\n",
    "       'Gross Profit', 'Total Employees', 'Cash from Ops.', 'Total Revenue',\n",
    "       'Number of employees', 'Employee development', 'Number of notices',\n",
    "       'Migrating work experience', 'Number of New Joiners',\n",
    "       'New joining work experience', 'Fluctuation rate','Company ID']]\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif['Features'] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f888db6",
   "metadata": {},
   "source": [
    "High vif in the fianancial kpis and linkedin features should be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp4 = df_pp4.drop(['Number of employees', 'EBITDA', 'Total Debt', 'Cash from Ops.'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2567b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = df_pp4[['Rating', 'Market Capitalization', 'EBIT', 'Net Income',\n",
    "       'Total Equity', 'Total Assets', 'Net Debt',\n",
    "       'Gross Profit', 'Total Employees', 'Total Revenue',\n",
    "       'Employee development', 'Number of notices',\n",
    "       'Migrating work experience', 'Number of New Joiners',\n",
    "       'New joining work experience', 'Fluctuation rate','Company ID']]\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif['Features'] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7ca34",
   "metadata": {},
   "source": [
    "Vif in New joining work experience and Gross profit ist accepted, since it is close to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f364cc",
   "metadata": {},
   "source": [
    "# 4. Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848c139",
   "metadata": {},
   "source": [
    "In the following, the required data frames are first formed on the basis of the cleaned data (4.0). Then the ML models are first applied to the financial ratios (4.1), then to the LinkedIn features (4.2) and finally to the combined data (4.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c7ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df_pp4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956cb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e526d",
   "metadata": {},
   "source": [
    "## 4.1 Building dataframes\n",
    "\n",
    "Create the required data frames:\n",
    "- df_financials\n",
    "- df_linkedin\n",
    "- df_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf94886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting column names\n",
    "column_names = [\"{}\".format(col) for col in dfm.columns]\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns for financials\n",
    "selected_columns = ['Market Capitalization', 'EBIT', 'Net Income', 'Total Equity', 'Total Assets', 'Net Debt', 'Gross Profit', 'Total Employees', 'Total Revenue', 'Downgrade',]\n",
    "# Creat new dataframe\n",
    "df_financials = dfm[selected_columns]\n",
    "df_financials.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfm is the DataFrame you want to save\n",
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Downgrade\\\\Financials\\\\downgrade_financials.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_financials.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e129a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns for linkedin\n",
    "selected_columns = ['Employee development', 'Number of notices', 'Migrating work experience', 'Number of New Joiners', 'New joining work experience', 'Fluctuation rate', 'Downgrade',]\n",
    "# Creat new dataframe\n",
    "df_linkedin = dfm[selected_columns]\n",
    "df_linkedin.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7986ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Downgrade\\\\LinkedIn\\\\downgrade_LinkedIn.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_linkedin.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com = dfm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.drop(\"Company ID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dfm is the DataFrame you want to save\n",
    "# Replace 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Default' with your desired path\n",
    "file_path = 'C:\\\\Users\\\\chiar\\\\OneDrive\\\\Masterthesis\\\\Modell\\\\Dataframes\\\\Downgrade\\\\Com\\\\downgrade_combined.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_com.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e167ae6",
   "metadata": {},
   "source": [
    "## 4.2 Running modells on df_financials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b895d",
   "metadata": {},
   "source": [
    "### 4.2.1 Splitting between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index values to integers\n",
    "df_financials.index = df_financials.index.astype(int)\n",
    "\n",
    "# Splitting the data into training and test data\n",
    "train_df = df_financials[df_financials.index < 2018]   # All years before 2018 will be used as training data\n",
    "test_df = df_financials[df_financials.index == 2018]   # Data for the year 2018 will be used as test data\n",
    "\n",
    "# Defining the features\n",
    "features = ['Market Capitalization', 'EBIT', 'Net Income', 'Total Equity', 'Total Assets', 'Net Debt', 'Gross Profit', 'Total Employees', 'Total Revenue',]  # Here you would specify the desired features\n",
    "\n",
    "# Splitting the features and the target variable\n",
    "X_train_pre = train_df[features] #training data from all properties that are not the target column (80%).\n",
    "y_train_pre = train_df[\"Downgrade\"] #Training data from the target variable (80%)\n",
    "X_test = test_df[features] #analog X_train, but only 20%.\n",
    "y_test = test_df[\"Downgrade\"] #analog Y_train, but only 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ab7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62a898",
   "metadata": {},
   "source": [
    "### 4.2.2 Generating synthetic data of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Instanz der ADASYN-Klasse\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Anwenden von ADASYN, um synthetische Daten zu generieren\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_pre, y_train_pre)\n",
    "\n",
    "# Sie können auch die generierten numpy-Arrays wieder in Dataframes umwandeln, falls erforderlich\n",
    "X_train = pd.DataFrame(X_train_adasyn, columns=X_train_pre.columns)\n",
    "y_train = pd.Series(y_train_adasyn, name=y_train_pre.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caadefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame(y_train_pre)\n",
    "temp2 = pd.DataFrame(y_train)\n",
    "\n",
    "print('Before SMOTE')\n",
    "print(temp1['Downgrade'].value_counts())\n",
    "print('After SMOTE')\n",
    "print(temp2['Downgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bff2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.DataFrame(y_test)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Downgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538d5ee",
   "metadata": {},
   "source": [
    "Oversampling of train data succsessfull. Test data still unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964483",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Financials\\x_test_financials.csv', index=False)\n",
    "X_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Financials\\x_train_financials.csv', index=False)\n",
    "y_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Financials\\y_test_financials.csv', index=False)\n",
    "y_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Financials\\y_train_financials.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb06025",
   "metadata": {},
   "source": [
    "### 4.2.4  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2cae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466927c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149df97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the X-target variable is compared with the predicted values\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e70948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "plot_confusion_matrix(classifier,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp/(fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"True Negatives: \" + str(tn))\n",
    "print(\"False Positives: \" + str(fp))\n",
    "print(\"False Negatives: \" + str(fn))\n",
    "print(\"True Positives: \" + str(tp))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cb6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11addbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9143c4be",
   "metadata": {},
   "source": [
    "For verification, we check how the target variable of the training data is predicted. Therefore, the y_train is predicted using logistic regression and using the properties (x_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26400e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711596c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparison and results check \n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials = {\n",
    "    \"Logistic Regression\": [0.04, 0.09, 0.80, 0.45],}\n",
    "# precision, recall, acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38a018",
   "metadata": {},
   "source": [
    "### 4.2.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451df95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4398f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred_tree)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59473f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_tree = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the Decision Tree classifier\n",
    "auc_score_tree = roc_auc_score(y_test, y_pred_prob_tree)\n",
    "print(\"AUC Score for Decision Tree:\", auc_score_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1da344",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree,X_train,y_train, cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials.update({\n",
    "    \"Decision Tree\": [0.16, 0.13, 0.89, 0.54]\n",
    "})\n",
    "# precision, recall, acc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbfddb",
   "metadata": {},
   "source": [
    "### 4.2.6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada54a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = [5, 10, 20]\n",
    "for i in tree_depth:\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Max tree depth: ', i)\n",
    "    print('Train results: ', classification_report(y_train, rf.predict(X_train)))\n",
    "    print('Test results: ',classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22201f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the RandomForestClassifier\n",
    "auc_score_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
    "print('AUC Score for Random Forest:', auc_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b63402",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials.update({\n",
    "    \"Random Forest\": [0.18, 0.09, 0.91, 0.47],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcef5a",
   "metadata": {},
   "source": [
    "### 4.2.7 XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d52780",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = plot_importance(xgb, height=0.9, max_num_features=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0f83a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Train results: ', classification_report(y_train, xgb.predict(X_train)))\n",
    "print('Test results: ',classification_report(y_test, xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the XGBoost classifier\n",
    "auc_score_xgb = roc_auc_score(y_test, y_pred_prob_xgb)\n",
    "print('AUC Score for XGBoost:', auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39518130",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_financials.update({\n",
    "    \"XGBoost\": [0.18, 0.09, 0.91, 0.46],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c14c5",
   "metadata": {},
   "source": [
    "## 4.3 Running modells on df_linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246fe388",
   "metadata": {},
   "source": [
    "### 4.3.1 Splitting between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index values to integers\n",
    "df_linkedin.index = df_linkedin.index.astype(int)\n",
    "\n",
    "# Splitting the data into training and test data\n",
    "train_df = df_linkedin[df_linkedin.index < 2018]   # All years before 2018 will be used as training data\n",
    "test_df = df_linkedin[df_linkedin.index == 2018]   # Data for the year 2018 will be used as test data\n",
    "\n",
    "# Defining the features\n",
    "features = ['Employee development', 'Number of notices', 'Migrating work experience', 'Number of New Joiners', 'New joining work experience', 'Fluctuation rate',]  # Here you would specify the desired features\n",
    "\n",
    "# Splitting the features and the target variable\n",
    "X_train_pre = train_df[features] #training data from all properties that are not the target column (80%).\n",
    "y_train_pre = train_df[\"Downgrade\"] #Training data from the target variable (80%)\n",
    "X_test = test_df[features] #analog X_train, but only 20%.\n",
    "y_test = test_df[\"Downgrade\"] #analog Y_train, but only 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac32fd",
   "metadata": {},
   "source": [
    "### 4.3.2 Generating syntetic data of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43374d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Instanz der ADASYN-Klasse\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Anwenden von ADASYN, um synthetische Daten zu generieren\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_pre, y_train_pre)\n",
    "\n",
    "# Sie können auch die generierten numpy-Arrays wieder in Dataframes umwandeln, falls erforderlich\n",
    "X_train = pd.DataFrame(X_train_adasyn, columns=X_train_pre.columns)\n",
    "y_train = pd.Series(y_train_adasyn, name=y_train_pre.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ab6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\LinkedIn\\x_test_linkedin.csv', index=False)\n",
    "X_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\LinkedIn\\x_train_linkedin.csv', index=False)\n",
    "y_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\LinkedIn\\y_test_linkedin.csv', index=False)\n",
    "y_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\LinkedIn\\y_train_linkedin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c234b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame(y_train_pre)\n",
    "temp2 = pd.DataFrame(y_train)\n",
    "\n",
    "print('Before SMOTE')\n",
    "print(temp1['Downgrade'].value_counts())\n",
    "print('After SMOTE')\n",
    "print(temp2['Downgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e59ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.DataFrame(y_test)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Downgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a2167",
   "metadata": {},
   "source": [
    "Oversampling successful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4778b6",
   "metadata": {},
   "source": [
    "### 4.3.3  Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e52eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ab1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the X-target variable is compared with the predicted values\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27186b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "plot_confusion_matrix(classifier,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f69dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp/(fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"True Negatives: \" + str(tn))\n",
    "print(\"False Positives: \" + str(fp))\n",
    "print(\"False Negatives: \" + str(fn))\n",
    "print(\"True Positives: \" + str(tp))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99dfb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison and results check \n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin = {\n",
    "    \"Logistic Regression\": [0.07, 0.96, 0.12 ,0.56],}\n",
    "# first test, then train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c824de5",
   "metadata": {},
   "source": [
    "### 4.3.4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5464aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4852270",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(tree,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_tree)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1601fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_tree = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the Decision Tree classifier\n",
    "auc_score_tree = roc_auc_score(y_test, y_pred_prob_tree)\n",
    "print(\"AUC Score for Decision Tree:\", auc_score_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75619f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = tree.predict(X_train)\n",
    "plot_confusion_matrix(tree,X_train,y_train, cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71344e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin.update({\n",
    "    \"Decision Tree\": [0.03, 0.04, 0.84, 0.47],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda18e5",
   "metadata": {},
   "source": [
    "### 4.3.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449723e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_depth = [5, 10, 20]\n",
    "for i in tree_depth:\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Max tree depth: ', i)\n",
    "    print('Train results: ', classification_report(y_train, rf.predict(X_train)))\n",
    "    print('Test results: ',classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4575b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db645f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the RandomForestClassifier\n",
    "auc_score_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
    "print('AUC Score for Random Forest:', auc_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f829da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin.update({\n",
    "    \"Random Forest\": [0.00, 0.00, 0.91, 0.48],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864eef47",
   "metadata": {},
   "source": [
    "### 4.3.5 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = plot_importance(xgb, height=0.9, max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the XGBoost classifier\n",
    "auc_score_xgb = roc_auc_score(y_test, y_pred_prob_xgb)\n",
    "print('AUC Score for XGBoost:', auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14ec8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train results: ', classification_report(y_train, xgb.predict(X_train)))\n",
    "print('Test results: ',classification_report(y_test, xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_linkedin.update({\n",
    "    \"XGBoost\": [0.0, 0.0, 0.87, 0.45],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770175df",
   "metadata": {},
   "source": [
    "## 4.4 Running modells on df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae7d12",
   "metadata": {},
   "source": [
    "### 4.4.1 Splitting between train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ae5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index values to integers\n",
    "df_com.index = df_com.index.astype(int)\n",
    "\n",
    "# Splitting the data into training and test data\n",
    "train_df = df_com[df_com.index < 2018]   # All years before 2018 will be used as training data\n",
    "test_df = df_com[df_com.index == 2018]   # Data for the year 2018 will be used as test data\n",
    "\n",
    "# Defining the features\n",
    "features = ['Market Capitalization', 'EBIT', 'Net Income', 'Total Equity', 'Total Assets', 'Net Debt', 'Gross Profit', 'Total Employees', 'Total Revenue','Employee development', 'Number of notices', 'Migrating work experience', 'Number of New Joiners', 'New joining work experience', 'Fluctuation rate',]  # Here you would specify the desired features\n",
    "\n",
    "# Splitting the features and the target variable\n",
    "X_train_pre = train_df[features] #training data from all properties that are not the target column (80%).\n",
    "y_train_pre = train_df[\"Downgrade\"] #Training data from the target variable (80%)\n",
    "X_test = test_df[features] #analog X_train, but only 20%.\n",
    "y_test = test_df[\"Downgrade\"] #analog Y_train, but only 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f83488",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7695e9",
   "metadata": {},
   "source": [
    "### 4.4.2 Oversampling of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c28b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen Sie eine Instanz der ADASYN-Klasse\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Anwenden von ADASYN, um synthetische Daten zu generieren\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_pre, y_train_pre)\n",
    "\n",
    "# Sie können auch die generierten numpy-Arrays wieder in Dataframes umwandeln, falls erforderlich\n",
    "X_train = pd.DataFrame(X_train_adasyn, columns=X_train_pre.columns)\n",
    "y_train = pd.Series(y_train_adasyn, name=y_train_pre.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.DataFrame(y_train_pre)\n",
    "temp2 = pd.DataFrame(y_train)\n",
    "\n",
    "print('Before SMOTE')\n",
    "print(temp1['Downgrade'].value_counts())\n",
    "print('After SMOTE')\n",
    "print(temp2['Downgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.DataFrame(y_test)\n",
    "\n",
    "print('Check for test data')\n",
    "print(temp3['Downgrade'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7308908",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Com\\x_test_com.csv', index=False)\n",
    "X_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Com\\x_train_com.csv', index=False)\n",
    "y_test.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Com\\y_test_com.csv', index=False)\n",
    "y_train.to_csv(r'C:\\Users\\chiar\\OneDrive\\Masterthesis\\Modell\\Dataframes\\Downgrade\\Com\\y_train_com.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ade16",
   "metadata": {},
   "source": [
    "### 4.4.3  Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "# Here the X-target variable is compared with the predicted values\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "print (\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "plot_confusion_matrix(classifier,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "recall = tp/(fn+tp)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"True Negatives: \" + str(tn))\n",
    "print(\"False Positives: \" + str(fp))\n",
    "print(\"False Negatives: \" + str(fn))\n",
    "print(\"True Positives: \" + str(tp))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"Precision: \" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701df7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_train)\n",
    "# Comparison and results check \n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c685349",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com= {\n",
    "    \"Logistic Regression\": [0.06, 0.22, 0.73, 0.46],}\n",
    "# first test, then train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1265e",
   "metadata": {},
   "source": [
    "### 4.4.4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = tree.predict(X_test)\n",
    "plot_confusion_matrix(tree,X_test,y_test,cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred_tree)\n",
    "print (\"Accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f72a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_tree = tree.predict(X_train)\n",
    "plot_confusion_matrix(tree,X_train,y_train, cmap='Blues')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59439e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_tree = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the Decision Tree classifier\n",
    "auc_score_tree = roc_auc_score(y_test, y_pred_prob_tree)\n",
    "print(\"AUC Score for Decision Tree:\", auc_score_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ae958",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com.update({\n",
    "    \"Decision Tree\": [0.17, 0.13, 0.89, 0.54],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd8924",
   "metadata": {},
   "source": [
    "### 4.4.5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = [5, 10, 20]\n",
    "for i in tree_depth:\n",
    "    rf = RandomForestClassifier(max_depth=i)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Max tree depth: ', i)\n",
    "    print('Train results: ', classification_report(y_train, rf.predict(X_train)))\n",
    "    print('Test results: ',classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11639f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24464ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the RandomForestClassifier\n",
    "auc_score_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
    "print('AUC Score for Random Forest:', auc_score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751db07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com.update({\n",
    "    \"Random Forest\": [0.25, 0.04, 0.93, 0.50],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38384e0f",
   "metadata": {},
   "source": [
    "### 4.4.6 XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = plot_importance(xgb, height=0.9, max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f18cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train results: ', classification_report(y_train, xgb.predict(X_train)))\n",
    "print('Test results: ',classification_report(y_test, xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted probabilities for the positive class (class 1)\n",
    "y_pred_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the AUC score for the XGBoost classifier\n",
    "auc_score_xgb = roc_auc_score(y_test, y_pred_prob_xgb)\n",
    "print('AUC Score for XGBoost:', auc_score_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54559c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precisions_com.update({\n",
    "    \"XGBoost\": [0.36, 0.17, 0.92 ,0.59],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1ec67",
   "metadata": {},
   "source": [
    "# 5. Evaluating and comparing Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce265c6",
   "metadata": {},
   "source": [
    "The results are compiled and neatly presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75367774",
   "metadata": {},
   "source": [
    "## 5.1 Results financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"\", \"Precision (1)\", \"Recall (1)\", \"Accuracy\", \"AUC\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = headers\n",
    "\n",
    "# List to store the maximum values in each column\n",
    "max_values = [0.0] * len(headers)\n",
    "\n",
    "best_model_auc = \"\"\n",
    "max_auc_value = 0.0\n",
    "\n",
    "for model, metrics in Precisions_financials.items():\n",
    "    precision_test_data = metrics[0]\n",
    "    precision_train_data = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    auc = metrics[3]\n",
    "\n",
    "    # Update the maximum values for each column\n",
    "    max_values[1] = max(max_values[1], precision_test_data)\n",
    "    max_values[2] = max(max_values[2], precision_train_data)\n",
    "    max_values[3] = max(max_values[3], accuracy)\n",
    "    max_values[4] = max(max_values[4], auc)\n",
    "\n",
    "    # Update the best model based on the highest AUC score\n",
    "    if auc > max_auc_value:\n",
    "        max_auc_value = auc\n",
    "        best_model_auc = model\n",
    "\n",
    "    # Add a row to the table\n",
    "    table.add_row([model, precision_test_data, precision_train_data, accuracy, auc])\n",
    "\n",
    "# Mark the highest value in each column in red\n",
    "for row in table._rows:\n",
    "    for i in range(1, len(headers)):\n",
    "        if row[i] == max_values[i]:\n",
    "            row[i] = f\"\\033[31m{row[i]}\\033[0m\"  # Red color for the highest value\n",
    "\n",
    "# Print the table with the highest values in each column marked in red\n",
    "print(table)\n",
    "\n",
    "# Print the \"Best model\" message in red\n",
    "print(f\"\\033[31mBest model based on AUC: {best_model_auc}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308cbfd",
   "metadata": {},
   "source": [
    "## 5.2 Results linkedin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"\", \"Precision (1)\", \"Recall (1)\", \"Accuracy\", \"AUC\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = headers\n",
    "\n",
    "# List to store the maximum values in each column\n",
    "max_values = [0.0] * len(headers)\n",
    "\n",
    "best_model_auc = \"\"\n",
    "max_auc_value = 0.0\n",
    "\n",
    "for model, metrics in Precisions_linkedin.items():\n",
    "    precision_test_data = metrics[0]\n",
    "    precision_train_data = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    auc = metrics[3]\n",
    "\n",
    "    # Update the maximum values for each column\n",
    "    max_values[1] = max(max_values[1], precision_test_data)\n",
    "    max_values[2] = max(max_values[2], precision_train_data)\n",
    "    max_values[3] = max(max_values[3], accuracy)\n",
    "    max_values[4] = max(max_values[4], auc)\n",
    "\n",
    "    # Update the best model based on the highest AUC score\n",
    "    if auc > max_auc_value:\n",
    "        max_auc_value = auc\n",
    "        best_model_auc = model\n",
    "\n",
    "    # Add a row to the table\n",
    "    table.add_row([model, precision_test_data, precision_train_data, accuracy, auc])\n",
    "\n",
    "# Mark the highest value in each column in red\n",
    "for row in table._rows:\n",
    "    for i in range(1, len(headers)):\n",
    "        if row[i] == max_values[i]:\n",
    "            row[i] = f\"\\033[31m{row[i]}\\033[0m\"  # Red color for the highest value\n",
    "\n",
    "# Print the table with the highest values in each column marked in red\n",
    "print(table)\n",
    "\n",
    "# Print the \"Best model\" message in red\n",
    "print(f\"\\033[31mBest model based on AUC: {best_model_auc}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe8164",
   "metadata": {},
   "source": [
    "## 5.3 Results combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b204ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"\", \"Precision (1)\", \"Recall (1)\", \"Accuracy\", \"AUC\"]\n",
    "table = PrettyTable()\n",
    "table.field_names = headers\n",
    "\n",
    "# List to store the maximum values in each column\n",
    "max_values = [0.0] * len(headers)\n",
    "\n",
    "best_model_auc = \"\"\n",
    "max_auc_value = 0.0\n",
    "\n",
    "for model, metrics in Precisions_com.items():\n",
    "    precision_test_data = metrics[0]\n",
    "    precision_train_data = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    auc = metrics[3]\n",
    "\n",
    "    # Update the maximum values for each column\n",
    "    max_values[1] = max(max_values[1], precision_test_data)\n",
    "    max_values[2] = max(max_values[2], precision_train_data)\n",
    "    max_values[3] = max(max_values[3], accuracy)\n",
    "    max_values[4] = max(max_values[4], auc)\n",
    "\n",
    "    # Update the best model based on the highest AUC score\n",
    "    if auc > max_auc_value:\n",
    "        max_auc_value = auc\n",
    "        best_model_auc = model\n",
    "\n",
    "    # Add a row to the table\n",
    "    table.add_row([model, precision_test_data, precision_train_data, accuracy, auc])\n",
    "\n",
    "# Mark the highest value in each column in red\n",
    "for row in table._rows:\n",
    "    for i in range(1, len(headers)):\n",
    "        if row[i] == max_values[i]:\n",
    "            row[i] = f\"\\033[31m{row[i]}\\033[0m\"  # Red color for the highest value\n",
    "\n",
    "# Print the table with the highest values in each column marked in red\n",
    "print(table)\n",
    "\n",
    "# Print the \"Best model\" message in red\n",
    "print(f\"\\033[31mBest model based on AUC: {best_model_auc}\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
